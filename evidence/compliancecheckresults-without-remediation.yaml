apiVersion: v1
items:
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Restrict Automounting of Service Account Tokens
    Mounting service account tokens inside pods can provide an avenue
    for privilege escalation attacks where an attacker is able to
    compromise a single pod in the cluster.
  id: xccdf_org.ssgproject.content_rule_accounts_restrict_service_account_tokens
  instructions: |-
    For each pod in the cluster, review the pod specification and
    ensure that pods that do not need to explicitly communicate with
    the API server have automountServiceAccountToken
    configured to false.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: accounts-restrict-service-account-tokens
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-accounts-restrict-service-account-tokens
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38298"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-accounts-restrict-service-account-tokens
    uid: 400c971d-4fcf-4446-8bc5-cf1850dc0119
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Ensure Usage of Unique Service Accounts \nKubernetes provides a default service account which is used by\ncluster workloads where no specific service account is assigned to the pod.\nWhere access to the Kubernetes API from a pod is required, a specific service account\nshould be created for that pod, and rights granted to that service account.\nThis increases auditability of service account rights and access making it\neasier and more accurate to trace potential malicious behaviors to a specific\nservice account and project."
  id: xccdf_org.ssgproject.content_rule_accounts_unique_service_account
  instructions: |-
    For each namespace in the cluster, review the rights assigned
    to the default service account. There should be no cluster or local roles
    assigned to the default other than the defaults.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: accounts-unique-service-account
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-cis-accounts-unique-service-account
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38474"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-accounts-unique-service-account
    uid: 01ffe04d-6682-4dba-9446-eb860df4131d
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable the AlwaysAdmit Admission Control Plugin
    Enabling the admission control plugin AlwaysAdmit allows all
    requests and does not provide any filtering.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysAdmit
  instructions: |-
    To verify that the AlwaysAdmit admission control plugin is not set, run the following command:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
    The output should not contain AlwaysAdmit
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-alwaysadmit
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-api-server-admission-control-plugin-alwaysadmit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38302"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-alwaysadmit
    uid: 43f473a6-250d-4601-a63d-14a4ca9f7b10
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the Admission Control Plugin AlwaysPullImages is not set
    Setting admission control policy to AlwaysPullImages forces every new pod
    to pull the required images every time. In a multi-tenant cluster users can
    be assured that their private images can only be used by those who have the
    credentials to pull them. Without this admission control policy, once an
    image has been pulled to a node, any pod from any user can use it simply by
    knowing the imageâ€™s name, without any authorization check against the image
    ownership. When this plug-in is enabled, images are always pulled prior to
    starting containers, which means valid credentials are required.

    However, turning on this admission plugin can introduce new kinds of
    cluster failure modes. OpenShift 4 master and infrastructure components are
    deployed as pods. Enabling this feature can result in cases where loss of
    contact to an image registry can cause a redeployed infrastructure pod
    (oauth-server for example) to fail on an image pull for an image that is
    currently present on the node. We use PullIfNotPresent so that a loss of
    image registry access does not prevent the pod from starting.  If it
    becomes PullAlways, then an image registry access outage can cause key
    infrastructure components to fail.

    The pull policy can be managed per container, using
    imagePullPolicy.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysPullImages
  instructions: |-
    Run the following command:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
    The output list should not contain "AlwaysPullImages".
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-alwayspullimages
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-api-server-admission-control-plugin-alwayspullimages
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38368"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-alwayspullimages
    uid: c21de6da-9d45-446a-b7f8-f63679cadcdc
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the NamespaceLifecycle Admission Control Plugin
    Setting admission control policy to NamespaceLifecycle ensures that
    objects cannot be created in non-existent namespaces, and that namespaces
    undergoing termination are not used for creating new objects. This
    is recommended to enforce the integrity of the namespace termination process
    and also for the availability of new objects.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NamespaceLifecycle
  instructions: |-
    To verify that the NamespaceLifecycle plugin is enabled in
    the apiserver configuration, run:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-namespacelifecycle
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-cis-api-server-admission-control-plugin-namespacelifecycle
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38480"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-namespacelifecycle
    uid: 4dd2ce69-6e35-4ff3-99b2-936e9f508efe
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the NodeRestriction Admission Control Plugin
    Using the NodeRestriction plugin ensures that the kubelet is
    restricted to the Node and Pod objects that it could
    modify as defined. Such kubelets will only be allowed to modify their
    own Node API object, and only modify Pod API objects
    that are bound to their node.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NodeRestriction
  instructions: |-
    Ensure that the NodeRestriction plugin is enabled in the list of enabled plugins in
    the apiserver configuration by running the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep 'NodeRestriction'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-noderestriction
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-admission-control-plugin-noderestriction
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38461"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-noderestriction
    uid: 056683bc-5e0a-4010-ad7c-9f05c395a4cf
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the SecurityContextConstraint Admission Control Plugin
    A Security Context Constraint is a cluster-level resource that controls the actions
    which a pod can perform and what the pod may access. The
    SecurityContextConstraint objects define a set of conditions that a pod
    must run with in order to be accepted into the system. Security Context Constraints
    are comprised of settings and strategies that control the security features
    a pod has access to and hence this must be used to control pod access
    permissions.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_Scc
  instructions: |-
    The SecurityContextConstraint plugin should be enabled in the list of enabled plugins in
    the apiserver configuration:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-scc
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-api-server-admission-control-plugin-scc
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38433"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-scc
    uid: 6f9d4ec9-2b7e-4636-b600-943ff78e300c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used
    The SecurityContextDeny admission control plugin disallows
    setting any security options for your pods. SecurityContextConstraints
    allow you to enforce RBAC rules on who can set these options on the pods, and
    what they're allowed to set. Thus, using the SecurityContextDeny
    will deter you from enforcing granular permissions on your pods.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_SecurityContextDeny
  instructions: |-
    The SecurityContextDeny plugin should not be enabled in the list of enabled plugins in the apiserver configuration:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-securitycontextdeny
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-admission-control-plugin-securitycontextdeny
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38240"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-securitycontextdeny
    uid: 2e5b3128-f6e8-446a-8a88-27c23bf2a7b8
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the ServiceAccount Admission Control Plugin
    When a pod is created, if a service account is not specified, the pod
    is automatically assigned the default service account in the same
    namespace. OpenShift operators should create unique service accounts
    and let the API Server manage its security tokens.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_ServiceAccount
  instructions: |-
    The ServiceAccount plugin should be enabled in the list of enabled plugins in
    the apiserver configuration:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-serviceaccount
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-api-server-admission-control-plugin-serviceaccount
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38385"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-admission-control-plugin-serviceaccount
    uid: d37a2156-50d6-4d9d-9169-eb05a7aadc99
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that anonymous requests to the API Server are authorized
    When enabled, requests that are not rejected by other configured
    authentication methods are treated as anonymous requests. These requests
    are then served by the API server. If you are using RBAC authorization,
    it is generally considered reasonable to allow anonymous access to the
    API Server for health checks and discovery purposes, and hence this
    recommendation is not scored. However, you should consider whether
    anonymous discovery is an acceptable risk for your purposes.
  id: xccdf_org.ssgproject.content_rule_api_server_anonymous_auth
  instructions: |-
    Run the following command to view the authorization rules for anonymous requests:
    $ oc describe clusterrolebindings
    Make sure that there exists at least one clusterrolebinding that binds
    either the system:unauthenticated group or the system:anonymous
    user.
    To test that an anonymous request is authorized to access the readyz
    endpoint, run:
    $ oc get --as="system:anonymous" --raw='/readyz?verbose'
    In contrast, a request to list all projects should not be authorized:
    $ oc get --as="system:anonymous" projects
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-anonymous-auth
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-anonymous-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38464"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-anonymous-auth
    uid: 2222ba3f-0525-4dc0-b26b-0898cad26cdd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure catch-all FlowSchema object for API Priority and Fairness Exists
    The FlowSchema API objects enforce a limit on the
    number of events that the API Server will accept in a given time slice
    In a large multi-tenant cluster, there might be a small percentage of
    misbehaving tenants which could have a significant impact on the
    performance of the cluster overall. It is recommended to limit the rate
    of events that the API Server will accept.
  id: xccdf_org.ssgproject.content_rule_api_server_api_priority_flowschema_catch_all
  instructions: |-
    Run the following commands:
    oc get flowschema
    and inspect the FlowSchema objects. Make sure that at least the catch-all
    object exists by calling:
    oc describe flowschema catch-all
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-api-priority-flowschema-catch-all
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-api-server-api-priority-flowschema-catch-all
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38311"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-api-priority-flowschema-catch-all
    uid: dfafbe17-77dc-426b-89f3-9c85583b95b4
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - Note that this is only applicable in OpenShift Container Platform version 4.8 and higher
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the APIPriorityAndFairness feature gate
    The APIPriorityAndFairness feature gate enables the use of the
    FlowSchema API objects which enforce a limit on the number of
    events that the API Server will accept in a given time slice In a large
    multi-tenant cluster, there might be a small percentage of misbehaving
    tenants which could have a significant impact on the performance of
    the cluster overall. It is recommended to limit the rate of events
    that the API Server will accept.
  id: xccdf_org.ssgproject.content_rule_api_server_api_priority_gate_enabled
  instructions: |-
    To verify that APIPriorityAndFairness is enabled, run the following command:
    oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.apiServerArguments["feature-gates"]'
    The output should contain "APIPriorityAndFairness=true"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-api-priority-gate-enabled
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-api-server-api-priority-gate-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38341"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-api-priority-gate-enabled
    uid: 8e4b0917-7ad3-47d5-96ec-b9a6e4217dc9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure catch-all FlowSchema object for API Priority and Fairness Exists (v1alpha1)
    The FlowSchema API objects enforce a limit on the
    number of events that the API Server will accept in a given time slice
    In a large multi-tenant cluster, there might be a small percentage of
    misbehaving tenants which could have a significant impact on the
    performance of the cluster overall. It is recommended to limit the rate
    of events that the API Server will accept.
  id: xccdf_org.ssgproject.content_rule_api_server_api_priority_v1alpha1_flowschema_catch_all
  instructions: |-
    Run the following commands:
    oc get flowschema
    and inspect the FlowSchema objects. Make sure that at least the catch-all
    object exists by calling:
    oc describe flowschema catch-all
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-api-priority-v1alpha1-flowschema-catch-all
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-api-priority-v1alpha1-flowschema-catch-all
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38235"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-api-priority-v1alpha1-flowschema-catch-all
    uid: 0b81d467-cd80-49cf-b8c1-0edcf3aa7a85
  severity: medium
  status: PASS
  warnings:
  - Note that this rule is only applicable in OpenShift Container Platform versions 4.7 and below.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Kubernetes API Server Maximum Retained Audit Logs
    OpenShift automatically rotates the log files. Retaining old log files ensures
    OpenShift Operators will have sufficient log data available for carrying out
    any investigation or correlation. For example, if the audit log size is set to
    100 MB and the number of retained log files is set to 10, OpenShift Operators
    would have approximately 1 GB of log data to use during analysis.
  id: xccdf_org.ssgproject.content_rule_api_server_audit_log_maxbackup
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxbackup"][0]'
    The output should return a value of 10 or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-audit-log-maxbackup
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-api-server-audit-log-maxbackup
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38402"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-audit-log-maxbackup
    uid: 139015fd-1a90-4c02-8880-40c0806d3f95
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Kubernetes API Server Maximum Audit Log Size
    OpenShift automatically rotates log files. Retaining old log files ensures that
    OpenShift Operators have sufficient log data available for carrying out any
    investigation or correlation. If you have set file size of 100 MB and the number of
    old log files to keep as 10, there would be approximately 1 GB of log data
    available for use in analysis.
  id: xccdf_org.ssgproject.content_rule_api_server_audit_log_maxsize
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxsize"]'
    The output should return a value of ["100"] or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-audit-log-maxsize
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-audit-log-maxsize
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38445"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-audit-log-maxsize
    uid: 79e18aa7-39ed-4809-920d-e6370f06a5bf
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Audit Log Path
    Auditing of the Kubernetes API Server is not enabled by default. Auditing the API Server
    provides a security-relevant chronological set of records documenting the sequence
    of activities that have affected the system by users, administrators, or other
    system components.
  id: xccdf_org.ssgproject.content_rule_api_server_audit_log_path
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-path"]'
    The output should return a valid audit log path.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-audit-log-path
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-api-server-audit-log-path
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38399"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-audit-log-path
    uid: 09d32cc4-433a-4b8d-bca1-3522fbb790f7
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    The authorization-mode cannot be AlwaysAllow
    The API Server, can be configured to allow all requests. This mode should not be used on any production cluster.
  id: xccdf_org.ssgproject.content_rule_api_server_auth_mode_no_aa
  instructions: |-
    To verify that the Node authorization mode is be configured and enabled in
    the apiserver configuration, run:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep '"authorization-mode":\[[^]]*"AlwaysAllow"'
    The output should be empty - the "authorization-mode" list does NOT contain the "AlwaysAllow" authorizer.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-auth-mode-no-aa
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-api-server-auth-mode-no-aa
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38371"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-auth-mode-no-aa
    uid: 5119776a-2228-4056-9fd2-e1b1b03188db
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure authorization-mode Node is configured
    The Node authorization mode only allows kubelets to read Secret,
    ConfigMap, PersistentVolume, and PersistentVolumeClaim objects
    associated with their nodes.
  id: xccdf_org.ssgproject.content_rule_api_server_auth_mode_node
  instructions: |-
    To verify that Node authorization mode is enabled, run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep '"authorization-mode":\[[^]]*"Node"'
    The output should show that the "authorization-mode" list contains the "Node" authorizer.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-auth-mode-node
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-auth-mode-node
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38448"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-auth-mode-node
    uid: 0023b488-f579-4ecc-81fa-41d97db7f447
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure authorization-mode RBAC is configured
    Role Based Access Control (RBAC) allows fine-grained control over the
    operations that different entities can perform on different objects in
    the cluster. Enabling RBAC is critical in regulating access to an
    OpenShift cluster as the RBAC rules specify, given a user, which operations
    can be executed over a set of namespaced or cluster-wide resources.
  id: xccdf_org.ssgproject.content_rule_api_server_auth_mode_rbac
  instructions: |-
    To verify that RBAC authorization mode is enabled, run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep '"authorization-mode":\[[^]]*"RBAC"'
    The output should show that the "authorization-mode" list contains the "RBAC" authorizer.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-auth-mode-rbac
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-cis-api-server-auth-mode-rbac
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38478"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-auth-mode-rbac
    uid: 39c79166-b6a3-4156-8a26-bf36c4cd2499
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable basic-auth-file for the API Server
    Basic authentication uses plaintext credentials for authentication.
    Currently the basic authentication credentials last indefinitely, and
    the password cannot be changed without restarting the API Server. The
    Basic Authentication is currently supported for convenience and is
    not intended for production workloads.
  id: xccdf_org.ssgproject.content_rule_api_server_basic_auth
  instructions: |-
    To verify that basic-auth-file is configured and enabled for the API server, run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["basic-auth-file"]'
    The output should be empty as OpenShift does not support basic authentication at all.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-basic-auth
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-basic-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38238"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-basic-auth
    uid: a988938a-9b3b-4d04-aaf8-fd25b9becad2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the bindAddress is set to a relevant secure port
    The OpenShift API server is served over HTTPS with authentication and authorization;
    the secure API endpoint is bound to 0.0.0.0:6443 by default. In OpenShift, the only
    supported way to access the API server pod is through the load balancer and then through
    the internal service.  The value is set by the bindAddress argument under the servingInfo
    parameter.
  id: xccdf_org.ssgproject.content_rule_api_server_bind_address
  instructions: |-
    Run the following command:
    oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.servingInfo["bindAddress"]'
    The output should return 0.0.0.0:6443.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-bind-address
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-bind-address
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38443"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-bind-address
    uid: 1c3ac71f-5730-42c5-b9cd-5166efe812ca
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Client Certificate Authority for the API Server
    API Server communication contains sensitive parameters that should remain
    encrypted in transit. Configure the API Server to serve only HTTPS traffic.
    If -clientCA is set, any request presenting a client
    certificate signed by one of the authorities in the client-ca-file
    is authenticated with an identity corresponding to the CommonName of
    the client certificate.
  id: xccdf_org.ssgproject.content_rule_api_server_client_ca
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["client-ca-file"]'
    The output should return a configured TLS CA certificate file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-client-ca
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-client-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38242"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-client-ca
    uid: 7f59fdf5-8c03-4e29-abc1-0cd1dcaf9907
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Encryption Provider Cipher
    aescbc is currently the strongest encryption provider, it should
    be preferred over other providers.
  id: xccdf_org.ssgproject.content_rule_api_server_encryption_provider_cipher
  instructions: |-
    Run the following command:
    $ oc get apiserver cluster -ojson | jq -r '.spec.encryption.type'
    The output should return aescdc as the encryption type.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-encryption-provider-cipher
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-api-server-encryption-provider-cipher
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38348"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-encryption-provider-cipher
    uid: ea3a7bc1-ebc2-4da5-9f92-3640e60f0ce5
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Encryption Provider
    etcd is a highly available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are
    sensitive in nature and should be encrypted at rest to avoid any
    disclosures.
  id: xccdf_org.ssgproject.content_rule_api_server_encryption_provider_config
  instructions: |-
    Run the following command:
    $ oc get apiserver cluster -ojson | jq -r '.spec.encryption.type'
    The output should return aescdc as the encryption type.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-encryption-provider-config
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-encryption-provider-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38450"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-encryption-provider-config
    uid: 680ab90c-5323-4327-a268-83943d1c1a3e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the etcd Certificate Authority for the API Server
    etcd is a highly-available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are
    sensitive in nature and should be protected by client authentication. This
    requires the API Server to identify itself to the etcd server using
    a SSL Certificate Authority file.
  id: xccdf_org.ssgproject.content_rule_api_server_etcd_ca
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["etcd-cafile"]'
    The output should return a configured CA certificate for ETCD.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-etcd-ca
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-api-server-etcd-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38355"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-etcd-ca
    uid: c3287e0d-d9bd-4856-9b0d-6d97c15f0424
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the etcd Certificate for the API Server
    etcd is a highly-available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are sensitive
    in nature and should be protected by client authentication. This requires the
    API Server to identify itself to the etcd server using a client certificate
    and key.
  id: xccdf_org.ssgproject.content_rule_api_server_etcd_cert
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."etcd-certfile"'
    The output should return a configured certificate file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-etcd-cert
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-api-server-etcd-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38381"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-etcd-cert
    uid: fe31c33e-cb1e-4548-b07b-4f00b66a3941
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the etcd Certificate Key for the API Server
    etcd is a highly-available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are sensitive
    in nature and should be protected by client authentication. This requires the
    API Server to identify itself to the etcd server using a client certificate
    and key.
  id: xccdf_org.ssgproject.content_rule_api_server_etcd_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."etcd-keyfile"'
    The output should return a configured certificate key file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-etcd-key
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-api-server-etcd-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38334"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-etcd-key
    uid: 046c6ceb-3187-4549-8562-01806a8898cc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the --kubelet-https argument is set to true
    Connections from the kube-apiserver to kubelets could potentially carry
    sensitive data such as secrets and keys. It is thus important to use
    in-transit encryption for any communication between the apiserver and
    kubelets.
  id: xccdf_org.ssgproject.content_rule_api_server_https_for_kubelet_conn
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["kubelet-https"]'
    The output should return true, or no output at all.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-https-for-kubelet-conn
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-api-server-https-for-kubelet-conn
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38415"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-https-for-kubelet-conn
    uid: f023f2cd-8141-4b82-9bf2-087a9978a46f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Use of the Insecure Bind Address
    If the API Server is bound to an insecure address the installation would
    be susceptible to unauthented and unencrypted access to the master node(s).
    The API Server does not perform authentication checking for insecure
    binds and the traffic is generally not encrypted.
  id: xccdf_org.ssgproject.content_rule_api_server_insecure_bind_address
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["insecure-bind-address"]'
    The output should be empty.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-insecure-bind-address
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-insecure-bind-address
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38239"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-insecure-bind-address
    uid: 8998e7c3-8638-4a9f-9050-5e1d07b17f93
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Prevent Insecure Port Access
    Configuring the API Server on an insecure port would allow unauthenticated
    and unencrypted access to your master node(s). It is assumed firewall rules
    will be configured to ensure this port is not reachable from outside
    the cluster, however as a defense in depth measure, OpenShift should not
    be configured to use insecure ports.
  id: xccdf_org.ssgproject.content_rule_api_server_insecure_port
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["insecure-port"]'
    The output should return 0.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-insecure-port
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-insecure-port
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38245"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-insecure-port
    uid: c536ecac-9c3a-40c9-ab21-5c6a49f90f00
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the kubelet Certificate Authority for the API Server
    Connections from the API Server to the kubelet are used for fetching logs
    for pods, attaching (through kubectl) to running pods, and using the kubelet
    port-forwarding functionality. These connections terminate at the kubelet
    HTTPS endpoint. By default, the API Server does not verify the kubelet serving
    certificate, which makes the connection subject to man-in-the-middle attacks,
    and unsafe to run over untrusted and/or public networks.
  id: xccdf_org.ssgproject.content_rule_api_server_kubelet_certificate_authority
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."kubelet-certificate-authority"'
    The output should return a configured certificate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-kubelet-certificate-authority
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-api-server-kubelet-certificate-authority
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38345"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-kubelet-certificate-authority
    uid: d0fe85e0-a32d-42cb-804f-b63adf50492d
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the kubelet Certificate File for the API Server
    By default the API Server does not authenticate itself to the kublet's
    HTTPS endpoints. Requests from the API Server are treated anonymously.
    Configuring certificate-based kubelet authentication ensures that the
    API Server authenticates itself to kubelets when submitting requests.
  id: xccdf_org.ssgproject.content_rule_api_server_kubelet_client_cert
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["kubelet-client-certificate"]'
    The output should return /etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-kubelet-client-cert
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-api-server-kubelet-client-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38459"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-kubelet-client-cert
    uid: 870cc565-0e51-4184-a488-105c53b568c5
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the kubelet Certificate Key for the API Server
    By default the API Server does not authenticate itself to the kubelet's
    HTTPS endpoints. Requests from the API Server are treated anonymously.
    Configuring certificate-based kubelet authentication ensures that the
    API Server authenticates itself to kubelets when submitting requests.
  id: xccdf_org.ssgproject.content_rule_api_server_kubelet_client_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["kubelet-client-key"]'
    The output should return /etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-kubelet-client-key
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-api-server-kubelet-client-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38406"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-kubelet-client-key
    uid: f19e7e41-2b97-4691-9f14-afad6dc4dd13
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure all admission control plugins are enabled
    Several hardening controls depend on certain API server admission plugins
    being enabled. Checking that no admission control plugins are disabled
    helps assert that all the critical admission control plugins are indeed
    enabled and providing the security benefits required.
  id: xccdf_org.ssgproject.content_rule_api_server_no_adm_ctrl_plugins_disabled
  instructions: |-
    To verify that the list of disabled admission plugins is empty, run the following command:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."disable-admission-plugins"'
    There should be no output.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-no-adm-ctrl-plugins-disabled
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-no-adm-ctrl-plugins-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38243"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-no-adm-ctrl-plugins-disabled
    uid: 7008ec6d-474e-4aa2-894d-d8401a116b1f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure the openshift-oauth-apiserver service uses TLS
    Connections between the kube-apiserver and the extension
    openshift-oauth-apiserver could potentially carry sensitive data such
    as secrets and keys. It is important to use in-transit encryption
    for any communication between the kube-apiserver and the extension
    openshift-apiserver.
  id: xccdf_org.ssgproject.content_rule_api_server_oauth_https_serving_cert
  instructions: |-
    Run the following command:
    $ oc -n openshift-oauth-apiserver describe secret serving-cert
    Verify that the serving-cert for the openshift-apiserver is type
    kubernetes.io/tls and that returned Data includes tls.crt
    and tls.key.
          Is it the case that The openshift-apiserver serving-cert is not set to type
    <tt>kubernetes.io/tls</tt> and that returned Data does not include <tt>tls.crt</tt>
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-oauth-https-serving-cert
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-api-server-oauth-https-serving-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38408"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-oauth-https-serving-cert
    uid: 9ac42509-1480-4936-886e-df521d65fcf6
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure the openshift-oauth-apiserver service uses TLS
    Connections between the kube-apiserver and the extension
    openshift-apiserver could potentially carry sensitive data such
    as secrets and keys. It is important to use in-transit encryption
    for any communication between the kube-apiserver and the extension
    openshift-apiserver.
  id: xccdf_org.ssgproject.content_rule_api_server_openshift_https_serving_cert
  instructions: |-
    Run the following command:
    $ oc -n openshift-apiserver describe secret serving-cert
    Verify that the serving-cert for the openshift-apiserver is type
    kubernetes.io/tls and that returned Data includes tls.crt
    and tls.key.
          Is it the case that The openshift-apiserver serving-cert is not set to type
    <tt>kubernetes.io/tls</tt> and that returned Data does not include <tt>tls.crt</tt>
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-openshift-https-serving-cert
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-openshift-https-serving-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38234"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-openshift-https-serving-cert
    uid: af5c6fbc-a56d-4280-b731-41088de774e2
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Profiling is protected by RBAC\nProfiling allows for the identification of specific performance bottlenecks.\nIt generates a significant amount of program data that could potentially be\nexploited to uncover system and program details. \nTo ensure the collected data is not exploited, profiling endpoints are secured\nvia RBAC (see cluster-debugger role). By default, the profiling endpoints are\naccessible only by users bound to cluster-admin or cluster-debugger role.\nProfiling can not be disabled."
  id: xccdf_org.ssgproject.content_rule_api_server_profiling_protected_by_rbac
  instructions: |-
    To verify that the cluster-debugger role is configured correctly,
    run the following command:
    $ oc get clusterroles cluster-debugger -o jsonpath='{.rules[0].nonResourceURLs}'
    and verify that the /metrics path is included there.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-profiling-protected-by-rbac
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-api-server-profiling-protected-by-rbac
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38374"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-profiling-protected-by-rbac
    uid: bf9131ec-51af-4e09-8c69-65bcb64fe14c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the API Server Minimum Request Timeout
    Setting global request timout allows extending the API Server request
    timeout limit to a duration appropriate to the user's connection speed.  By
    default, it is set to 1800 seconds which might not be suitable for some
    environments. Setting the limit too low may result in excessive timeouts,
    and a limit that is too large may exhaust the API Server resources making
    it prone to Denial-of-Service attack. It is recommended to set this limit
    as appropriate and change the default limit of 1800 seconds only if needed.
  id: xccdf_org.ssgproject.content_rule_api_server_request_timeout
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["min-request-timeout"]'
    The output should return .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-request-timeout
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-api-server-request-timeout
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38265"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-request-timeout
    uid: e32e3da2-9e8c-4d5c-9874-07f8ba60b832
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the service-account-lookup argument is set to true
    If service-account-lookup is not enabled, the apiserver
    only verifies that the authentication token is valid, and
    does not validate that the service account token mentioned
    in the request is actually present in etcd. This allows
    using a service account token even after the corresponding
    service account is deleted. This is an example of time of
    check to time of use security issue.
  id: xccdf_org.ssgproject.content_rule_api_server_service_account_lookup
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -o json | \
        jq -r '.data["config.yaml"]' | \
        jq -r '.apiServerArguments["service-account-lookup"]'
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-service-account-lookup
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-api-server-service-account-lookup
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38269"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-service-account-lookup
    uid: 0d87725f-b9d8-4ecc-9a13-5c973d89ed0a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Service Account Public Key for the API Server
    By default if no service-account-key-file is specified
    to the apiserver, it uses the private key from the TLS serving
    certificate to verify service account tokens. To ensure that the
    keys for service account tokens are rotated as needed, a
    separate public/private key pair should be used for signing service
    account tokens.
  id: xccdf_org.ssgproject.content_rule_api_server_service_account_public_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .serviceAccountPublicKeyFiles
    The output should return configured certificate key file(s).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-service-account-public-key
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-api-server-service-account-public-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38428"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-service-account-public-key
    uid: 672c9c18-2527-4906-a420-ba7b8175a887
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Certificate for the API Server
    API Server communication contains sensitive parameters that should remain
    encrypted in transit. Configure the API Server to serve only HTTPS
    traffic.
  id: xccdf_org.ssgproject.content_rule_api_server_tls_cert
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."tls-cert-file"'
    The output should return /etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-tls-cert
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-tls-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38231"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-tls-cert
    uid: e1a0ed4a-1596-4898-b089-e3b079382e29
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Use Strong Cryptographic Ciphers on the API Server
    TLS ciphers have had a number of known vulnerabilities and weaknesses,
    which can reduce the protection provided. By default, OpenShift supports
    a number of TLS ciphersuites including some that have security concerns,
    weakening the protection provided.
  id: xccdf_org.ssgproject.content_rule_api_server_tls_cipher_suites
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.servingInfo["cipherSuites"]'
    Verify that the set of ciphers contains only the following:

    "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256",
    "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-tls-cipher-suites
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-api-server-tls-cipher-suites
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38397"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-tls-cipher-suites
    uid: ea3218e8-57e5-4fc4-a38f-87cefa61908a
  severity: medium
  status: PASS
  warnings:
  - Once configured, API Server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Certificate Key for the API Server
    API Server communication contains sensitive parameters that should remain
    encrypted in transit. Configure the API Server to serve only HTTPS
    traffic.
  id: xccdf_org.ssgproject.content_rule_api_server_tls_private_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."tls-private-key-file"'
    The output should return /etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-tls-private-key
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-api-server-tls-private-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38272"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-tls-private-key
    uid: 991dced5-0aec-4aa4-b332-29c5f37fbf32
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Token-based Authentication
    The token-based authentication utilizes static tokens to authenticate
    requests to the API Server. The tokens are stored in clear-text in a file
    on the API Server, and cannot be revoked or rotated without restarting the
    API Server.
  id: xccdf_org.ssgproject.content_rule_api_server_token_auth
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments' | grep "token-auth-file"
    The output should be empty as OpenShift does not support token-based authentication at all.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-token-auth
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-api-server-token-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38244"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-api-server-token-auth
    uid: 7731f70a-01df-4059-8ddf-76b3010e52d2
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that Audit Log Forwarding Is Enabled
    Retaining logs ensures the ability to go back in time to investigate or correlate any events.
    Offloading audit logs from the cluster ensures that an attacker that has access to the cluster will not be able to
    tamper with the logs because of the logs being stored off-site.
  id: xccdf_org.ssgproject.content_rule_audit_log_forwarding_enabled
  instructions: |-
    Run the following command:
    oc get clusterlogforwarders instance -n openshift-logging -ojson | jq -r '.spec.pipelines[].inputRefs | contains(["audit"])'
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-log-forwarding-enabled
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-audit-log-forwarding-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38431"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-audit-log-forwarding-enabled
    uid: 89f57d4a-9548-426f-a2d4-7e6fcbf9fff8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the cluster's audit profile is properly set
    Logging is an important detective control for all systems, to detect potential
    unauthorised access.
  id: xccdf_org.ssgproject.content_rule_audit_profile_set
  instructions: |-
    Run the following command to retrieve the current audit profile:
    $ oc get apiservers cluster -ojsonpath='{.spec.audit.profile}'
    Make sure the profile returned matches the one that should be used.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-profile-set
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-audit-profile-set
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38366"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-audit-profile-set
    uid: 86a10d84-924f-4b7f-a921-5fd11c425d80
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the CNI in use supports Network Policies
    Kubernetes network policies are enforced by the CNI plugin in use. As such
    it is important to ensure that the CNI plugin supports both Ingress and
    Egress network policies.
  id: xccdf_org.ssgproject.content_rule_configure_network_policies
  instructions: |-
    Verify on OpenShift that the NetworkPolicy plugin is being used:
    $ oc explain networkpolicy
    The resulting output should be an explanation of the NetworkPolicy resource.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-network-policies
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-configure-network-policies
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38404"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-configure-network-policies
    uid: af92d5e0-cc38-43fd-8ea3-cf12b5788f7e
  severity: high
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that application Namespaces have Network Policies defined.
    Running different applications on the same Kubernetes cluster creates a risk of one
    compromised application attacking a neighboring application. Network segmentation is
    important to ensure that containers can communicate only with those they are supposed
    to. When a network policy is introduced to a given namespace, all traffic not allowed
    by the policy is denied. However, if there are no network policies in a namespace all
    traffic will be allowed into and out of the pods in that namespace.
  id: xccdf_org.ssgproject.content_rule_configure_network_policies_namespaces
  instructions: |-
    Verify on OpenShift namespaces that network policies are in use:
    $ oc get networkpolicy --all-namespaces
    Ensure that each namespace defined in the cluster has at least one NetworkPolicy.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-network-policies-namespaces
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-cis-configure-network-policies-namespaces
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38471"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-configure-network-policies-namespaces
    uid: 70331190-48bb-48c9-9990-05f7c634d006
  severity: high
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Controller insecure port argument is unset
    The Controller Manager API service is used for health and metrics
    information and is available without authentication or encryption. As such, it
    should only be bound to a localhost interface to minimize the cluster's
    attack surface.
  id: xccdf_org.ssgproject.content_rule_controller_insecure_port_disabled
  instructions: |-
    To verify that port is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson |   jq -r '.data["config.yaml"]' | jq '.extendedArguments["port"][]'
    Verify that it's disabled (the value is 0).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-insecure-port-disabled
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-controller-insecure-port-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38438"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-controller-insecure-port-disabled
    uid: 6ff76755-8fee-4159-b4c6-24cab90e994b
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the RotateKubeletServerCertificate argument is set
    Enabling kubelet certificate rotation causes the kubelet to both request
    a serving certificate after bootstrapping its client credentials and rotate the
    certificate as its existing credentials expire. This automated periodic rotation
    ensures that there are no downtimes due to expired certificates and thus
    addressing the availability in the C/I/A security triad.
  id: xccdf_org.ssgproject.content_rule_controller_rotate_kubelet_server_certs
  instructions: |-
    To verify that RotateKubeletServerCertificate is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["feature-gates"]'
    The output should return RotateKubeletServerCertificate=true.
          Is it the case that <tt>RotateKubeletServerCertificate</tt> argument is set to <tt>false</tt> in the
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-rotate-kubelet-server-certs
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-controller-rotate-kubelet-server-certs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38467"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-controller-rotate-kubelet-server-certs
    uid: 65099e43-390b-4ff5-aa19-e313d706adcd
  severity: medium
  status: PASS
  warnings:
  - This recommendation only applies if you let kubelets get their certificates from the API Server. In case your certificates come from an outside Certificate Authority/tool (e.g. Vault) then you need to take care of rotation yourself
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Controller secure-port argument is set
    The Controller Manager API service is used for health and metrics
    information and is available without authentication or encryption. As such, it
    should only be bound to a localhost interface to minimize the cluster's
    attack surface.
  id: xccdf_org.ssgproject.content_rule_controller_secure_port
  instructions: |-
    To verify that secure-port is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson |   jq -r '.data["config.yaml"]' | jq '.extendedArguments["secure-port"][]'
    Verify that it's using an appropriate port (the value is not 0).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-secure-port
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-controller-secure-port
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38229"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-controller-secure-port
    uid: b8ca0b70-de91-4a3a-8e23-7d3517692483
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Service Account Certificate Authority Key for the Controller Manager
    Service accounts authenticate to the API using tokens signed by a private RSA
    key. The authentication layer verifies the signature using a matching public RSA key.
    Configuring the certificate authority file ensures that the API server's signing
    certificates are validated.
  id: xccdf_org.ssgproject.content_rule_controller_service_account_ca
  instructions: |-
    To verify that root-ca-file is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["root-ca-file"]'
    The output should return a configured certificate authority file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-service-account-ca
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-controller-service-account-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38424"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-controller-service-account-ca
    uid: baf512a7-5439-4856-9fc1-ebd2a6a9eb05
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Service Account Private Key for the Controller Manager
    By default if no private key file is specified to the
    API Server, the API Server uses the private key from the TLS serving
    certificate to verify service account tokens. To ensure that the keys
    for service account tokens could be rotated as needed, a separate
    public/private key pair should be used for signing service account
    tokens.
  id: xccdf_org.ssgproject.content_rule_controller_service_account_private_key
  instructions: |-
    To verify that service-account-private-key-file is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["service-account-private-key-file"]'
    The output should return a configured private key file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-service-account-private-key
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-controller-service-account-private-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38304"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-controller-service-account-private-key
    uid: 1db4961c-8812-4190-8dc9-e5bb0ec8c589
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that use-service-account-credentials is enabled
    The controller manager creates a service account per controller in
    kube-system namespace, generates an API token and credentials for it,
    then builds a dedicated API client with that service account credential
    for each controller loop to use. Setting the
    use-service-account-credentials to true runs each
    control loop within the contoller manager using a separate service
    account credential. When used in combination with RBAC, this ensures
    that the control loops run with the minimum permissions required to
    perform their intended tasks.
  id: xccdf_org.ssgproject.content_rule_controller_use_service_account
  instructions: |-
    To verify that service-account-credentials is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["use-service-account-credentials"]'
    The value of use-service-account-credentials should be true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-use-service-account
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-controller-use-service-account
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38422"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-controller-use-service-account
    uid: 52ab76a1-7597-45b5-92c4-8703129c1884
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable etcd Self-Signed Certificates
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection. Using self-signed
    certificates ensures that the certificates are never validated
    against a certificate authority and could lead to compromised
    and invalidated data.
  id: xccdf_org.ssgproject.content_rule_etcd_auto_tls
  instructions: |-
    Run the following command:
    $ oc get cm/etcd-pod -n openshift-etcd -o yaml
    The etcd pod configuration contained in the configmap should not
    contain the --auto-tls=true flag.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-auto-tls
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-etcd-auto-tls
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38284"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-auto-tls
    uid: 8a7bd151-e8c6-4207-b746-ffeab3eaa96d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Client Certificate Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_cert_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep -E "\-\-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.crt"
    Verify that there is a certificate configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-cert-file
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-etcd-cert-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38325"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-cert-file
    uid: eeab6a7e-bf8d-4ae9-be67-72717010ff91
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable The Client Certificate Authentication
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_client_cert_auth
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-client-cert-auth="
    The parameter should be set to true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-client-cert-auth
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-etcd-client-cert-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38394"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-client-cert-auth
    uid: 6f3c5a9c-a0c5-4e32-96e3-e016853989b9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Key File Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_key_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.key"
    Verify that there is a private key configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-key-file
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-etcd-key-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38360"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-key-file
    uid: 124fd24c-ac7a-4d35-ac48-10ed870649a7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable etcd Peer Self-Signed Certificates
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection. Using self-signed
    certificates ensures that the certificates are never validated
    against a certificate authority and could lead to compromised
    and invalidated data.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_auto_tls
  instructions: |-
    Run the following command:
    $ oc get cm/etcd-pod -n openshift-etcd -o yaml
    The etcd pod configuration contained in the configmap should not
    contain the --peer-auto-tls=true flag.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-auto-tls
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-etcd-peer-auto-tls
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38327"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-peer-auto-tls
    uid: 7e79d9d2-a1d9-4c9a-b37b-fef5aab097de
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Peer Client Certificate Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_cert_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.crt"
    Verify that there is a certificate configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-cert-file
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-etcd-peer-cert-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38227"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-peer-cert-file
    uid: 8274107f-6e6d-42e7-874a-9f0c3092c7cd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable The Peer Client Certificate Authentication
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_client_cert_auth
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-peer-client-cert-auth="
    The parameter should be set to true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-client-cert-auth
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-etcd-peer-client-cert-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38337"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-peer-client-cert-auth
    uid: bc18ecc3-d11f-4d66-a0f0-18bcb8c45bc4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Peer Key File Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_key_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.key"
    Verify that there is a private key configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-key-file
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-etcd-peer-key-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38420"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-etcd-peer-key-file
    uid: 83d9099a-07a6-4cc5-9317-b088f1582e59
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Worker Proxy Kubeconfig File
    The kubeconfig file for kube-proxy provides permissions to the kube-proxy service.
    The proxy kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.

    The file is provided via a ConfigMap mount, so the kubelet itself makes sure that the
    file permissions are appropriate for the container taking it into use.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_proxy_kubeconfig
  instructions: |-
    Run the following command:

     $ for i in $(oc get pods -n openshift-sdn -l app=sdn -oname)
       do
          oc exec -n openshift-sdn $i -- stat -Lc %U:%G /config/kube-proxy-config.yaml
       done

    The output should be root:root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-proxy-kubeconfig
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-file-groupowner-proxy-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38418"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-file-groupowner-proxy-kubeconfig
    uid: 968a519a-1fa6-429b-9cfc-5deb0f176bc4
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Worker Proxy Kubeconfig File
    The kubeconfig file for kube-proxy provides permissions to the kube-proxy service.
    The proxy kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.

    The file is provided via a ConfigMap mount, so the kubelet itself makes sure that the
    file permissions are appropriate for the container taking it into use.
  id: xccdf_org.ssgproject.content_rule_file_owner_proxy_kubeconfig
  instructions: |-
    Run the following command:

     $ for i in $(oc get pods -n openshift-sdn -l app=sdn -oname)
       do
          oc exec -n openshift-sdn $i -- stat -Lc %U:%G /config/kube-proxy-config.yaml
       done

    The output should be root:root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-proxy-kubeconfig
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-cis-file-owner-proxy-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38412"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-file-owner-proxy-kubeconfig
    uid: 8e69b6aa-675b-4b9b-a48d-88f4410334bc
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Worker Proxy Kubeconfig File
    The kube-proxy kubeconfig file controls various parameters of the kube-proxy
    service in the worker node. If used, you should restrict its file permissions
    to maintain the integrity of the file. The file should be writable by only
    the administrators on the system.

    The kube-proxy runs with the kubeconfig parameters configured as
    a Kubernetes ConfigMap instead of a file. In this case, there is no proxy
    kubeconfig file. But appropriate permissions still need to be set in the
    ConfigMap mount.
  id: xccdf_org.ssgproject.content_rule_file_permissions_proxy_kubeconfig
  instructions: |-
    Run the following command:
    $ oc get -nopenshift-sdn ds sdn -ojson | jq -r '.spec.template.spec.volumes[] | select(.configMap.name == "sdn-config") | .configMap.defaultMode'
    The output should return a value of 420.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-proxy-kubeconfig
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-file-permissions-proxy-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38233"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-file-permissions-proxy-kubeconfig
    uid: a2154b08-1e16-4ca0-981b-5762fa4810bc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Apply Security Context to Your Pods and Containers
    A security context defines the operating system security settings (uid, gid,
    capabilities, SELinux role, etc..) applied to a container. When designing your
    containers and pods, make sure that you configure the security context for your
    pods, containers, and volumes. A security context is a property defined in the
    deployment yaml. It controls the security parameters that will be assigned to
    the pod/container/volume. There are two levels of security context: pod level
    security context, and container level security context.
  id: xccdf_org.ssgproject.content_rule_general_apply_scc
  instructions: |-
    Review the pod definitions in your cluster and verify that you have security
    contexts defined as appropriate.  OpenShift's Security Context Constraint
    feature is on by default in OpenShift 4 and applied to all pods deployed. SCC
    selection is determined by a combination of the values in the securityContext
    and the rolebindings for the account deploying the pod.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-apply-scc
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-general-apply-scc
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38362"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-general-apply-scc
    uid: f9e93833-51cd-4772-be7c-05b8aca0920f
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Manage Image Provenance Using ImagePolicyWebhook
    Image Policy ensures that only approved container images are allowed to be ran on the OpenShift platform.
  id: xccdf_org.ssgproject.content_rule_general_configure_imagepolicywebhook
  instructions: |-
    To ensure that an image policy is configured, review the output
    returned from the following command:
    $ oc get image.config.openshift.io/cluster -o yaml
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-configure-imagepolicywebhook
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-general-configure-imagepolicywebhook
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38228"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-general-configure-imagepolicywebhook
    uid: 4accadcc-67b3-43b3-a8c5-64a2b835ff71
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    The default namespace should not be used
    Resources in a Kubernetes cluster should be segregated by namespace, to allow
    for security controls to be applied at that level and to make it easier to
    manage resources.
  id: xccdf_org.ssgproject.content_rule_general_default_namespace_use
  instructions: |-
    Run the following command to list objects in the default namespace:
    $ oc get all -n default
    The only entries there should be system-managed resources such as the
    kubernetes and openshift service.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-default-namespace-use
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-general-default-namespace-use
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38339"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-general-default-namespace-use
    uid: f183b386-b26c-4528-9e42-86f0cdf7886f
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Seccomp Profile Pod Definitions
    Seccomp (secure computing mode) is used to restrict the set of system calls
    applications can make, allowing cluster administrators greater control over
    the security of workloads running in the cluster. Kubernetes disables
    seccomp profiles by default for historical reasons. You should enable it to
    ensure that the workloads have restricted actions available within the
    container.
  id: xccdf_org.ssgproject.content_rule_general_default_seccomp_profile
  instructions: |-
    In OpenShift 4, CRI-O is the supported runtime. CRI-O runs unconfined by
    default in order to meet CRI conformance criteria.  On RHEL CoreOS, the
    default seccomp policy is associated with CRI-O and stored in
    /etc/crio/seccomp.json.  The default profile is applied when the user asks
    for the runtime/default profile via annotation to the pod and when the
    associated SCC allows use of the specified seccomp profile.

    Configuration of allowable seccomp profiles is managed through OpenShift
    Security Context Constraints.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-default-seccomp-profile
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-general-default-seccomp-profile
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38314"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-general-default-seccomp-profile
    uid: 3e446996-6e0f-441f-b777-6e54f95d5006
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Create administrative boundaries between resources using namespaces
    Limiting the scope of user permissions can reduce the impact of mistakes or
    malicious activities. A Kubernetes namespace allows you to partition created
    resources into logically named groups. Resources created in one namespace can
    be hidden from other namespaces. By default, each resource created by a user
    in Kubernetes cluster runs in a default namespace, called default. You can
    create additional namespaces and attach resources and users to them. You can
    use Kubernetes Authorization plugins to create policies that segregate access
    to namespace resources between different users.
  id: xccdf_org.ssgproject.content_rule_general_namespaces_in_use
  instructions: |-
    OpenShift projects wrap Kubernetes namespaces and are used by default in
    OpenShift 4.  Run the following command and review the namespaces created in
    the cluster.  $ oc get namespaces Ensure that the namespaces are
    the ones you need and are adequately administered.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-namespaces-in-use
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-general-namespaces-in-use
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38316"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-general-namespaces-in-use
    uid: fd827148-a4e0-42b2-b2cf-c08dc24222ae
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Configure An Identity Provider\n\n              \nWith any authentication mechanism the ability to revoke credentials if they\nare compromised or no longer required, is a key control. Kubernetes client\ncertificate authentication does not allow for this due to a lack of support\nfor certificate revocation.\n\n              \nOpenShift's built-in OAuth server allows credential revocation by relying on\nthe Identity provider, as well as giving the administrators the ability to\nrevoke any tokens given to a specific user.\n\n            "
  id: xccdf_org.ssgproject.content_rule_idp_is_configured
  instructions: "Run the following command to list the identity providers configured:\n$ oc get oauths cluster -ojsonpath='{.spec.identityProviders}' | jq \nMake sure that there exists at least one item referenced by the above path."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: idp-is-configured
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-idp-is-configured
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38279"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-idp-is-configured
    uid: b23adb22-9c43-47b6-93d1-cf7c398d2fb8
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The kubelet Client Certificate Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_tls_cert
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["kubelet-client-certificate"]'
    Verify that a client certificate is configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-tls-cert
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-kubelet-configure-tls-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38441"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-kubelet-configure-tls-cert
    uid: d5cc003f-f5b4-42b8-8fd8-9ea9238da311
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The kubelet Server Key Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_tls_key
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["kubelet-client-key"]'
    Verify that a client certificate is configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-tls-key
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-kubelet-configure-tls-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38263"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-kubelet-configure-tls-key
    uid: 6c298446-2b27-423e-8003-6eb824fc7975
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Disable the Read-Only Port
    OpenShift disables the read-only port (10255) on all nodes by setting the
    read-only port kubelet flag to 0. This ensures only
    authenticated connections are able to receive information about the OpenShift
    system.
  id: xccdf_org.ssgproject.content_rule_kubelet_disable_readonly_port
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["kubelet-read-only-port"]'
    The output should be 0.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-disable-readonly-port
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-kubelet-disable-readonly-port
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38318"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-kubelet-disable-readonly-port
    uid: 883deb27-1fd3-430c-8941-f8dcc06256bc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure A Unique CA Certificate for etcd
    The Kubernetes API server and etcd utilize separate CA certificates in
    OpenShift.  This ensures that the etcd data is still protected in the event
    that the API server CA is compromised.
  id: xccdf_org.ssgproject.content_rule_etcd_unique_ca
  instructions: |-
    Run the following command:
    oc debug node/$NODE -- diff /host/etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-serving-ca/ca-bundle.crt /host/etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/client-ca/ca-bundle.crt
    where $NODE is a master node. If you don't see diff output
    the differences, you might have a compromise and should isolate the cluster.
    OpenShift will use separate PKIs by default.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-unique-ca
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-etcd-unique-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39702"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-etcd-unique-ca
    uid: 86db5945-7f3d-468f-aaa4-4b7fad8e6405
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Container Network Interface Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_cni_conf
  instructions: |-
    To check the group ownership of /etc/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/cni/net.d/*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-cni-conf
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-groupowner-cni-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39763"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-cni-conf
    uid: f448d5d8-5274-4665-bd22-62d2dd554908
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Controller Manager Kubeconfig File
    The Controller Manager's kubeconfig contains information about how the
    component will access the API server. You should set its file ownership to
    maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_controller_manager_kubeconfig
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-controller-manager-kubeconfig
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-groupowner-controller-manager-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39708"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-controller-manager-kubeconfig
    uid: 85fbc2ca-fcc5-4e0d-a29a-1eccaf02bf92
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Etcd Database Directory
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_dir
  instructions: |-
    To check the group ownership of /var/lib/etcd/member/,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-data-dir
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-groupowner-etcd-data-dir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39717"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-etcd-data-dir
    uid: 7651a040-607d-4754-a410-4989893f67dd
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Etcd Write-Ahead-Log Files
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_files
  instructions: |-
    To check the group ownership of /var/lib/etcd/member/wal/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/wal/*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-data-files
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-groupowner-etcd-data-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39672"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-etcd-data-files
    uid: 8d84a54a-27a6-4a08-8eb3-73cea8bb611e
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The etcd Member Pod Specification File
    The etcd pod specification file controls various parameters that
    set the behavior of the etcd service in the master node. etcd is a
    highly-available key-value store which Kubernetes uses for persistent
    storage of all of its REST API object. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_member
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-member
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-groupowner-etcd-member
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39638"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-etcd-member
    uid: ce81bca7-1e33-4fe7-bff5-337d57194fef
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Etcd PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by the system administrator.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_pki_cert_files
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.crt\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:24Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:24Z"
    name: ocp4-cis-node-master-file-groupowner-etcd-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39785"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-etcd-pki-cert-files
    uid: 50b05d46-fe18-467f-89e1-d02fda5ff1aa
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocations
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ip_allocations
  instructions: |-
    To check the group ownership of /var/lib/cni/networks/openshift-sdn/.*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/cni/networks/openshift-sdn/.*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ip-allocations
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-groupowner-ip-allocations
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39670"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ip-allocations
    uid: c9aa3ec9-4bcc-4c34-9e1c-a6ae2a605c1b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes API Server Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes API Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kube_apiserver
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kube-apiserver
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-groupowner-kube-apiserver
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39726"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-kube-apiserver
    uid: f66a3655-ed8f-4e05-b11c-05ed9f58506a
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes Controller Manager Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes Controller Manager Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kube_controller_manager
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kube-controller-manager
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-groupowner-kube-controller-manager
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39655"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-kube-controller-manager
    uid: 69ecb6f1-7b0f-4a23-aa93-246a9adbad9c
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes Scheduler Pod Specification File
    The Kubernetes Specification file contains information about the configuration of the
    Kubernetes scheduler that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kube_scheduler
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kube-scheduler
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-groupowner-kube-scheduler
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39678"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-kube-scheduler
    uid: 970cb6e6-0022-4e62-972c-aafc54b3c006
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubelet Configuration File
    The kubelet configuration file contains information about the configuration of the
    OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kubelet_conf
  instructions: |-
    To check the group ownership of /etc/kubernetes/kubelet.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet.conf
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kubelet-conf
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-groupowner-kubelet-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39679"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-kubelet-conf
    uid: 779b11e5-6683-42e3-a460-32eca23aac0a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Admin Kubeconfig Files
    There are various kubeconfig files that can be used by the administrator,
    defining various settings for the administration of the cluster. These files
    contain credentials that can be used to control the cluster and are needed
    for disaster recovery and each kubeconfig points to a different endpoint in
    the cluster. You should restrict its file permissions to maintain the
    integrity of the kubeconfig file as an attacker who gains access to these
    files can take over the cluster.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_master_admin_kubeconfigs
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-master-admin-kubeconfigs
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-groupowner-master-admin-kubeconfigs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39764"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-master-admin-kubeconfigs
    uid: 1b81d02b-406a-4e4e-ad4b-abc92e3d19f2
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Multus Container Network Interface Plugin Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_multus_conf
  instructions: |-
    To check the group ownership of /var/run/multus/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/multus/cni/net.d/*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-multus-conf
    creationTimestamp: "2021-07-13T16:58:15Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:15Z"
    name: ocp4-cis-node-master-file-groupowner-multus-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39625"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-multus-conf
    uid: 166427a0-0ac0-4518-bfea-aca38f4da0f4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by the system administrator.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_cert_files
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/.*/.*/.*/tls.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/.*/.*/.*/tls.crt\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-openshift-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-groupowner-openshift-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39738"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-openshift-pki-cert-files
    uid: b24071b8-224d-47ad-8f19-5e52e9af9bca
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift PKI Private Key Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by root:root.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_key_files
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.key,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.key\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-openshift-pki-key-files
    creationTimestamp: "2021-07-13T16:58:15Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:15Z"
    name: ocp4-cis-node-master-file-groupowner-openshift-pki-key-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39624"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-openshift-pki-key-files
    uid: dbe533c6-aece-4f2d-a655-db5de4799db5
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift SDN CNI Server Config
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_openshift_sdn_cniserver_config
  instructions: |-
    To check the group ownership of /var/run/openshift-sdn/cniserver/config.json,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/openshift-sdn/cniserver/config.json
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-openshift-sdn-cniserver-config
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-groupowner-openshift-sdn-cniserver-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39696"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-openshift-sdn-cniserver-config
    uid: 74868e41-fda1-4431-8dd2-dc3b0514e972
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Configuration Database
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db
  instructions: |-
    To check the group ownership of /etc/openvswitch/conf.db,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/conf.db
    If properly configured, the output should indicate the following group-owner:
    hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-conf-db
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-groupowner-ovs-conf-db
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39632"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ovs-conf-db
    uid: b3849273-5869-4e34-a6f5-6c4c166b659a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Configuration Database Lock
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db_lock
  instructions: |-
    To check the group ownership of /etc/openvswitch/.conf.db.~lock~,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/.conf.db.~lock~
    If properly configured, the output should indicate the following group-owner:
    hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-conf-db-lock
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-groupowner-ovs-conf-db-lock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39752"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ovs-conf-db-lock
    uid: 02f467d7-cfc2-4c75-a78b-c58f98d2de75
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Process ID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_pid
  instructions: |-
    To check the group ownership of /var/run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:


    $ oc debug node/$NODE_NAME


    At the sh-4.4# prompt, run:


    # chroot /host


    Then,run the command:

    $ ls -l /var/run/openvswitch/ovs-vswitchd.pid

    If properly configured, the output should indicate one of the following group-owners:
    openvswitch or hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-pid
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-groupowner-ovs-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39663"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ovs-pid
    uid: 03dc9b75-89ee-4557-b3c2-0de8689e42a2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Persistent System ID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_sys_id_conf
  instructions: |-
    To check the group ownership of /etc/openvswitch/system-id.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/system-id.conf
    If properly configured, the output should indicate the following group-owner:
    hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-sys-id-conf
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-groupowner-ovs-sys-id-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39647"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ovs-sys-id-conf
    uid: 97c06a2c-ec05-4dbb-a633-dfb72ddd88f5
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Daemon PID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_vswitchd_pid
  instructions: |-
    To check the group ownership of /run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:


    $ oc debug node/$NODE_NAME


    At the sh-4.4# prompt, run:


    # chroot /host


    Then,run the command:

    $ ls -l /run/openvswitch/ovs-vswitchd.pid

    If properly configured, the output should indicate one of the following group-owners:
    openvswitch or hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-vswitchd-pid
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-groupowner-ovs-vswitchd-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39662"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ovs-vswitchd-pid
    uid: 5cd36f8e-8b80-4b4c-954c-9ce2f67a261c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Database Server PID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovsdb_server_pid
  instructions: |-
    To check the group ownership of /run/openvswitch/ovsdb-server.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:


    $ oc debug node/$NODE_NAME


    At the sh-4.4# prompt, run:


    # chroot /host


    Then,run the command:

    $ ls -l /run/openvswitch/ovsdb-server.pid

    If properly configured, the output should indicate one of the following group-owners:
    openvswitch or hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovsdb-server-pid
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-groupowner-ovsdb-server-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39688"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-ovsdb-server-pid
    uid: f89919d4-9b8d-4990-92fa-623037f3b0fb
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes Scheduler Kubeconfig File
    The kubeconfig for the Scheduler contains paramters for the scheduler
    to access the Kube API.
    You should set its file ownership to maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_scheduler_kubeconfig
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-scheduler-kubeconfig
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-groupowner-scheduler-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39745"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-scheduler-kubeconfig
    uid: f35b1276-23aa-4ffb-8e95-3afb951ee6b2
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns the Worker Certificate Authority File
    The worker certificate authority file contains the certificate authority
    certificate for an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_worker_ca
  instructions: |-
    To check the group ownership of /etc/kubernetes/kubelet-ca.crt,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet-ca.crt
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-worker-ca
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-groupowner-worker-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39755"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-worker-ca
    uid: bde39295-fa1b-4ad2-a700-e8f815de1343
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Worker Kubeconfig File
    The worker kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_worker_kubeconfig
  instructions: |-
    To check the group ownership of /var/lib/kubelet/kubeconfig,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/kubelet/kubeconfig
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-worker-kubeconfig
    creationTimestamp: "2021-07-13T16:58:15Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:15Z"
    name: ocp4-cis-node-master-file-groupowner-worker-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39627"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-worker-kubeconfig
    uid: d12af370-37cb-45a6-8e8a-55a6dcbbf65a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Node Service File
    The /etc/systemd/system/kubelet.service
    file contains information about the configuration of the
    OpenShift node service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_worker_service
  instructions: |-
    To check the group ownership of /etc/systemd/system/kubelet.service,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/systemd/system/kubelet.service
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-worker-service
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-groupowner-worker-service
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39660"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-groupowner-worker-service
    uid: 4caae453-d208-409d-be13-40b9220c9cc0
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Container Network Interface Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_cni_conf
  instructions: |-
    To check the ownership of /etc/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/cni/net.d/*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-cni-conf
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-owner-cni-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39665"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-cni-conf
    uid: 441e79f8-335a-4fce-8cb4-52329e7dde02
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Controller Manager Kubeconfig File
    The Controller Manager's kubeconfig contains information about how the
    component will access the API server. You should set its file ownership to
    maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_owner_controller_manager_kubeconfig
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-controller-manager-kubeconfig
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-owner-controller-manager-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39703"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-controller-manager-kubeconfig
    uid: 0e3dfca8-e72f-4412-9937-7eb106c5aab4
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd Database Directory
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_data_dir
  instructions: |-
    To check the ownership of /var/lib/etcd/member/,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-data-dir
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-owner-etcd-data-dir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39671"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-etcd-data-dir
    uid: a0e70eec-8f91-4c12-835e-9321fb52e430
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd Write-Ahead-Log Files
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_data_files
  instructions: |-
    To check the ownership of /var/lib/etcd/member/wal/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/wal/*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-data-files
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-owner-etcd-data-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39691"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-etcd-data-files
    uid: df4cb04c-c392-4e43-9c04-9f3fa491a1d3
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd Member Pod Specification File
    The etcd pod specification file controls various parameters that
    set the behavior of the etcd service in the master node. etcd is a
    highly-available key-value store which Kubernetes uses for persistent
    storage of all of its REST API object. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_member
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-member
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-etcd-member
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39635"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-etcd-member
    uid: 1865464c-0f64-42ed-bda7-e48c9b444f56
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by the system administrator.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_pki_cert_files
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.crt\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-owner-etcd-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39687"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-etcd-pki-cert-files
    uid: 16657be3-28c5-4f26-9769-2149a7e76fd0
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocations
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ip_allocations
  instructions: |-
    To check the ownership of /var/lib/cni/networks/openshift-sdn/.*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/cni/networks/openshift-sdn/.*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ip-allocations
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-ip-allocations
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39646"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ip-allocations
    uid: f091a9dc-d7d7-44e8-86ee-992b666d8a8d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes API Server Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes API Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kube_apiserver
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kube-apiserver
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-kube-apiserver
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39631"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-kube-apiserver
    uid: 7a00f115-7267-4427-8ba1-a0578f7003b1
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes Controller Manager Pod Specificiation File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes Controller Manager Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kube_controller_manager
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kube-controller-manager
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-owner-kube-controller-manager
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39664"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-kube-controller-manager
    uid: 9bcdfe50-dfc5-4df1-9376-912f5bb9d33e
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes Scheduler Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes scheduler that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kube_scheduler
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kube-scheduler
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-owner-kube-scheduler
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39743"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-kube-scheduler
    uid: 175dbf17-485c-4553-ae22-c5ebc6f1bd93
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubelet Configuration File
    The kubelet configuration file contains information about the configuration of the
    OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kubelet_conf
  instructions: |-
    To check the ownership of /etc/kubernetes/kubelet.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet.conf
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kubelet-conf
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-owner-kubelet-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39698"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-kubelet-conf
    uid: c4be27b9-bb69-45ef-af78-8b83c5cb1c48
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Admin Kubeconfig Files
    There are various kubeconfig files that can be used by the administrator,
    defining various settings for the administration of the cluster. These files
    contain credentials that can be used to control the cluster and are needed
    for disaster recovery and each kubeconfig points to a different endpoint in
    the cluster. You should restrict its file permissions to maintain the
    integrity of the kubeconfig file as an attacker who gains access to these
    files can take over the cluster.
  id: xccdf_org.ssgproject.content_rule_file_owner_master_admin_kubeconfigs
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-master-admin-kubeconfigs
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-master-admin-kubeconfigs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39649"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-master-admin-kubeconfigs
    uid: f702127b-f8e4-42c2-a3c1-fbd23dca2d71
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Multus Container Network Interface Plugin Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_multus_conf
  instructions: |-
    To check the ownership of /var/run/multus/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/multus/cni/net.d/*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-multus-conf
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-multus-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39639"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-multus-conf
    uid: d562fec8-55ce-4059-b012-84439977cbef
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
  id: xccdf_org.ssgproject.content_rule_file_owner_openshift_pki_cert_files
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/*/*/*/tls.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/tls.crt\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-openshift-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-owner-openshift-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39780"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-openshift-pki-cert-files
    uid: 7d8dc847-00e3-43b2-af4d-434c3cfb175e
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift PKI Private Key Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by root:root.
  id: xccdf_org.ssgproject.content_rule_file_owner_openshift_pki_key_files
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.key,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.key\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-openshift-pki-key-files
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-owner-openshift-pki-key-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39716"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-openshift-pki-key-files
    uid: c74d5325-7e6d-4f17-8557-d48f4cb22b03
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift SDN CNI Server Config
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_openshift_sdn_cniserver_config
  instructions: |-
    To check the ownership of /var/run/openshift-sdn/cniserver/config.json,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/openshift-sdn/cniserver/config.json
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-openshift-sdn-cniserver-config
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-openshift-sdn-cniserver-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39651"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-openshift-sdn-cniserver-config
    uid: c0d456bb-9511-483c-82c3-8c45affd65c2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Configuration Database
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db
  instructions: |-
    To check the ownership of /etc/openvswitch/conf.db,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/conf.db
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-conf-db
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-ovs-conf-db
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39637"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ovs-conf-db
    uid: ef02666a-e4d2-47a5-8de0-a70cf7ddf659
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Configuration Database Lock
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db_lock
  instructions: |-
    To check the ownership of /etc/openvswitch/.conf.db.~lock~,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/.conf.db.~lock~
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-conf-db-lock
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-owner-ovs-conf-db-lock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39730"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ovs-conf-db-lock
    uid: 2f74f94e-e91c-424a-b8aa-850b892c3c5f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Process ID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_pid
  instructions: |-
    To check the ownership of /var/run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-pid
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-owner-ovs-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39641"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ovs-pid
    uid: 78896fe5-7d25-4906-9bae-b201fa2516ed
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Persistent System ID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_sys_id_conf
  instructions: |-
    To check the ownership of /etc/openvswitch/system-id.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/system-id.conf
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-sys-id-conf
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-owner-ovs-sys-id-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39695"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ovs-sys-id-conf
    uid: 0f727f36-6c01-49bf-aa33-6c954ef2ecab
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Daemon PID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_vswitchd_pid
  instructions: |-
    To check the ownership of /run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-vswitchd-pid
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-owner-ovs-vswitchd-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39746"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ovs-vswitchd-pid
    uid: a8c0d49f-bb8c-475f-8a48-a49cd7606fdf
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Database Server PID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovsdb_server_pid
  instructions: |-
    To check the ownership of /run/openvswitch/ovsdb-server.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /run/openvswitch/ovsdb-server.pid
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovsdb-server-pid
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-owner-ovsdb-server-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39728"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-ovsdb-server-pid
    uid: eb276dd1-c273-48f2-81af-d57ddf7457ef
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes Scheduler Kubeconfig File
    The kubeconfig for the Scheduler contains paramters for the scheduler
    to access the Kube API.
    You should set its file ownership to maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_owner_scheduler_kubeconfig
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-scheduler-kubeconfig
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-owner-scheduler-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39689"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-scheduler-kubeconfig
    uid: 0720c528-7dc4-46b7-94a7-c7b6dbaca8b0
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns the Worker Certificate Authority File
    The worker certificate authority file contains the certificate authority
    certificate for an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_worker_ca
  instructions: |-
    To check the ownership of /etc/kubernetes/kubelet-ca.crt,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet-ca.crt
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-worker-ca
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-owner-worker-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39667"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-worker-ca
    uid: c6b4adad-1383-412b-a22d-66b82fdadf69
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Worker Kubeconfig File
    The worker kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_worker_kubeconfig
  instructions: |-
    To check the ownership of /var/lib/kubelet/kubeconfig,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/kubelet/kubeconfig
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-worker-kubeconfig
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-owner-worker-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39701"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-worker-kubeconfig
    uid: 2fb1bdc1-4b67-4849-92a8-bf5b9ea3b2ef
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Node Service File
    The /etc/systemd/system/kubelet.service
    file contains information about the configuration of the
    OpenShift node service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_worker_service
  instructions: |-
    To check the ownership of /etc/systemd/system/kubelet.service,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/systemd/system/kubelet.service
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-worker-service
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-owner-worker-service
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39652"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-owner-worker-service
    uid: 9f6dbca1-fe62-4509-801d-849a3246678c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Container Network Interface Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_cni_conf
  instructions: |-
    To check the permissions of /etc/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/cni/net.d/*
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-cni-conf
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-permissions-cni-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39750"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-cni-conf
    uid: 01915ca6-8467-41e3-bcac-87167f1e8c70
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Controller Manager Kubeconfig File
    The Controller Manager's kubeconfig contains information about how the
    component will access the API server. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_permissions_controller_manager_kubeconfig
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following permissions:\n  -rw-r--r--"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-controller-manager-kubeconfig
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-permissions-controller-manager-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39677"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-controller-manager-kubeconfig
    uid: 3805bc5d-cc2c-4bbb-bb26-4d9cfc0a49c5
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd Database Directory
    etcd is a highly-available key-value store used by Kubernetes deployments for persistent
    storage of all of its REST API objects. This data directory should be protected from any
    unauthorized reads or writes. It should not be readable or writable by any group members
    or the world.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_data_dir
  instructions: |-
    To check the permissions of /var/lib/etcd,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/etcd
    If properly configured, the output should indicate the following permissions:
    drwx------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-data-dir
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-permissions-etcd-data-dir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39636"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-etcd-data-dir
    uid: 143ece03-d543-43a7-951f-56be74ad9549
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd Write-Ahead-Log Files
    etcd is a highly-available key-value store used by Kubernetes deployments for persistent
    storage of all of its REST API objects. This data directory should be protected from any
    unauthorized reads or writes. It should not be readable or writable by any group members
    or the world.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_data_files
  instructions: |-
    To check the permissions of /var/lib/etcd/member/wal/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/etcd/member/wal/*
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-data-files
    creationTimestamp: "2021-07-13T16:58:15Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:15Z"
    name: ocp4-cis-node-master-file-permissions-etcd-data-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39626"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-etcd-data-files
    uid: ce34375b-2eb6-41bf-a4ac-e189bf15b86f
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd Member Pod Specification File
    The etcd pod specification file controls various parameters that
    set the behavior of the etcd service in the master node. etcd is a
    highly-available key-value store which Kubernetes uses for persistent
    storage of all of its REST API object. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_member
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-member
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-permissions-etcd-member
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39640"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-etcd-member
    uid: 82f3db6b-6062-4956-bdd0-dfe5f029096a
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd PKI Certificate Files
    OpenShift makes use of a number of certificate files as part of the operation
    of its components. The permissions on these files should be set to
    600 or more restrictive to protect their integrity.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_pki_cert_files
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crt\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-permissions-etcd-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39749"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-etcd-pki-cert-files
    uid: d54c391b-dff6-4fe0-a3ef-2e6722c64250
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address Allocations
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ip_allocations
  instructions: |-
    To check the permissions of /var/lib/cni/networks/openshift-sdn/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/cni/networks/openshift-sdn/*
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ip-allocations
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-permissions-ip-allocations
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39754"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ip-allocations
    uid: cc759349-2eb0-4ce7-bda8-da99dd9b1556
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes API Server Pod Specification File
    If the Kubernetes specification file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    the Kubernetes API server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_kube_apiserver
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-kube-apiserver
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-permissions-kube-apiserver
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39643"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-kube-apiserver
    uid: 0efa27c2-8325-4607-884c-f6bd7c125e5b
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes Controller Manager Pod Specificiation File
    If the Kubernetes specification file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    an Kubernetes Controller Manager server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_kube_controller_manager
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-kube-controller-manager
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-permissions-kube-controller-manager
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39630"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-kube-controller-manager
    uid: d4d752dc-5c64-4adb-8e8f-351d6a3971e4
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on The Kubelet Configuration File
    If the kubelet configuration file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_kubelet_conf
  instructions: |-
    To check the permissions of /etc/kubernetes/kubelet.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/kubelet.conf
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-kubelet-conf
    creationTimestamp: "2021-07-13T16:58:24Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:24Z"
    name: ocp4-cis-node-master-file-permissions-kubelet-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39783"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-kubelet-conf
    uid: 52e871ae-faaf-4021-b471-8808025d679c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Admin Kubeconfig Files
    There are various kubeconfig files that can be used by the administrator,
    defining various settings for the administration of the cluster. These files
    contain credentials that can be used to control the cluster and are needed
    for disaster recovery and each kubeconfig points to a different endpoint in
    the cluster. You should restrict its file permissions to maintain the
    integrity of the kubeconfig file as an attacker who gains access to these
    files can take over the cluster.
  id: xccdf_org.ssgproject.content_rule_file_permissions_master_admin_kubeconfigs
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-master-admin-kubeconfigs
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-permissions-master-admin-kubeconfigs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39704"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-master-admin-kubeconfigs
    uid: 42bfdd91-93e9-439a-b922-a3ebd85ce8e6
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Multus Container Network Interface Plugin Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_multus_conf
  instructions: |-
    To check the permissions of /var/run/multus/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/run/multus/cni/net.d/*
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-multus-conf
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-permissions-multus-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39686"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-multus-conf
    uid: 49d7e38d-74fb-4e94-81e6-bdda2a4c4a79
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift PKI Certificate Files
    OpenShift makes use of a number of certificate files as part of the operation
    of its components. The permissions on these files should be set to
    600 or more restrictive to protect their integrity.
  id: xccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_cert_files
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crt\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-openshift-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-file-permissions-openshift-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39741"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-openshift-pki-cert-files
    uid: 1e45dafd-1b8d-4267-8172-fbc3e2686ce1
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift PKI Private Key Files
    OpenShift makes use of a number of key files as part of the operation of its
    components. The permissions on these files should be set to 600
    to protect their integrity and confidentiality.
  id: xccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_key_files
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/*/*/*/*.key,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/*/*/*/*.key\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-openshift-pki-key-files
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-permissions-openshift-pki-key-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39674"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-openshift-pki-key-files
    uid: 747652c5-c572-48be-ab9b-8664f06f5590
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Configuration Database
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db
  instructions: |-
    To check the permissions of /etc/openvswitch/conf.db,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/openvswitch/conf.db
    If properly configured, the output should indicate the following permissions:
    -rw-r-----
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-conf-db
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-file-permissions-ovs-conf-db
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39650"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ovs-conf-db
    uid: 465585e6-ed4b-4dcd-bffe-da60a454ad05
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Configuration Database Lock
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db_lock
  instructions: |-
    To check the permissions of /etc/openvswitch/.conf.db.~lock~,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/openvswitch/.conf.db.~lock~
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-conf-db-lock
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-permissions-ovs-conf-db-lock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39757"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ovs-conf-db-lock
    uid: c2f3df4b-419d-4a29-9e21-93faaa3902f9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Process ID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_pid
  instructions: |-
    To check the permissions of /var/run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-pid
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-file-permissions-ovs-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39727"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ovs-pid
    uid: 9d997aca-eccc-49f2-a7fe-3c5ded162ad4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Persistent System ID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_sys_id_conf
  instructions: |-
    To check the permissions of /etc/openvswitch/system-id.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/openvswitch/system-id.conf
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-sys-id-conf
    creationTimestamp: "2021-07-13T16:58:24Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:24Z"
    name: ocp4-cis-node-master-file-permissions-ovs-sys-id-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39784"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ovs-sys-id-conf
    uid: 494655f0-b8ff-4f3d-82ed-f11020bbe2b5
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Daemon PID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_vswitchd_pid
  instructions: |-
    To check the permissions of /run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-vswitchd-pid
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-permissions-ovs-vswitchd-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39654"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ovs-vswitchd-pid
    uid: 31a75109-0ca1-4471-ae9d-9927e53a317b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Database Server PID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovsdb_server_pid
  instructions: |-
    To check the permissions of /run/openvswitch/ovsdb-server.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /run/openvswitch/ovsdb-server.pid
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovsdb-server-pid
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-permissions-ovsdb-server-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39694"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-ovsdb-server-pid
    uid: 862e7496-4fe3-4dce-9619-28c77af3b026
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes Scheduler Pod Specification File
    If the Kubernetes specification file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    an Kubernetes Scheduler service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_scheduler
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-scheduler
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-permissions-scheduler
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39769"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-scheduler
    uid: e3face77-7d66-4c1f-a744-6e7858fcd559
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes Scheduler Kubeconfig File
    The kubeconfig for the Scheduler contains paramters for the scheduler
    to access the Kube API. You should restrict its file permissions to maintain
    the integrity of the file. The file should be writable by only the
    administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_permissions_scheduler_kubeconfig
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following permissions:\n  -rw-r--r--"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-scheduler-kubeconfig
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-file-permissions-scheduler-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39659"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-scheduler-kubeconfig
    uid: 940e5974-4702-4454-8061-b0f1186bb05d
  severity: medium
  status: PASS
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Worker Certificate Authority File
    If the worker certificate authority file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the certificate authority
    certificate for an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_worker_ca
  instructions: |-
    To check the permissions of /etc/kubernetes/kubelet-ca.crt,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/kubelet-ca.crt
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-worker-ca
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-file-permissions-worker-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39680"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-worker-ca
    uid: d2ed640c-20d9-49b2-9779-7ac24a20c021
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Worker Kubeconfig File
    If the worker kubeconfig file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the administration configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_worker_kubeconfig
  instructions: |-
    To check the permissions of /var/lib/kubelet/kubeconfig,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/kubelet/kubeconfig
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-worker-kubeconfig
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-file-permissions-worker-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39666"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-worker-kubeconfig
    uid: 47dea6d7-1f20-455e-ab95-83bba0faec3c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Node Service File
    If the /etc/systemd/system/kubelet.service
    file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the service configuration of the
    OpenShift node service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_worker_service
  instructions: "To check the permissions of /etc/systemd/system/kubelet.service,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/systemd/system/kubelet.service\n  If properly configured, the output should indicate the following permissions:\n  -rw-r--r--"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-worker-service
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-file-permissions-worker-service
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39756"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-permissions-worker-service
    uid: 7556da11-fba0-4117-b6fc-d0d64e2c90ac
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift SDN CNI Server Config
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_perms_openshift_sdn_cniserver_config
  instructions: |-
    To check the permissions of /var/run/openshift-sdn/cniserver/config.json,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/run/openshift-sdn/cniserver/config.json
    If properly configured, the output should indicate the following permissions:
    -r--r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-perms-openshift-sdn-cniserver-config
    creationTimestamp: "2021-07-13T16:58:20Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:20Z"
    name: ocp4-cis-node-master-file-perms-openshift-sdn-cniserver-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39706"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-file-perms-openshift-sdn-cniserver-config
    uid: 9089c688-7e30-4ef4-8f30-878c35685d4d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Anonymous Authentication to the Kubelet
    When enabled, requests that are not rejected by other configured
    authentication methods are treated as anonymous requests. These
    requests are then served by the Kubelet server. OpenShift Operators should
    rely on authentication to authorize access and disallow anonymous
    requests.
  id: xccdf_org.ssgproject.content_rule_kubelet_anonymous_auth
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep -A1 anonymous /etc/kubernetes/kubelet.conf
    The output should return enabled: false.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-anonymous-auth
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-kubelet-anonymous-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39675"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-anonymous-auth
    uid: d364827d-c3df-4f38-a6ad-9123c67a360b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure authorization is set to Webhook
    Ensuring that the authorization is configured correctly helps enforce that
    unauthenticated/unauthorized users have no access to OpenShift nodes.
  id: xccdf_org.ssgproject.content_rule_kubelet_authorization_mode
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep -A1 authorization /etc/kubernetes/kubelet.conf
    Verify that the output is not set to mode: AlwaysAllow, or missing
    (defaults to mode: Webhook).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-authorization-mode
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-kubelet-authorization-mode
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39629"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-authorization-mode
    uid: 2728d4df-bfc2-4f17-ac15-bee734fb5e94
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Configure the Client CA Certificate
    Not having a CA certificate for the kubelet will subject the kubelet to possible
    man-in-the-middle attacks especially on unsafe or untrusted networks.
    Certificate validation for the kubelet allows the API server to validate
    the kubelet's identity.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_client_ca
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep -A1 x509 /etc/kubernetes/kubelet.conf
    The output should contain a configured certificate like /etc/kubernetes/kubelet-ca.crt.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-client-ca
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-kubelet-configure-client-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39634"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-configure-client-ca
    uid: fc683d82-b1cc-4de4-9e9f-5ae879555b89
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Kubelet - Ensure Event Creation Is Configured
    It is important to capture all events and not restrict event creation.
    Events are an important source of security information and analytics that
    ensure that your environment is consistently monitored using the event
    data.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_event_creation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep eventRecordQPS /etc/kubernetes/kubelet.conf
    The output should return .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-event-creation
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-kubelet-configure-event-creation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39661"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-configure-event-creation
    uid: 61c9e7f5-5e2c-4635-b992-75b51249096c
  severity: medium
  status: FAIL
  warnings:
  - The MachineConfig Operator does not merge KubeletConfig objects, the last object is used instead. In case you need to set multiple options for kubelet, consider putting all the custom options into a single KubeletConfig object.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers
    TLS ciphers have had a number of known vulnerabilities and weaknesses,
    which can reduce the protection provided by them. By default Kubernetes
    supports a number of TLS ciphersuites including some that have security
    concerns, weakening the protection provided.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_tls_cipher_suites
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep tlsCipherSuites /etc/kubernetes/kubelet.conf
    Verify that the set of ciphers contains only the following:

    TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
    TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
    TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,
    TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-tls-cipher-suites
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-kubelet-configure-tls-cipher-suites
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39760"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-configure-tls-cipher-suites
    uid: 409fde02-45dd-4585-8577-7a4cb5fc8f79
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Disable Hostname Override
    Allowing hostnames to be overrided creates issues around resolving nodes
    in addition to TLS configuration, certificate validation, and log correlation
    and validation.
  id: xccdf_org.ssgproject.content_rule_kubelet_disable_hostname_override
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep hostname-override /etc/systemd/system/kubelet.service
    The output should return no output.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-disable-hostname-override
    creationTimestamp: "2021-07-13T16:58:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:17Z"
    name: ocp4-cis-node-master-kubelet-disable-hostname-override
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39653"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-disable-hostname-override
    uid: f08ed451-6842-4f88-987e-0772470e127f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Certificate Rotation
    Allowing the kubelet to auto-update the certificates ensure that there is no downtime
    in certificate renewal as well as ensures confidentiality and integrity.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_cert_rotation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep rotateCertificates /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-cert-rotation
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-kubelet-enable-cert-rotation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39762"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-enable-cert-rotation
    uid: 20b029da-3b22-4118-ae15-86140012603b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Client Certificate Rotation
    Allowing the kubelet to auto-update the certificates ensure that there is no downtime
    in certificate renewal as well as ensures confidentiality and integrity.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_client_cert_rotation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep RotateKubeletClientCertificate /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-client-cert-rotation
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-kubelet-enable-client-cert-rotation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39633"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-enable-client-cert-rotation
    uid: 78608900-cb3e-481f-84a9-53b464b210fc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Allow Automatic Firewall Configuration
    The kubelet should automatically configure the firewall settings to allow access and
    networking traffic through. This ensures that when a pod or container is running that
    the correct ports are configured as well as removing the ports when a pod or
    container is no longer in existence.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_iptables_util_chains
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep makeIPTablesUtilChains /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-iptables-util-chains
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-kubelet-enable-iptables-util-chains
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39668"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-enable-iptables-util-chains
    uid: 3ac6f0c3-ebce-4722-ac69-c1e157199e9b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Protect Kernel Defaults
    Kernel parameters are usually tuned and hardened by the system administrators
    before putting the systems into production. These parameters protect the
    kernel and the system. Your kubelet kernel defaults that rely on such
    parameters should be appropriately set to match the desired secured system
    state. Ignoring this could potentially lead to running pods with undesired
    kernel behavior.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_protect_kernel_defaults
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep protectKernelDefaults /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-protect-kernel-defaults
    creationTimestamp: "2021-07-13T16:58:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:16Z"
    name: ocp4-cis-node-master-kubelet-enable-protect-kernel-defaults
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39645"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-enable-protect-kernel-defaults
    uid: 95b635e4-c354-4d85-915f-d2f35ea92098
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Server Certificate Rotation
    Allowing the kubelet to auto-update the certificates ensure that there is no downtime
    in certificate renewal as well as ensures confidentiality and integrity.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_server_cert_rotation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep RotateKubeletServerCertificate /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-server-cert-rotation
    creationTimestamp: "2021-07-13T16:58:24Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:24Z"
    name: ocp4-cis-node-master-kubelet-enable-server-cert-rotation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39788"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-enable-server-cert-rotation
    uid: 1fb9ef56-3dd2-4dbb-8699-553d705c1414
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Do Not Disable Streaming Timeouts
    Ensuring connections have timeouts helps to protect against denial-of-service attacks as
    well as disconnect inactive connections. In addition, setting connections timeouts helps
    to prevent from running out of ephemeral ports.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_streaming_connections
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep streamingConnectionIdleTimeout /etc/kubernetes/kubelet.conf
    The output should return .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-streaming-connections
    creationTimestamp: "2021-07-13T16:58:23Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:23Z"
    name: ocp4-cis-node-master-kubelet-enable-streaming-connections
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39758"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-enable-streaming-connections
    uid: 06042b10-3c8c-4f74-91d4-e6359347d91a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."imagefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-imagefs-available
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39684"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available
    uid: 778c4e7c-abb0-40f0-9e91-dd205396b8ac
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."imagefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-imagefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:15Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:15Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39628"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree
    uid: 26ade130-ea91-4230-ad70-67635e210354
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: memory.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_memory_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."memory.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-memory-available
    creationTimestamp: "2021-07-13T16:58:18Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:18Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-memory-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39669"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-memory-available
    uid: 4a29524f-eea3-492e-a56b-3176dbacf7bf
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."nodefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-nodefs-available
    creationTimestamp: "2021-07-13T16:58:24Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:24Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39786"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-available
    uid: 43e7005c-4f88-4678-9482-2e5d002e5621
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."nodefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-nodefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39747"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree
    uid: f87950e2-5ae9-4e26-a88c-6f6aadbd3f63
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."imagefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-imagefs-available
    creationTimestamp: "2021-07-13T16:58:24Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:24Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39787"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-available
    uid: b32408b1-8b91-4b9d-8196-5cc014042b82
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."imagefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-imagefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39723"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree
    uid: 17f14b7c-5ddd-429d-be39-e33e9d4ecb6e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: memory.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_memory_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."memory.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-memory-available
    creationTimestamp: "2021-07-13T16:58:21Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:21Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-memory-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39720"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-memory-available
    uid: 8ad48a04-0c10-4bfd-af43-ab7996ead2ea
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."nodefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-nodefs-available
    creationTimestamp: "2021-07-13T16:58:22Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:22Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39748"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-available
    uid: cde14259-f4b0-4274-95c4-eb58145ef584
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."nodefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-nodefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:19Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-master
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"15803fe9-a0ee-409b-b406-9c56b80a877a"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:19Z"
    name: ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-master
      uid: 15803fe9-a0ee-409b-b406-9c56b80a877a
    resourceVersion: "39685"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree
    uid: 04d1d8c8-b600-40ca-ad98-4576676052d7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure A Unique CA Certificate for etcd
    The Kubernetes API server and etcd utilize separate CA certificates in
    OpenShift.  This ensures that the etcd data is still protected in the event
    that the API server CA is compromised.
  id: xccdf_org.ssgproject.content_rule_etcd_unique_ca
  instructions: |-
    Run the following command:
    oc debug node/$NODE -- diff /host/etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-serving-ca/ca-bundle.crt /host/etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/client-ca/ca-bundle.crt
    where $NODE is a master node. If you don't see diff output
    the differences, you might have a compromise and should isolate the cluster.
    OpenShift will use separate PKIs by default.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-unique-ca
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-etcd-unique-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39440"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-etcd-unique-ca
    uid: 952765bf-5abf-486b-bfe9-1024c2c587e9
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Container Network Interface Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_cni_conf
  instructions: |-
    To check the group ownership of /etc/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/cni/net.d/*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-cni-conf
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-groupowner-cni-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39600"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-cni-conf
    uid: 7515882d-4172-4ded-9137-9f84cbcb20ad
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Controller Manager Kubeconfig File
    The Controller Manager's kubeconfig contains information about how the
    component will access the API server. You should set its file ownership to
    maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_controller_manager_kubeconfig
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-controller-manager-kubeconfig
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-groupowner-controller-manager-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39560"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-controller-manager-kubeconfig
    uid: 874be950-1c0d-4dc0-842a-038fb3c5d99e
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Etcd Database Directory
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_dir
  instructions: |-
    To check the group ownership of /var/lib/etcd/member/,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-data-dir
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-groupowner-etcd-data-dir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39512"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-etcd-data-dir
    uid: 43af185a-87e9-4eb4-8225-e4919abe5f7f
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Etcd Write-Ahead-Log Files
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_files
  instructions: |-
    To check the group ownership of /var/lib/etcd/member/wal/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/wal/*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-data-files
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-etcd-data-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39457"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-etcd-data-files
    uid: 0194adbc-8765-4a2a-9cb9-064be1ce3f24
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The etcd Member Pod Specification File
    The etcd pod specification file controls various parameters that
    set the behavior of the etcd service in the master node. etcd is a
    highly-available key-value store which Kubernetes uses for persistent
    storage of all of its REST API object. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_member
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-member
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-groupowner-etcd-member
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39487"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-etcd-member
    uid: 3fe32ab5-a1ff-4cd1-ad1f-d47e11af99ab
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Etcd PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by the system administrator.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_etcd_pki_cert_files
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.crt\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-etcd-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-etcd-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39443"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-etcd-pki-cert-files
    uid: 41d8a4b3-fcf6-4af3-b210-7bff6ae25075
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocations
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ip_allocations
  instructions: |-
    To check the group ownership of /var/lib/cni/networks/openshift-sdn/.*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/cni/networks/openshift-sdn/.*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ip-allocations
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-groupowner-ip-allocations
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39596"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ip-allocations
    uid: bba04763-3763-45a2-84a8-fe6856c7e16c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes API Server Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes API Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kube_apiserver
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kube-apiserver
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-groupowner-kube-apiserver
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39473"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-kube-apiserver
    uid: be2cf404-926f-4f8e-84f0-eecee4e774e5
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes Controller Manager Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes Controller Manager Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kube_controller_manager
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kube-controller-manager
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-kube-controller-manager
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39445"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-kube-controller-manager
    uid: 02ebe2a1-9de8-4390-8b46-fded803cdec2
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes Scheduler Pod Specification File
    The Kubernetes Specification file contains information about the configuration of the
    Kubernetes scheduler that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kube_scheduler
  instructions: |-
    To check the group ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kube-scheduler
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-groupowner-kube-scheduler
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39528"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-kube-scheduler
    uid: 13cedbf9-3263-4f07-9a70-8576187e3c4c
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubelet Configuration File
    The kubelet configuration file contains information about the configuration of the
    OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_kubelet_conf
  instructions: |-
    To check the group ownership of /etc/kubernetes/kubelet.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet.conf
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-kubelet-conf
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-kubelet-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39447"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-kubelet-conf
    uid: 76e67874-18b5-4f3d-a565-5642ddf483a1
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Admin Kubeconfig Files
    There are various kubeconfig files that can be used by the administrator,
    defining various settings for the administration of the cluster. These files
    contain credentials that can be used to control the cluster and are needed
    for disaster recovery and each kubeconfig points to a different endpoint in
    the cluster. You should restrict its file permissions to maintain the
    integrity of the kubeconfig file as an attacker who gains access to these
    files can take over the cluster.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_master_admin_kubeconfigs
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-master-admin-kubeconfigs
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-groupowner-master-admin-kubeconfigs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39465"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-master-admin-kubeconfigs
    uid: 9654f0f0-d69a-4552-ba58-7b0ca0e10eaa
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Multus Container Network Interface Plugin Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_multus_conf
  instructions: |-
    To check the group ownership of /var/run/multus/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/multus/cni/net.d/*
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-multus-conf
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-groupowner-multus-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39543"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-multus-conf
    uid: d2e5ccc2-3514-4ac1-8c74-38b3e8cf2c43
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by the system administrator.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_cert_files
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/.*/.*/.*/tls.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/.*/.*/.*/tls.crt\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-openshift-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-groupowner-openshift-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39559"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-openshift-pki-cert-files
    uid: 359ab866-7883-4205-9da6-396990027dda
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift PKI Private Key Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by root:root.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_key_files
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.key,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.key\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-openshift-pki-key-files
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-openshift-pki-key-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39462"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-openshift-pki-key-files
    uid: 1aaa5aaf-8aa8-44af-a90b-dd55a8e7d460
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift SDN CNI Server Config
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_openshift_sdn_cniserver_config
  instructions: |-
    To check the group ownership of /var/run/openshift-sdn/cniserver/config.json,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/openshift-sdn/cniserver/config.json
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-openshift-sdn-cniserver-config
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-file-groupowner-openshift-sdn-cniserver-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39567"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-openshift-sdn-cniserver-config
    uid: 14e16263-dcb3-4ea4-b326-7040f1af06ff
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Configuration Database
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db
  instructions: |-
    To check the group ownership of /etc/openvswitch/conf.db,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/conf.db
    If properly configured, the output should indicate the following group-owner:
    hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-conf-db
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-groupowner-ovs-conf-db
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39471"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ovs-conf-db
    uid: be9f00d9-5546-4660-9597-bb202d5c4458
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Configuration Database Lock
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db_lock
  instructions: |-
    To check the group ownership of /etc/openvswitch/.conf.db.~lock~,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/.conf.db.~lock~
    If properly configured, the output should indicate the following group-owner:
    hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-conf-db-lock
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-groupowner-ovs-conf-db-lock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39495"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ovs-conf-db-lock
    uid: f63d64dc-2264-44d0-b55d-5e80e0d3b1f8
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Process ID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_pid
  instructions: |-
    To check the group ownership of /var/run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:


    $ oc debug node/$NODE_NAME


    At the sh-4.4# prompt, run:


    # chroot /host


    Then,run the command:

    $ ls -l /var/run/openvswitch/ovs-vswitchd.pid

    If properly configured, the output should indicate one of the following group-owners:
    openvswitch or hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-pid
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-groupowner-ovs-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39598"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ovs-pid
    uid: 42704a72-7b37-46d4-99f0-8698eb01d802
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Persistent System ID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_sys_id_conf
  instructions: |-
    To check the group ownership of /etc/openvswitch/system-id.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/system-id.conf
    If properly configured, the output should indicate the following group-owner:
    hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-sys-id-conf
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-groupowner-ovs-sys-id-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39464"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ovs-sys-id-conf
    uid: c2b45d12-6a07-4008-ab3a-e10a76ec9bbd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Daemon PID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovs_vswitchd_pid
  instructions: |-
    To check the group ownership of /run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:


    $ oc debug node/$NODE_NAME


    At the sh-4.4# prompt, run:


    # chroot /host


    Then,run the command:

    $ ls -l /run/openvswitch/ovs-vswitchd.pid

    If properly configured, the output should indicate one of the following group-owners:
    openvswitch or hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovs-vswitchd-pid
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-groupowner-ovs-vswitchd-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39521"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ovs-vswitchd-pid
    uid: cb6d91bd-67c4-45af-81ba-b8bc89396aeb
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Open vSwitch Database Server PID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_ovsdb_server_pid
  instructions: |-
    To check the group ownership of /run/openvswitch/ovsdb-server.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:


    $ oc debug node/$NODE_NAME


    At the sh-4.4# prompt, run:


    # chroot /host


    Then,run the command:

    $ ls -l /run/openvswitch/ovsdb-server.pid

    If properly configured, the output should indicate one of the following group-owners:
    openvswitch or hugetlbfs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-ovsdb-server-pid
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-ovsdb-server-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39459"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-ovsdb-server-pid
    uid: 665fbf5b-536c-48de-b229-75161a7cab03
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Kubernetes Scheduler Kubeconfig File
    The kubeconfig for the Scheduler contains paramters for the scheduler
    to access the Kube API.
    You should set its file ownership to maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_scheduler_kubeconfig
  instructions: "To check the group ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following group-owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-scheduler-kubeconfig
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-scheduler-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39444"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-scheduler-kubeconfig
    uid: d1a101b1-018f-42a8-8ad9-4777f191f524
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns the Worker Certificate Authority File
    The worker certificate authority file contains the certificate authority
    certificate for an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_worker_ca
  instructions: |-
    To check the group ownership of /etc/kubernetes/kubelet-ca.crt,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet-ca.crt
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-worker-ca
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-groupowner-worker-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39516"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-worker-ca
    uid: d6aeae1f-e064-4533-9a4e-811f409251f6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Worker Kubeconfig File
    The worker kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_worker_kubeconfig
  instructions: |-
    To check the group ownership of /var/lib/kubelet/kubeconfig,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/kubelet/kubeconfig
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-worker-kubeconfig
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-groupowner-worker-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39439"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-worker-kubeconfig
    uid: c1785626-67ef-4329-8d07-526d27b0dfd3
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The OpenShift Node Service File
    The /etc/systemd/system/kubelet.service
    file contains information about the configuration of the
    OpenShift node service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_worker_service
  instructions: |-
    To check the group ownership of /etc/systemd/system/kubelet.service,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/systemd/system/kubelet.service
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-worker-service
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-groupowner-worker-service
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39589"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-groupowner-worker-service
    uid: 6a924306-7c80-43fd-8ffe-d11f9569ce9b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Container Network Interface Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_cni_conf
  instructions: |-
    To check the ownership of /etc/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/cni/net.d/*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-cni-conf
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-owner-cni-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39463"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-cni-conf
    uid: 242a999a-1b66-453c-8dc8-2c2c01908a48
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Controller Manager Kubeconfig File
    The Controller Manager's kubeconfig contains information about how the
    component will access the API server. You should set its file ownership to
    maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_owner_controller_manager_kubeconfig
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-controller-manager-kubeconfig
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-file-owner-controller-manager-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39571"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-controller-manager-kubeconfig
    uid: d9999e21-2a5f-4e53-982f-127e505c064d
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd Database Directory
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_data_dir
  instructions: |-
    To check the ownership of /var/lib/etcd/member/,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-data-dir
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-owner-etcd-data-dir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39542"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-etcd-data-dir
    uid: 124ce3d0-7a02-46bd-9868-10bc448596b9
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd Write-Ahead-Log Files
    etcd is a highly-available key-value store used by Kubernetes deployments for
    persistent storage of all of its REST API objects. This data directory should
    be protected from any unauthorized reads or writes.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_data_files
  instructions: |-
    To check the ownership of /var/lib/etcd/member/wal/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/etcd/member/wal/*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-data-files
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-etcd-data-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39452"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-etcd-data-files
    uid: db31f858-cbdb-4303-9c9c-b02cea83cb2d
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd Member Pod Specification File
    The etcd pod specification file controls various parameters that
    set the behavior of the etcd service in the master node. etcd is a
    highly-available key-value store which Kubernetes uses for persistent
    storage of all of its REST API object. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_member
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-member
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-owner-etcd-member
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39509"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-etcd-member
    uid: fd381941-df19-4e98-82de-69f9d1381f65
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Etcd PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by the system administrator.
  id: xccdf_org.ssgproject.content_rule_file_owner_etcd_pki_cert_files
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.crt\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-etcd-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-owner-etcd-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39474"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-etcd-pki-cert-files
    uid: 4816b3e3-1b4b-467d-a419-2310de1df421
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocations
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ip_allocations
  instructions: |-
    To check the ownership of /var/lib/cni/networks/openshift-sdn/.*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/cni/networks/openshift-sdn/.*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ip-allocations
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-ip-allocations
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39449"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ip-allocations
    uid: 6df1aed0-c671-4fe1-a7a2-813822a2f8f4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes API Server Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes API Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kube_apiserver
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kube-apiserver
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-kube-apiserver
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39461"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-kube-apiserver
    uid: 26cefb70-e00e-44a2-aead-674861e402e0
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes Controller Manager Pod Specificiation File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes Controller Manager Server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kube_controller_manager
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kube-controller-manager
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-file-owner-kube-controller-manager
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39583"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-kube-controller-manager
    uid: 3e465ce5-e473-43c8-bd58-118ba8109d7f
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes Scheduler Pod Specification File
    The Kubernetes specification file contains information about the configuration of the
    Kubernetes scheduler that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kube_scheduler
  instructions: |-
    To check the ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kube-scheduler
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-kube-scheduler
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39442"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-kube-scheduler
    uid: 0bb8ca01-e612-40fc-a176-c7b3792fab64
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubelet Configuration File
    The kubelet configuration file contains information about the configuration of the
    OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_kubelet_conf
  instructions: |-
    To check the ownership of /etc/kubernetes/kubelet.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet.conf
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-kubelet-conf
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-kubelet-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39448"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-kubelet-conf
    uid: e25a0c8c-12f7-462c-8b5f-169262b71e9c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Admin Kubeconfig Files
    There are various kubeconfig files that can be used by the administrator,
    defining various settings for the administration of the cluster. These files
    contain credentials that can be used to control the cluster and are needed
    for disaster recovery and each kubeconfig points to a different endpoint in
    the cluster. You should restrict its file permissions to maintain the
    integrity of the kubeconfig file as an attacker who gains access to these
    files can take over the cluster.
  id: xccdf_org.ssgproject.content_rule_file_owner_master_admin_kubeconfigs
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-master-admin-kubeconfigs
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-owner-master-admin-kubeconfigs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39609"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-master-admin-kubeconfigs
    uid: 49ece531-8727-4792-a257-c86cb4878f5b
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Multus Container Network Interface Plugin Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_multus_conf
  instructions: |-
    To check the ownership of /var/run/multus/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/multus/cni/net.d/*
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-multus-conf
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-owner-multus-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39507"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-multus-conf
    uid: 4eda0cd9-4b00-4ddb-9408-444fd8c3347e
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift PKI Certificate Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
  id: xccdf_org.ssgproject.content_rule_file_owner_openshift_pki_cert_files
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/*/*/*/tls.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/tls.crt\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-openshift-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-owner-openshift-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39498"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-openshift-pki-cert-files
    uid: 8a920851-c052-44b0-8e1b-134a77e1c902
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift PKI Private Key Files
    OpenShift makes use of a number of certificates as part of its operation.
    You should verify the ownership of the directory containing the PKI
    information and all files in that directory to maintain their integrity.
    The directory and files should be owned by root:root.
  id: xccdf_org.ssgproject.content_rule_file_owner_openshift_pki_key_files
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/*/*/*/*.key,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/*/*/*/*.key\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-openshift-pki-key-files
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-owner-openshift-pki-key-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39588"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-openshift-pki-key-files
    uid: 3be8eb21-298d-4685-af70-9a24a8b85a00
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift SDN CNI Server Config
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_openshift_sdn_cniserver_config
  instructions: |-
    To check the ownership of /var/run/openshift-sdn/cniserver/config.json,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/openshift-sdn/cniserver/config.json
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-openshift-sdn-cniserver-config
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-owner-openshift-sdn-cniserver-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39476"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-openshift-sdn-cniserver-config
    uid: 524d9f64-91b8-4c0b-b7b2-d0e456a76185
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Configuration Database
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db
  instructions: |-
    To check the ownership of /etc/openvswitch/conf.db,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/conf.db
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-conf-db
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-owner-ovs-conf-db
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39502"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ovs-conf-db
    uid: f3539121-6b7f-4050-9414-71ba5220da58
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Configuration Database Lock
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db_lock
  instructions: |-
    To check the ownership of /etc/openvswitch/.conf.db.~lock~,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/.conf.db.~lock~
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-conf-db-lock
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-owner-ovs-conf-db-lock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39534"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ovs-conf-db-lock
    uid: 15b88be4-d26d-4ed1-87ca-ca1c33f653e2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Process ID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_pid
  instructions: |-
    To check the ownership of /var/run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-pid
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-owner-ovs-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39504"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ovs-pid
    uid: b1b8ddc8-eff5-475e-8e7c-645216385b74
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Persistent System ID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_sys_id_conf
  instructions: |-
    To check the ownership of /etc/openvswitch/system-id.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/openvswitch/system-id.conf
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-sys-id-conf
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-file-owner-ovs-sys-id-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39575"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ovs-sys-id-conf
    uid: ddb71798-185c-4b58-b141-2fd74d76ef2e
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Daemon PID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovs_vswitchd_pid
  instructions: |-
    To check the ownership of /run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovs-vswitchd-pid
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-owner-ovs-vswitchd-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39477"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ovs-vswitchd-pid
    uid: 80e59f04-7f86-49dd-9044-69296cff1266
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Open vSwitch Database Server PID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_owner_ovsdb_server_pid
  instructions: |-
    To check the ownership of /run/openvswitch/ovsdb-server.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /run/openvswitch/ovsdb-server.pid
    If properly configured, the output should indicate the following owner:
    openvswitch
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-ovsdb-server-pid
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-ovsdb-server-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39441"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-ovsdb-server-pid
    uid: 0d586247-d968-42db-874f-8b048ad34c92
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Kubernetes Scheduler Kubeconfig File
    The kubeconfig for the Scheduler contains paramters for the scheduler
    to access the Kube API.
    You should set its file ownership to maintain the integrity of the file.
  id: xccdf_org.ssgproject.content_rule_file_owner_scheduler_kubeconfig
  instructions: "To check the ownership of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -lL /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following owner:\n  root"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-scheduler-kubeconfig
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-owner-scheduler-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39450"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-scheduler-kubeconfig
    uid: d3ee705c-98ce-404e-a4c8-1d1572ec9fa3
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns the Worker Certificate Authority File
    The worker certificate authority file contains the certificate authority
    certificate for an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_worker_ca
  instructions: |-
    To check the ownership of /etc/kubernetes/kubelet-ca.crt,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/kubernetes/kubelet-ca.crt
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-worker-ca
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-owner-worker-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39607"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-worker-ca
    uid: 5189a559-94db-419f-8dcd-654eccdae10d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Worker Kubeconfig File
    The worker kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_worker_kubeconfig
  instructions: |-
    To check the ownership of /var/lib/kubelet/kubeconfig,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /var/lib/kubelet/kubeconfig
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-worker-kubeconfig
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-owner-worker-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39479"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-worker-kubeconfig
    uid: 06ad6c49-8aa1-427e-9f02-360f8d41f19a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The OpenShift Node Service File
    The /etc/systemd/system/kubelet.service
    file contains information about the configuration of the
    OpenShift node service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_owner_worker_service
  instructions: |-
    To check the ownership of /etc/systemd/system/kubelet.service,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/systemd/system/kubelet.service
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-worker-service
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-owner-worker-service
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39547"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-owner-worker-service
    uid: d78aba58-cc7d-4db1-94cd-8e0b85fa02a9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Container Network Interface Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_cni_conf
  instructions: |-
    To check the permissions of /etc/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/cni/net.d/*
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-cni-conf
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-permissions-cni-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39585"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-cni-conf
    uid: 41ddf5d5-3682-47e8-87f0-dbb9fc3d206c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Controller Manager Kubeconfig File
    The Controller Manager's kubeconfig contains information about how the
    component will access the API server. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_permissions_controller_manager_kubeconfig
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following permissions:\n  -rw-r--r--"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-controller-manager-kubeconfig
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-permissions-controller-manager-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39491"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-controller-manager-kubeconfig
    uid: 9d6c129f-c7ca-4ae1-a05f-fc8576a977ac
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd Database Directory
    etcd is a highly-available key-value store used by Kubernetes deployments for persistent
    storage of all of its REST API objects. This data directory should be protected from any
    unauthorized reads or writes. It should not be readable or writable by any group members
    or the world.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_data_dir
  instructions: |-
    To check the permissions of /var/lib/etcd,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/etcd
    If properly configured, the output should indicate the following permissions:
    drwx------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-data-dir
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-file-permissions-etcd-data-dir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39565"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-etcd-data-dir
    uid: b6734324-24d8-4e3e-a77b-f2e0f3958b40
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd Write-Ahead-Log Files
    etcd is a highly-available key-value store used by Kubernetes deployments for persistent
    storage of all of its REST API objects. This data directory should be protected from any
    unauthorized reads or writes. It should not be readable or writable by any group members
    or the world.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_data_files
  instructions: |-
    To check the permissions of /var/lib/etcd/member/wal/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/etcd/member/wal/*
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-data-files
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-permissions-etcd-data-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39506"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-etcd-data-files
    uid: 29ec933e-301a-4e32-a6d4-331c1864df4d
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd Member Pod Specification File
    The etcd pod specification file controls various parameters that
    set the behavior of the etcd service in the master node. etcd is a
    highly-available key-value store which Kubernetes uses for persistent
    storage of all of its REST API object. You should restrict its file
    permissions to maintain the integrity of the file. The file should be
    writable by only the administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_member
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-member
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-permissions-etcd-member
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39511"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-etcd-member
    uid: bac86a4f-bc88-4ea3-8cb8-6ff36c4303d0
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Etcd PKI Certificate Files
    OpenShift makes use of a number of certificate files as part of the operation
    of its components. The permissions on these files should be set to
    600 or more restrictive to protect their integrity.
  id: xccdf_org.ssgproject.content_rule_file_permissions_etcd_pki_cert_files
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crt\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-etcd-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-permissions-etcd-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39553"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-etcd-pki-cert-files
    uid: f74a26d1-de0d-41a4-a2d8-bf74dccb78b1
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address Allocations
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ip_allocations
  instructions: |-
    To check the permissions of /var/lib/cni/networks/openshift-sdn/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/cni/networks/openshift-sdn/*
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ip-allocations
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-permissions-ip-allocations
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39526"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ip-allocations
    uid: 90332602-90a1-43a2-a5eb-93e01aba0b11
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes API Server Pod Specification File
    If the Kubernetes specification file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    the Kubernetes API server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_kube_apiserver
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-kube-apiserver
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-permissions-kube-apiserver
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39606"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-kube-apiserver
    uid: e9a93ab4-e9bd-4a28-bda9-aa868e6c2a31
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes Controller Manager Pod Specificiation File
    If the Kubernetes specification file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    an Kubernetes Controller Manager server that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_kube_controller_manager
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-kube-controller-manager
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-file-permissions-kube-controller-manager
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39587"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-kube-controller-manager
    uid: 0cafee1a-4dc5-45f4-ad8c-ecec73ef74f0
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on The Kubelet Configuration File
    If the kubelet configuration file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_kubelet_conf
  instructions: |-
    To check the permissions of /etc/kubernetes/kubelet.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/kubelet.conf
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-kubelet-conf
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-permissions-kubelet-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39446"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-kubelet-conf
    uid: 11880afa-8ed7-44e8-b62f-0cb400863129
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Admin Kubeconfig Files
    There are various kubeconfig files that can be used by the administrator,
    defining various settings for the administration of the cluster. These files
    contain credentials that can be used to control the cluster and are needed
    for disaster recovery and each kubeconfig points to a different endpoint in
    the cluster. You should restrict its file permissions to maintain the
    integrity of the kubeconfig file as an attacker who gains access to these
    files can take over the cluster.
  id: xccdf_org.ssgproject.content_rule_file_permissions_master_admin_kubeconfigs
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-master-admin-kubeconfigs
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-permissions-master-admin-kubeconfigs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39551"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-master-admin-kubeconfigs
    uid: b5010d1b-d7f5-4491-a1a5-3f6ba7ed896e
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Multus Container Network Interface Plugin Files
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_multus_conf
  instructions: |-
    To check the permissions of /var/run/multus/cni/net.d/*,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/run/multus/cni/net.d/*
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-multus-conf
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-file-permissions-multus-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39572"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-multus-conf
    uid: d13812d3-20f9-4ca5-a4df-6043fbfcdd00
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift PKI Certificate Files
    OpenShift makes use of a number of certificate files as part of the operation
    of its components. The permissions on these files should be set to
    600 or more restrictive to protect their integrity.
  id: xccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_cert_files
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crt,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crt\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-openshift-pki-cert-files
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-permissions-openshift-pki-cert-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39611"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-openshift-pki-cert-files
    uid: 1b0f8f24-a4c7-4075-b1e9-0ac3bbf796a6
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift PKI Private Key Files
    OpenShift makes use of a number of key files as part of the operation of its
    components. The permissions on these files should be set to 600
    to protect their integrity and confidentiality.
  id: xccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_key_files
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/*/*/*/*.key,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/*/*/*/*.key\n  If properly configured, the output should indicate the following permissions:\n  -rw-------"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-openshift-pki-key-files
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-permissions-openshift-pki-key-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39468"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-openshift-pki-key-files
    uid: 4235d8e6-9d77-4e8e-b4cd-a7a7370ca71b
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Configuration Database
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db
  instructions: |-
    To check the permissions of /etc/openvswitch/conf.db,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/openvswitch/conf.db
    If properly configured, the output should indicate the following permissions:
    -rw-r-----
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-conf-db
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-permissions-ovs-conf-db
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39602"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ovs-conf-db
    uid: 190ffebf-fa10-4c68-989b-7f040e49339a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Configuration Database Lock
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db_lock
  instructions: |-
    To check the permissions of /etc/openvswitch/.conf.db.~lock~,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/openvswitch/.conf.db.~lock~
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-conf-db-lock
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-permissions-ovs-conf-db-lock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39603"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ovs-conf-db-lock
    uid: b3017464-827d-4f19-9a28-82d7d43a57dd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Process ID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_pid
  instructions: |-
    To check the permissions of /var/run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-pid
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-permissions-ovs-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39552"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ovs-pid
    uid: a30f316c-6813-4af2-b9fb-ba0f804a9e66
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Persistent System ID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_sys_id_conf
  instructions: |-
    To check the permissions of /etc/openvswitch/system-id.conf,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/openvswitch/system-id.conf
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-sys-id-conf
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-file-permissions-ovs-sys-id-conf
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39470"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ovs-sys-id-conf
    uid: b7a9df8b-4aac-4c72-b322-b7522ad86d66
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Daemon PID File
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovs_vswitchd_pid
  instructions: |-
    To check the permissions of /run/openvswitch/ovs-vswitchd.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /run/openvswitch/ovs-vswitchd.pid
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovs-vswitchd-pid
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-file-permissions-ovs-vswitchd-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39508"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ovs-vswitchd-pid
    uid: 0cd5e995-bc48-4a89-bb7c-220adadd5cef
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Open vSwitch Database Server PID
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_permissions_ovsdb_server_pid
  instructions: |-
    To check the permissions of /run/openvswitch/ovsdb-server.pid,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /run/openvswitch/ovsdb-server.pid
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-ovsdb-server-pid
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-permissions-ovsdb-server-pid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39494"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-ovsdb-server-pid
    uid: 6c848fbc-bb2b-4b39-b571-addc4238b188
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes Scheduler Pod Specification File
    If the Kubernetes specification file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the configuration of
    an Kubernetes Scheduler service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_scheduler
  instructions: |-
    To check the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-scheduler
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-file-permissions-scheduler
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39538"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-scheduler
    uid: 779c45d4-c2ac-4714-ae0a-7d7e4d243a5d
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Kubernetes Scheduler Kubeconfig File
    The kubeconfig for the Scheduler contains paramters for the scheduler
    to access the Kube API. You should restrict its file permissions to maintain
    the integrity of the file. The file should be writable by only the
    administrators on the system.
  id: xccdf_org.ssgproject.content_rule_file_permissions_scheduler_kubeconfig
  instructions: "To check the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig\n  If properly configured, the output should indicate the following permissions:\n  -rw-r--r--"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-scheduler-kubeconfig
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-file-permissions-scheduler-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39605"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-scheduler-kubeconfig
    uid: 0095304d-bfe7-4cf0-8d3d-b108be0ed4e8
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled "master" by default.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Worker Certificate Authority File
    If the worker certificate authority file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the certificate authority
    certificate for an OpenShift node that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_worker_ca
  instructions: |-
    To check the permissions of /etc/kubernetes/kubelet-ca.crt,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/kubernetes/kubelet-ca.crt
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-worker-ca
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-permissions-worker-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39520"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-worker-ca
    uid: 867e5673-30e6-4112-a50c-4de6ccd3e96a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Worker Kubeconfig File
    If the worker kubeconfig file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the administration configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_worker_kubeconfig
  instructions: |-
    To check the permissions of /var/lib/kubelet/kubeconfig,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/lib/kubelet/kubeconfig
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-worker-kubeconfig
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-file-permissions-worker-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39478"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-worker-kubeconfig
    uid: 0240a4c4-3c15-4dec-8ada-1faf999c33d3
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift Node Service File
    If the /etc/systemd/system/kubelet.service
    file is writable by a group-owner or the
    world the risk of its compromise is increased. The file contains the service configuration of the
    OpenShift node service that is configured on the system. Protection of this file is
    critical for OpenShift security.
  id: xccdf_org.ssgproject.content_rule_file_permissions_worker_service
  instructions: "To check the permissions of /etc/systemd/system/kubelet.service,\n  you'll need to log into a node in the cluster.\n  As a user with administrator privileges, log into a node in the relevant pool:\n  \n  $ oc debug node/$NODE_NAME\n  \n  At the sh-4.4# prompt, run:\n  \n  # chroot /host\n  \n\n  Then,run the command:\n  $ ls -l /etc/systemd/system/kubelet.service\n  If properly configured, the output should indicate the following permissions:\n  -rw-r--r--"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-worker-service
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-file-permissions-worker-service
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39453"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-permissions-worker-service
    uid: 6dc1cda7-2ce1-4c97-870c-9c8ee72e4fbc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the OpenShift SDN CNI Server Config
    CNI (Container Network Interface) files consist of a specification and libraries for
    writing plugins to configure network interfaces in Linux containers, along with a number
    of supported plugins. Allowing writeable access to the files could allow an attacker to modify
    the networking configuration potentially adding a rogue network connection.
  id: xccdf_org.ssgproject.content_rule_file_perms_openshift_sdn_cniserver_config
  instructions: |-
    To check the permissions of /var/run/openshift-sdn/cniserver/config.json,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /var/run/openshift-sdn/cniserver/config.json
    If properly configured, the output should indicate the following permissions:
    -r--r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-perms-openshift-sdn-cniserver-config
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-file-perms-openshift-sdn-cniserver-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39522"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-file-perms-openshift-sdn-cniserver-config
    uid: bc9b22e6-4901-4355-a15c-9eeda5be7553
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Anonymous Authentication to the Kubelet
    When enabled, requests that are not rejected by other configured
    authentication methods are treated as anonymous requests. These
    requests are then served by the Kubelet server. OpenShift Operators should
    rely on authentication to authorize access and disallow anonymous
    requests.
  id: xccdf_org.ssgproject.content_rule_kubelet_anonymous_auth
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep -A1 anonymous /etc/kubernetes/kubelet.conf
    The output should return enabled: false.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-anonymous-auth
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-kubelet-anonymous-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39574"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-anonymous-auth
    uid: 45010a11-a0f2-47b1-84b1-158f52a70325
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure authorization is set to Webhook
    Ensuring that the authorization is configured correctly helps enforce that
    unauthenticated/unauthorized users have no access to OpenShift nodes.
  id: xccdf_org.ssgproject.content_rule_kubelet_authorization_mode
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep -A1 authorization /etc/kubernetes/kubelet.conf
    Verify that the output is not set to mode: AlwaysAllow, or missing
    (defaults to mode: Webhook).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-authorization-mode
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-kubelet-authorization-mode
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39501"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-authorization-mode
    uid: 077c0e2b-c4cb-4693-8a46-d1036ca74f6d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Configure the Client CA Certificate
    Not having a CA certificate for the kubelet will subject the kubelet to possible
    man-in-the-middle attacks especially on unsafe or untrusted networks.
    Certificate validation for the kubelet allows the API server to validate
    the kubelet's identity.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_client_ca
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep -A1 x509 /etc/kubernetes/kubelet.conf
    The output should contain a configured certificate like /etc/kubernetes/kubelet-ca.crt.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-client-ca
    creationTimestamp: "2021-07-13T16:58:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:11Z"
    name: ocp4-cis-node-worker-kubelet-configure-client-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39550"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-configure-client-ca
    uid: 65a53b32-cfa0-4487-bfc6-39b7ebcafce9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Kubelet - Ensure Event Creation Is Configured
    It is important to capture all events and not restrict event creation.
    Events are an important source of security information and analytics that
    ensure that your environment is consistently monitored using the event
    data.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_event_creation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep eventRecordQPS /etc/kubernetes/kubelet.conf
    The output should return .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-event-creation
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-kubelet-configure-event-creation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39472"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-configure-event-creation
    uid: 03da6b2f-dfd1-47ef-9a94-74c07037e26f
  severity: medium
  status: FAIL
  warnings:
  - The MachineConfig Operator does not merge KubeletConfig objects, the last object is used instead. In case you need to set multiple options for kubelet, consider putting all the custom options into a single KubeletConfig object.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers
    TLS ciphers have had a number of known vulnerabilities and weaknesses,
    which can reduce the protection provided by them. By default Kubernetes
    supports a number of TLS ciphersuites including some that have security
    concerns, weakening the protection provided.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_tls_cipher_suites
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep tlsCipherSuites /etc/kubernetes/kubelet.conf
    Verify that the set of ciphers contains only the following:

    TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
    TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
    TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,
    TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-tls-cipher-suites
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-kubelet-configure-tls-cipher-suites
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39573"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-configure-tls-cipher-suites
    uid: a606b4fb-576d-47cb-a87f-a00a3f2c401e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Disable Hostname Override
    Allowing hostnames to be overrided creates issues around resolving nodes
    in addition to TLS configuration, certificate validation, and log correlation
    and validation.
  id: xccdf_org.ssgproject.content_rule_kubelet_disable_hostname_override
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep hostname-override /etc/systemd/system/kubelet.service
    The output should return no output.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-disable-hostname-override
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-kubelet-disable-hostname-override
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39529"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-disable-hostname-override
    uid: 5998372e-0ccd-4a83-8e66-28e8be80d965
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Certificate Rotation
    Allowing the kubelet to auto-update the certificates ensure that there is no downtime
    in certificate renewal as well as ensures confidentiality and integrity.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_cert_rotation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep rotateCertificates /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-cert-rotation
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-kubelet-enable-cert-rotation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39451"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-enable-cert-rotation
    uid: 1cf88101-f5d7-4bc7-9390-a0fb4ba95432
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Client Certificate Rotation
    Allowing the kubelet to auto-update the certificates ensure that there is no downtime
    in certificate renewal as well as ensures confidentiality and integrity.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_client_cert_rotation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep RotateKubeletClientCertificate /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-client-cert-rotation
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-kubelet-enable-client-cert-rotation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39570"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-enable-client-cert-rotation
    uid: b02d5813-0235-40f9-a505-ef7913fc18bf
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Allow Automatic Firewall Configuration
    The kubelet should automatically configure the firewall settings to allow access and
    networking traffic through. This ensures that when a pod or container is running that
    the correct ports are configured as well as removing the ports when a pod or
    container is no longer in existence.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_iptables_util_chains
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep makeIPTablesUtilChains /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-iptables-util-chains
    creationTimestamp: "2021-07-13T16:58:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:09Z"
    name: ocp4-cis-node-worker-kubelet-enable-iptables-util-chains
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39500"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-enable-iptables-util-chains
    uid: 776317ce-7af5-4397-be4d-b868b30ca294
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Protect Kernel Defaults
    Kernel parameters are usually tuned and hardened by the system administrators
    before putting the systems into production. These parameters protect the
    kernel and the system. Your kubelet kernel defaults that rely on such
    parameters should be appropriately set to match the desired secured system
    state. Ignoring this could potentially lead to running pods with undesired
    kernel behavior.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_protect_kernel_defaults
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep protectKernelDefaults /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-protect-kernel-defaults
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-kubelet-enable-protect-kernel-defaults
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39455"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-enable-protect-kernel-defaults
    uid: 38b2acb5-c4fc-4793-9661-022ac7cacabc
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Enable Server Certificate Rotation
    Allowing the kubelet to auto-update the certificates ensure that there is no downtime
    in certificate renewal as well as ensures confidentiality and integrity.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_server_cert_rotation
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep RotateKubeletServerCertificate /etc/kubernetes/kubelet.conf
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-server-cert-rotation
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-kubelet-enable-server-cert-rotation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39601"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-enable-server-cert-rotation
    uid: fea590c5-23fc-4936-ae6d-fd0f6e822cab
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Do Not Disable Streaming Timeouts
    Ensuring connections have timeouts helps to protect against denial-of-service attacks as
    well as disconnect inactive connections. In addition, setting connections timeouts helps
    to prevent from running out of ephemeral ports.
  id: xccdf_org.ssgproject.content_rule_kubelet_enable_streaming_connections
  instructions: |-
    Run the following command on the kubelet node(s):
    $ sudo grep streamingConnectionIdleTimeout /etc/kubernetes/kubelet.conf
    The output should return .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-enable-streaming-connections
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-kubelet-enable-streaming-connections
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39593"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-enable-streaming-connections
    uid: 88cc16d7-3bd4-4307-b1cf-76d71fc9e54d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."imagefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-imagefs-available
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39530"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-available
    uid: 6b36b0a8-4cc2-4e7a-b924-f97270cbe712
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."imagefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-imagefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39595"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree
    uid: a2f7ea43-2276-4e1b-a25e-fd6271ed5131
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: memory.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_memory_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."memory.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-memory-available
    creationTimestamp: "2021-07-13T16:58:10Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:10Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-memory-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39533"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-memory-available
    uid: 0d9f398c-f5c9-45d1-91bb-7153da440e6b
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."nodefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-nodefs-available
    creationTimestamp: "2021-07-13T16:58:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:06Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39454"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-available
    uid: a18917ef-a28d-4d1e-aef8-18eab59841a2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionHard."nodefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-hard-nodefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:13Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39594"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree
    uid: 82bc1027-60d6-43b8-aebc-7084d502a2f2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."imagefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-imagefs-available
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39608"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-available
    uid: e7ce1e41-47fd-4ac2-b448-7f5b2601ba6f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."imagefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-imagefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:08Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39484"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree
    uid: fd305dae-f0fa-46ca-81c3-ef1b9287ea2f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: memory.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_memory_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."memory.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-memory-available
    creationTimestamp: "2021-07-13T16:58:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:07Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-memory-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39475"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-memory-available
    uid: bba53d9a-8c3a-45e0-ab26-72742c2fa11a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.available
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_available
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."nodefs.available"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-nodefs-available
    creationTimestamp: "2021-07-13T16:58:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:14Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-available
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39612"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-available
    uid: 52877574-5dc0-46a6-9623-b3f83b600be3
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFree
    Garbage collection is important to ensure sufficient resource availability
    and avoiding degraded performance and availability. In the worst case, the
    system might crash or just be unusable for a long period of time.
    Based on your system resources and tests, choose an appropriate threshold
    value to activate garbage collection.
  id: xccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc debug -q node/$NODE -- jq -r '.evictionSoft."nodefs.inodesFree"' /host/etc/kubernetes/kubelet.conf
    and make sure it outputs
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-eviction-thresholds-set-soft-nodefs-inodesfree
    creationTimestamp: "2021-07-13T16:58:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-cis-node-worker
      compliance.openshift.io/suite: cis-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:12Z"
    name: ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis-node-worker
      uid: fc5bf5eb-38b2-4dd1-8601-c3cacb0b740d
    resourceVersion: "39580"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree
    uid: b1b3970b-f1e5-4cb2-a824-35684994b9a7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the OpenShift API Server Maximum Retained Audit Logs
    OpenShift automatically rotates the log files. Retaining old log files ensures
    OpenShift Operators will have sufficient log data available for carrying out
    any investigation or correlation. For example, if the audit log size is set to
    100 MB and the number of retained log files is set to 10, OpenShift Operators
    would have approximately 1 GB of log data to use during analysis.
  id: xccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxbackup
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxbackup"][0]'
    The output should return a value of 10 or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-api-server-audit-log-maxbackup
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-ocp-api-server-audit-log-maxbackup
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38236"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-ocp-api-server-audit-log-maxbackup
    uid: 3fe8b373-8155-49bd-8ed1-0ea87540bb96
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure OpenShift API Server Maximum Audit Log Size
    OpenShift automatically rotates log files. Retaining old log files ensures that
    OpenShift Operators have sufficient log data available for carrying out any
    investigation or correlation. If you have set file size of 100 MB and the number of
    old log files to keep as 10, there would be approximately 1 GB of log data
    available for use in analysis.
  id: xccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxsize
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxsize"]'
    The output should return a value of ["100"] or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-api-server-audit-log-maxsize
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-cis-ocp-api-server-audit-log-maxsize
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38331"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-ocp-api-server-audit-log-maxsize
    uid: c76e569d-edba-472c-9498-67ae9a8554d4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Audit Log Path
    Auditing of the API Server is not enabled by default. Auditing the API Server
    provides a security-relevant chronological set of records documenting the sequence
    of activities that have affected the system by users, administrators, or other
    system components.
  id: xccdf_org.ssgproject.content_rule_openshift_api_server_audit_log_path
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-path"]'
    The output should return a valid audit log path.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: openshift-api-server-audit-log-path
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-openshift-api-server-audit-log-path
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38290"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-openshift-api-server-audit-log-path
    uid: 39ded534-b9e6-4125-b905-a409057e2590
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Profiling is protected by RBAC
    Profiling allows for the identification of specific performance bottlenecks.
    It generates a significant amount of program data that could potentially be
    exploited to uncover system and program details. If you are not experiencing
    any bottlenecks and do not need the profiler for troubleshooting purposes, it
    is recommended to turn it off to reduce the potential attack surface. To
    ensure the collected data is not exploited, profiling endpoints are secured
    via RBAC (see cluster-debugger role). By default, the profiling endpoints are
    accessible only by users bound to cluster-admin or cluster-debugger role.
    Profiling can not be disabled.
  id: xccdf_org.ssgproject.content_rule_rbac_debug_role_protects_pprof
  instructions: |-
    To verify that the cluster-debugger role is configured correctly,
    run the following command:
    $ oc get clusterroles cluster-debugger -o jsonpath='{.rules[0].nonResourceURLs}'
    and verify that the /debug/pprof path is included there.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-debug-role-protects-pprof
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-rbac-debug-role-protects-pprof
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38455"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-rbac-debug-role-protects-pprof
    uid: 9b737deb-5ebb-4327-8f90-b3f76b1ebac7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the cluster-admin role is only used where required
    Kubernetes provides a set of default roles where RBAC is used. Some of these
    roles such as cluster-admin provide wide-ranging privileges which should
    only be applied where absolutely necessary. Roles such as cluster-admin
    allow super-user access to perform any action on any resource. When used in
    a ClusterRoleBinding, it gives full control over every resource in the
    cluster and in all namespaces. When used in a RoleBinding, it gives full
    control over every resource in the rolebinding's namespace, including the
    namespace itself.
  id: xccdf_org.ssgproject.content_rule_rbac_limit_cluster_admin
  instructions: |-
    Review users and groups bound to cluster-admin and decide whether they
    require such access. Consider creating least-privilege roles for users and
    service accounts. Obtain a list of the users who have access to the
    cluster-admin role by reviewing the clusterrolebinding output for each role
    binding that has access to the cluster-admin role. To do this, run the
    following command:
    $ oc get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].kind | grep cluster-admin

    Care should be taken before removing any clusterrolebindings from the
    environment to ensure they are not required for operation of the cluster.
    Specifically, modifications should not be made to the default
    clusterrolebindings including those with the "system:" prefix.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-limit-cluster-admin
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-cis-rbac-limit-cluster-admin
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38457"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-rbac-limit-cluster-admin
    uid: d8005791-ef08-48d7-81be-fbb0d22f3225
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to Kubernetes Secrets
    Inappropriate access to secrets stored within the Kubernetes
    cluster can allow for an attacker to gain additional access to
    the Kubernetes cluster or external resources whose credentials
    are stored as secrets.
  id: xccdf_org.ssgproject.content_rule_rbac_limit_secrets_access
  instructions: |-
    To review the policy rules assigned to roles in all namespaces, run
    the following command:
    $ for ns in $(oc get projects -ojsonpath='{.items[*].metadata.name}'); do oc describe roles -n$ns; done
    To review the policy rules assigned to cluster roles, run the following
    command:
    $ for i in $(oc get clusterroles -o jsonpath='{.items[*].metadata.name}'); do oc describe clusterrole ${i}; done
    Review the output and ensure that only authorized roles have access to the
    secrets resource or all resources using a wildcard.
    To filter clusterroles that have assigned access to the secrets resources for clustrroles, run:
    $ oc get clusterroles -ojson | jq -r '.items[] | {name: .metadata.name, rules: .rules} | select(.rules[]?.resources | try contains(["secrets"])) | .name' | sort | uniq
    and similarly to filter namespace/role pairs:
    $ for ns in $(oc get projects -ojsonpath='{.items[*].metadata.name}'); do oc get roles -n$ns -ojson | jq -r '.items[] | {name: .metadata.name, namespace: .metadata.namespace, rules: .rules} | select(.rules[]?.resources | try contains(["secrets"])) | "\(.namespace)/\(.name)"' | sort | uniq; done
    note that the two commands above do not show roles and/or clusterroles with wildcard access to any resources.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-limit-secrets-access
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-cis-rbac-limit-secrets-access
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38469"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-rbac-limit-secrets-access
    uid: 8c71e503-cbee-47a0-b33d-3edb34980959
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Minimize Access to Pod Creation
    The ability to create pods in a cluster opens up the cluster
    for privilege escalation.
  id: xccdf_org.ssgproject.content_rule_rbac_pod_creation_access
  instructions: |-
    To review the pod creation privileges in roles, run the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    Review the output, and for any role/clusterrole defining create permissions
    for pods that are NOT an OpenShift "system:" or other system-provided
    role/clusterrole, determine if the users bound to the role truly have the
    need to create pods.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-pod-creation-access
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-rbac-pod-creation-access
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38307"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-rbac-pod-creation-access
    uid: 797b58d3-b12e-4e05-9155-2fec58c09a68
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Minimize Wildcard Usage in Cluster and Local Roles
    The principle of least privilege recommends that users are
    provided only the access required for their role and nothing
    more. The use of wildcard rights grants is likely to provide
    excessive rights to the Kubernetes API.
  id: xccdf_org.ssgproject.content_rule_rbac_wildcard_use
  instructions: |-
    To review the wildcard usage in roles, run the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    Review the output, and for any role/clusterrole specifying a wildcard
    resource that is NOT an OpenShift "system:" or other system-provided
    role/clusterrole, determine if the wildcard access can be replaced with
    specific resources.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-wildcard-use
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-rbac-wildcard-use
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38230"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-rbac-wildcard-use
    uid: e19394a5-9923-4e3a-a211-58927718f939
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Drop Container Capabilities
    By default, containers run with a default set of capabilities as assigned
    by the Container Runtime which can include dangerous or highly privileged
    capabilities. Capabilities should be dropped unless absolutely critical for
    the container to run software as added capabilities that are not required
    allow for malicious containers or attackers.
  id: xccdf_org.ssgproject.content_rule_scc_drop_container_capabilities
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that do not list any requiredDropCapabilities, examine the
    associated rolebindings to account for the users that are bound to the role.
    Review each SCC and determine that all capabilities are either
    completely disabled as a list entry under requiredDropCapabilities,
    or that all the un-required capabilities are dropped for containers and SCCs.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-drop-container-capabilities
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-scc-drop-container-capabilities
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38232"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-drop-container-capabilities
    uid: 05de867d-2633-4f5c-8f12-e5702ee694aa
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Container Capabilities
    By default, containers run with a default set of capabilities as assigned
    by the Container Runtime which can include dangerous or highly privileged
    capabilities. Capabilities should be dropped unless absolutely critical for
    the container to run software as added capabilities that are not required
    allow for malicious containers or attackers.
  id: xccdf_org.ssgproject.content_rule_scc_limit_container_allowed_capabilities
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that do not list an explicit allowedCapabilities, examine the
    associated rolebindings to account for the users that are bound to the role.
    Review each SCC and determine that only required capabilities are either
    completely added as a list entry under allowedCapabilities,
    or that all the un-required capabilities are dropped for containers and SCCs.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-container-allowed-capabilities
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-scc-limit-container-allowed-capabilities
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38241"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-container-allowed-capabilities
    uid: 116b3231-d7fa-4f69-a553-4c38ef41d174
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to the Host IPC Namespace
    A container running in the host's IPC namespace can use IPC
    to interact with processes outside the container potentially
    allowing an attacker to exploit a host process thereby enabling an
    attacker to exploit other services.
  id: xccdf_org.ssgproject.content_rule_scc_limit_ipc_namespace
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowHostIPC set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowHostIPC, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowHostIPC is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-ipc-namespace
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-cis-scc-limit-ipc-namespace
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38237"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-ipc-namespace
    uid: b66f3020-be4d-4760-adbf-f648d7c65c0a
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Use of the CAP_NET_RAW
    By default, containers run with a default set of capabilities as assigned
    by the Container Runtime which can include dangerous or highly privileged
    capabilities. If the CAP_NET_RAW is enabled, it may be misused
    by malicious containers or attackers.
  id: xccdf_org.ssgproject.content_rule_scc_limit_net_raw_capability
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that do not have NET_RAW or ALL set under requiredDropCapabilities.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that do not drop NET_RAW or ALL, examine the
    associated rolebindings to account for the users that are bound to the role.
    Review each SCC and determine that either NET_RAW or ALL
    is either included as a list entry under requiredDropCapabilities,
    or that either NET_RAW or ALL is only enabled to a small
    set of containers and SCCs.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-net-raw-capability
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-scc-limit-net-raw-capability
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38392"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-net-raw-capability
    uid: d9f68c56-ef46-4683-964b-9ad3f0678117
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to the Host Network Namespace
    A container running in the host's network namespace could
    access the host network traffic to and from other pods
    potentially allowing an attacker to exploit pods and network
    traffic.
  id: xccdf_org.ssgproject.content_rule_scc_limit_network_namespace
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowHostNetwork set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowHostNetwork, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowHostNetwork is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-network-namespace
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-scc-limit-network-namespace
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38274"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-network-namespace
    uid: eb596a37-0853-447d-b0a3-9407414de7c3
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Containers Ability to Escalate Privileges
    Privileged containers have access to more of the Linux Kernel
    capabilities and devices. If a privileged container were
    compromised, an attacker would have full access to the container
    and host.
  id: xccdf_org.ssgproject.content_rule_scc_limit_privilege_escalation
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowPrivilegeEscalation set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowPrivilegeEscalation, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowPrivilegeEscalation is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-privilege-escalation
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-cis-scc-limit-privilege-escalation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38388"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-privilege-escalation
    uid: 315a0ca1-e0d5-45bc-9f9d-2939b0afa714
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Privileged Container Use
    Privileged containers have access to all Linux Kernel
    capabilities and devices. If a privileged container were
    compromised, an attacker would have full access to the container
    and host.
  id: xccdf_org.ssgproject.content_rule_scc_limit_privileged_containers
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowPrivilegedContainer set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowPrivilegedContainer, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowPrivilegedContainer is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-privileged-containers
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-scc-limit-privileged-containers
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38323"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-privileged-containers
    uid: 6747e7b6-661b-4781-9ee9-606f28d1ad3b
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to the Host Process ID Namespace
    A container running in the host's PID namespace can inspect
    processes running outside the container which can be used to
    escalate privileges outside of the container.
  id: xccdf_org.ssgproject.content_rule_scc_limit_process_id_namespace
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowHostPID set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowHostPID, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowHostPID is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-process-id-namespace
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-cis-scc-limit-process-id-namespace
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38253"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-process-id-namespace
    uid: bafe4228-849b-4d0b-a899-a2da92cc97f8
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Container Running As Root User
    Privileged containers have access to all Linux Kernel
    capabilities and devices. If a privileged container were
    compromised, an attacker would have full access to the container
    and host.
  id: xccdf_org.ssgproject.content_rule_scc_limit_root_containers
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowPrivilegedContainer set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowPrivilegedContainer, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowPrivilegedContainer is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-root-containers
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-scc-limit-root-containers
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38436"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scc-limit-root-containers
    uid: 8a2c6ec1-3d7d-4bc5-9cf2-01d294b903bf
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the bind-address parameter is not used
    In OpenShift 4, The Kubernetes Scheduler operator manages and updates the
    Kubernetes Scheduler deployed on top of OpenShift. By default, the operator
    exposes metrics via metrics service. The metrics are collected from the
    Kubernetes Scheduler operator. Profiling data is sent to healthzPort,
    the port of the localhost healthz endpoint. Changing this value may disrupt
    components that monitor the kubelet health.
  id: xccdf_org.ssgproject.content_rule_scheduler_no_bind_address
  instructions: |-
    Run the following command:
    oc get -nopenshift-kube-scheduler cm kube-scheduler-pod -ojson | jq -r '.data["pod.yaml"]' | jq -r | grep bind-address
    The output should be empty
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scheduler-no-bind-address
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-cis-scheduler-no-bind-address
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38426"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-scheduler-no-bind-address
    uid: 48086f78-e3e8-42e4-9e87-691824c85503
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Consider external secret storage
    Kubernetes supports secrets as first-class objects, but care needs to be
    taken to ensure that access to secrets is carefully limited. Using an
    external secrets provider can ease the management of access to secrets,
    especially where secrets are used across both Kubernetes and non-Kubernetes
    environments.
  id: xccdf_org.ssgproject.content_rule_secrets_consider_external_storage
  instructions: |-
    Review the cluster configuration and determine if an appropriate secrets
    manager has been configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: secrets-consider-external-storage
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-secrets-consider-external-storage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38309"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-secrets-consider-external-storage
    uid: 7dffbcce-86ba-482d-98da-75589ccf9c0c
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Do Not Use Environment Variables with Secrets
    Environment variables are subject and very susceptible to
    malicious hijacking methods by an adversary, as such,
    environment variables should never be used for secrets.
  id: xccdf_org.ssgproject.content_rule_secrets_no_environment_variables
  instructions: |-
    To find workloads that use environment variables for secrets, run the following:
    $ oc get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.namespace} {.metadata.name} {"\n"}{end}' -A
    Review the output and ensure that workloads that can mount secrets as data
    volumes use that instead of environment variables.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: secrets-no-environment-variables
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-cis
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"031cffc9-a4f5-4af4-b30b-d3c1b551c452"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-cis-secrets-no-environment-variables
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-cis
      uid: 031cffc9-a4f5-4af4-b30b-d3c1b551c452
    resourceVersion: "38300"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-cis-secrets-no-environment-variables
    uid: 5c29fca5-c966-436e-aaf5-80b40ce3ad03
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Restrict Automounting of Service Account Tokens
    Mounting service account tokens inside pods can provide an avenue
    for privilege escalation attacks where an attacker is able to
    compromise a single pod in the cluster.
  id: xccdf_org.ssgproject.content_rule_accounts_restrict_service_account_tokens
  instructions: |-
    For each pod in the cluster, review the pod specification and
    ensure that pods that do not need to explicitly communicate with
    the API server have automountServiceAccountToken
    configured to false.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: accounts-restrict-service-account-tokens
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-accounts-restrict-service-account-tokens
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38416"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-accounts-restrict-service-account-tokens
    uid: dead8375-31b0-4193-b7b0-9f3dddab0460
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Ensure Usage of Unique Service Accounts \nKubernetes provides a default service account which is used by\ncluster workloads where no specific service account is assigned to the pod.\nWhere access to the Kubernetes API from a pod is required, a specific service account\nshould be created for that pod, and rights granted to that service account.\nThis increases auditability of service account rights and access making it\neasier and more accurate to trace potential malicious behaviors to a specific\nservice account and project."
  id: xccdf_org.ssgproject.content_rule_accounts_unique_service_account
  instructions: |-
    For each namespace in the cluster, review the rights assigned
    to the default service account. There should be no cluster or local roles
    assigned to the default other than the defaults.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: accounts-unique-service-account
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-accounts-unique-service-account
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38329"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-accounts-unique-service-account
    uid: e6beba73-a498-4101-8a78-9b8c35933614
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable the AlwaysAdmit Admission Control Plugin
    Enabling the admission control plugin AlwaysAdmit allows all
    requests and does not provide any filtering.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysAdmit
  instructions: |-
    To verify that the AlwaysAdmit admission control plugin is not set, run the following command:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
    The output should not contain AlwaysAdmit
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-alwaysadmit
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-api-server-admission-control-plugin-alwaysadmit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38315"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-alwaysadmit
    uid: 53d79292-aabf-4cfd-9a0c-e24f1860551c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the Admission Control Plugin AlwaysPullImages is not set
    Setting admission control policy to AlwaysPullImages forces every new pod
    to pull the required images every time. In a multi-tenant cluster users can
    be assured that their private images can only be used by those who have the
    credentials to pull them. Without this admission control policy, once an
    image has been pulled to a node, any pod from any user can use it simply by
    knowing the imageâ€™s name, without any authorization check against the image
    ownership. When this plug-in is enabled, images are always pulled prior to
    starting containers, which means valid credentials are required.

    However, turning on this admission plugin can introduce new kinds of
    cluster failure modes. OpenShift 4 master and infrastructure components are
    deployed as pods. Enabling this feature can result in cases where loss of
    contact to an image registry can cause a redeployed infrastructure pod
    (oauth-server for example) to fail on an image pull for an image that is
    currently present on the node. We use PullIfNotPresent so that a loss of
    image registry access does not prevent the pod from starting.  If it
    becomes PullAlways, then an image registry access outage can cause key
    infrastructure components to fail.

    The pull policy can be managed per container, using
    imagePullPolicy.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysPullImages
  instructions: |-
    Run the following command:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
    The output list should not contain "AlwaysPullImages".
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-alwayspullimages
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-api-server-admission-control-plugin-alwayspullimages
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38359"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-alwayspullimages
    uid: 4509b312-4814-46cc-b214-f7b2122e5efe
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the NamespaceLifecycle Admission Control Plugin
    Setting admission control policy to NamespaceLifecycle ensures that
    objects cannot be created in non-existent namespaces, and that namespaces
    undergoing termination are not used for creating new objects. This
    is recommended to enforce the integrity of the namespace termination process
    and also for the availability of new objects.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NamespaceLifecycle
  instructions: |-
    To verify that the NamespaceLifecycle plugin is enabled in
    the apiserver configuration, run:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-namespacelifecycle
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-api-server-admission-control-plugin-namespacelifecycle
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38382"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-namespacelifecycle
    uid: 03a5a2f0-d09a-4093-a729-8e4a023dd4a8
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the NodeRestriction Admission Control Plugin
    Using the NodeRestriction plugin ensures that the kubelet is
    restricted to the Node and Pod objects that it could
    modify as defined. Such kubelets will only be allowed to modify their
    own Node API object, and only modify Pod API objects
    that are bound to their node.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NodeRestriction
  instructions: |-
    Ensure that the NodeRestriction plugin is enabled in the list of enabled plugins in
    the apiserver configuration by running the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep 'NodeRestriction'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-noderestriction
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-api-server-admission-control-plugin-noderestriction
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38421"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-noderestriction
    uid: 31cca5a3-6ac9-4d6f-85a7-80bd554d770a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the SecurityContextConstraint Admission Control Plugin
    A Security Context Constraint is a cluster-level resource that controls the actions
    which a pod can perform and what the pod may access. The
    SecurityContextConstraint objects define a set of conditions that a pod
    must run with in order to be accepted into the system. Security Context Constraints
    are comprised of settings and strategies that control the security features
    a pod has access to and hence this must be used to control pod access
    permissions.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_Scc
  instructions: |-
    The SecurityContextConstraint plugin should be enabled in the list of enabled plugins in
    the apiserver configuration:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-scc
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-api-server-admission-control-plugin-scc
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38484"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-scc
    uid: 20cdb05d-7bf6-4bd8-b511-0095cb4bf468
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used
    The SecurityContextDeny admission control plugin disallows
    setting any security options for your pods. SecurityContextConstraints
    allow you to enforce RBAC rules on who can set these options on the pods, and
    what they're allowed to set. Thus, using the SecurityContextDeny
    will deter you from enforcing granular permissions on your pods.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_SecurityContextDeny
  instructions: |-
    The SecurityContextDeny plugin should not be enabled in the list of enabled plugins in the apiserver configuration:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-securitycontextdeny
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-admission-control-plugin-securitycontextdeny
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38249"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-securitycontextdeny
    uid: a880cf0d-53b6-460c-bcaa-60b0514285bc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the ServiceAccount Admission Control Plugin
    When a pod is created, if a service account is not specified, the pod
    is automatically assigned the default service account in the same
    namespace. OpenShift operators should create unique service accounts
    and let the API Server manage its security tokens.
  id: xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_ServiceAccount
  instructions: |-
    The ServiceAccount plugin should be enabled in the list of enabled plugins in
    the apiserver configuration:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-admission-control-plugin-serviceaccount
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-admission-control-plugin-serviceaccount
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38299"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-admission-control-plugin-serviceaccount
    uid: 4123f18c-fb1d-4178-92af-b8903bb01279
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that anonymous requests to the API Server are authorized
    When enabled, requests that are not rejected by other configured
    authentication methods are treated as anonymous requests. These requests
    are then served by the API server. If you are using RBAC authorization,
    it is generally considered reasonable to allow anonymous access to the
    API Server for health checks and discovery purposes, and hence this
    recommendation is not scored. However, you should consider whether
    anonymous discovery is an acceptable risk for your purposes.
  id: xccdf_org.ssgproject.content_rule_api_server_anonymous_auth
  instructions: |-
    Run the following command to view the authorization rules for anonymous requests:
    $ oc describe clusterrolebindings
    Make sure that there exists at least one clusterrolebinding that binds
    either the system:unauthenticated group or the system:anonymous
    user.
    To test that an anonymous request is authorized to access the readyz
    endpoint, run:
    $ oc get --as="system:anonymous" --raw='/readyz?verbose'
    In contrast, a request to list all projects should not be authorized:
    $ oc get --as="system:anonymous" projects
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-anonymous-auth
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-api-server-anonymous-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38405"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-anonymous-auth
    uid: d23d3ebc-1e5d-471c-aa2a-838430283da6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure catch-all FlowSchema object for API Priority and Fairness Exists
    The FlowSchema API objects enforce a limit on the
    number of events that the API Server will accept in a given time slice
    In a large multi-tenant cluster, there might be a small percentage of
    misbehaving tenants which could have a significant impact on the
    performance of the cluster overall. It is recommended to limit the rate
    of events that the API Server will accept.
  id: xccdf_org.ssgproject.content_rule_api_server_api_priority_flowschema_catch_all
  instructions: |-
    Run the following commands:
    oc get flowschema
    and inspect the FlowSchema objects. Make sure that at least the catch-all
    object exists by calling:
    oc describe flowschema catch-all
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-api-priority-flowschema-catch-all
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: NOT-APPLICABLE
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-api-priority-flowschema-catch-all
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38262"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-api-priority-flowschema-catch-all
    uid: dc67f5af-b576-42d3-b841-c6e4034064ce
  severity: medium
  status: NOT-APPLICABLE
  warnings:
  - Note that this is only applicable in OpenShift Container Platform version 4.8 and higher
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the APIPriorityAndFairness feature gate
    The APIPriorityAndFairness feature gate enables the use of the
    FlowSchema API objects which enforce a limit on the number of
    events that the API Server will accept in a given time slice In a large
    multi-tenant cluster, there might be a small percentage of misbehaving
    tenants which could have a significant impact on the performance of
    the cluster overall. It is recommended to limit the rate of events
    that the API Server will accept.
  id: xccdf_org.ssgproject.content_rule_api_server_api_priority_gate_enabled
  instructions: |-
    To verify that APIPriorityAndFairness is enabled, run the following command:
    oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.apiServerArguments["feature-gates"]'
    The output should contain "APIPriorityAndFairness=true"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-api-priority-gate-enabled
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-api-server-api-priority-gate-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38403"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-api-priority-gate-enabled
    uid: 9b4f7697-b59a-46e9-a1ba-f00afeb50e11
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure catch-all FlowSchema object for API Priority and Fairness Exists (v1alpha1)
    The FlowSchema API objects enforce a limit on the
    number of events that the API Server will accept in a given time slice
    In a large multi-tenant cluster, there might be a small percentage of
    misbehaving tenants which could have a significant impact on the
    performance of the cluster overall. It is recommended to limit the rate
    of events that the API Server will accept.
  id: xccdf_org.ssgproject.content_rule_api_server_api_priority_v1alpha1_flowschema_catch_all
  instructions: |-
    Run the following commands:
    oc get flowschema
    and inspect the FlowSchema objects. Make sure that at least the catch-all
    object exists by calling:
    oc describe flowschema catch-all
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-api-priority-v1alpha1-flowschema-catch-all
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-api-priority-v1alpha1-flowschema-catch-all
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38257"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-api-priority-v1alpha1-flowschema-catch-all
    uid: 53eacec4-f03f-4764-a918-628fe20f99a7
  severity: medium
  status: PASS
  warnings:
  - Note that this rule is only applicable in OpenShift Container Platform versions 4.7 and below.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Kubernetes API Server Maximum Retained Audit Logs
    OpenShift automatically rotates the log files. Retaining old log files ensures
    OpenShift Operators will have sufficient log data available for carrying out
    any investigation or correlation. For example, if the audit log size is set to
    100 MB and the number of retained log files is set to 10, OpenShift Operators
    would have approximately 1 GB of log data to use during analysis.
  id: xccdf_org.ssgproject.content_rule_api_server_audit_log_maxbackup
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxbackup"][0]'
    The output should return a value of 10 or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-audit-log-maxbackup
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-api-server-audit-log-maxbackup
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38407"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-audit-log-maxbackup
    uid: c5e02977-b64e-43df-abb8-4fc0b0d9a93a
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Kubernetes API Server Maximum Audit Log Size
    OpenShift automatically rotates log files. Retaining old log files ensures that
    OpenShift Operators have sufficient log data available for carrying out any
    investigation or correlation. If you have set file size of 100 MB and the number of
    old log files to keep as 10, there would be approximately 1 GB of log data
    available for use in analysis.
  id: xccdf_org.ssgproject.content_rule_api_server_audit_log_maxsize
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxsize"]'
    The output should return a value of ["100"] or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-audit-log-maxsize
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-api-server-audit-log-maxsize
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38476"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-audit-log-maxsize
    uid: 9ef21d8c-2f38-43b9-ab2f-9c1e2e964906
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Audit Log Path
    Auditing of the Kubernetes API Server is not enabled by default. Auditing the API Server
    provides a security-relevant chronological set of records documenting the sequence
    of activities that have affected the system by users, administrators, or other
    system components.
  id: xccdf_org.ssgproject.content_rule_api_server_audit_log_path
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-path"]'
    The output should return a valid audit log path.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-audit-log-path
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-api-server-audit-log-path
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38482"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-audit-log-path
    uid: 6d0832d4-3647-4ad4-ae48-90f7986ff642
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    The authorization-mode cannot be AlwaysAllow
    The API Server, can be configured to allow all requests. This mode should not be used on any production cluster.
  id: xccdf_org.ssgproject.content_rule_api_server_auth_mode_no_aa
  instructions: |-
    To verify that the Node authorization mode is be configured and enabled in
    the apiserver configuration, run:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep '"authorization-mode":\[[^]]*"AlwaysAllow"'
    The output should be empty - the "authorization-mode" list does NOT contain the "AlwaysAllow" authorizer.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-auth-mode-no-aa
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-api-server-auth-mode-no-aa
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38495"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-auth-mode-no-aa
    uid: e64c95ca-8f34-4a53-9250-6c7de2b4d13b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure authorization-mode Node is configured
    The Node authorization mode only allows kubelets to read Secret,
    ConfigMap, PersistentVolume, and PersistentVolumeClaim objects
    associated with their nodes.
  id: xccdf_org.ssgproject.content_rule_api_server_auth_mode_node
  instructions: |-
    To verify that Node authorization mode is enabled, run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep '"authorization-mode":\[[^]]*"Node"'
    The output should show that the "authorization-mode" list contains the "Node" authorizer.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-auth-mode-node
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-auth-mode-node
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38261"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-auth-mode-node
    uid: 7a31dac3-7b84-4233-8791-24bae2afc389
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure authorization-mode RBAC is configured
    Role Based Access Control (RBAC) allows fine-grained control over the
    operations that different entities can perform on different objects in
    the cluster. Enabling RBAC is critical in regulating access to an
    OpenShift cluster as the RBAC rules specify, given a user, which operations
    can be executed over a set of namespaced or cluster-wide resources.
  id: xccdf_org.ssgproject.content_rule_api_server_auth_mode_rbac
  instructions: |-
    To verify that RBAC authorization mode is enabled, run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | grep '"authorization-mode":\[[^]]*"RBAC"'
    The output should show that the "authorization-mode" list contains the "RBAC" authorizer.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-auth-mode-rbac
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-api-server-auth-mode-rbac
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38336"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-auth-mode-rbac
    uid: 83061c6b-4f75-4bc5-8ad6-ae8868534ab4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable basic-auth-file for the API Server
    Basic authentication uses plaintext credentials for authentication.
    Currently the basic authentication credentials last indefinitely, and
    the password cannot be changed without restarting the API Server. The
    Basic Authentication is currently supported for convenience and is
    not intended for production workloads.
  id: xccdf_org.ssgproject.content_rule_api_server_basic_auth
  instructions: |-
    To verify that basic-auth-file is configured and enabled for the API server, run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["basic-auth-file"]'
    The output should be empty as OpenShift does not support basic authentication at all.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-basic-auth
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-api-server-basic-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38340"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-basic-auth
    uid: f41b2401-50b6-4660-b921-bcdef7cb2645
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the bindAddress is set to a relevant secure port
    The OpenShift API server is served over HTTPS with authentication and authorization;
    the secure API endpoint is bound to 0.0.0.0:6443 by default. In OpenShift, the only
    supported way to access the API server pod is through the load balancer and then through
    the internal service.  The value is set by the bindAddress argument under the servingInfo
    parameter.
  id: xccdf_org.ssgproject.content_rule_api_server_bind_address
  instructions: |-
    Run the following command:
    oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.servingInfo["bindAddress"]'
    The output should return 0.0.0.0:6443.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-bind-address
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-api-server-bind-address
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38456"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-bind-address
    uid: 089f40db-42b9-4c06-96e6-a3af4502d8ff
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Client Certificate Authority for the API Server
    API Server communication contains sensitive parameters that should remain
    encrypted in transit. Configure the API Server to serve only HTTPS traffic.
    If -clientCA is set, any request presenting a client
    certificate signed by one of the authorities in the client-ca-file
    is authenticated with an identity corresponding to the CommonName of
    the client certificate.
  id: xccdf_org.ssgproject.content_rule_api_server_client_ca
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["client-ca-file"]'
    The output should return a configured TLS CA certificate file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-client-ca
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-api-server-client-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38313"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-client-ca
    uid: 7a1791de-c09c-46c1-b400-93d417bc13e7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Encryption Provider Cipher
    aescbc is currently the strongest encryption provider, it should
    be preferred over other providers.
  id: xccdf_org.ssgproject.content_rule_api_server_encryption_provider_cipher
  instructions: |-
    Run the following command:
    $ oc get apiserver cluster -ojson | jq -r '.spec.encryption.type'
    The output should return aescdc as the encryption type.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-encryption-provider-cipher
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-encryption-provider-cipher
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38285"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-encryption-provider-cipher
    uid: 4258a01f-4266-4818-8b2e-4ae2e73f6d83
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Encryption Provider
    etcd is a highly available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are
    sensitive in nature and should be encrypted at rest to avoid any
    disclosures.
  id: xccdf_org.ssgproject.content_rule_api_server_encryption_provider_config
  instructions: |-
    Run the following command:
    $ oc get apiserver cluster -ojson | jq -r '.spec.encryption.type'
    The output should return aescdc as the encryption type.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-encryption-provider-config
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-encryption-provider-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38292"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-encryption-provider-config
    uid: 7eb4f73f-a51f-4d3f-94c2-9dbaf6818d74
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the etcd Certificate Authority for the API Server
    etcd is a highly-available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are
    sensitive in nature and should be protected by client authentication. This
    requires the API Server to identify itself to the etcd server using
    a SSL Certificate Authority file.
  id: xccdf_org.ssgproject.content_rule_api_server_etcd_ca
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["etcd-cafile"]'
    The output should return a configured CA certificate for ETCD.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-etcd-ca
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-api-server-etcd-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38454"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-etcd-ca
    uid: c2ca8394-1aa6-4de8-83b1-8947bd390975
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the etcd Certificate for the API Server
    etcd is a highly-available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are sensitive
    in nature and should be protected by client authentication. This requires the
    API Server to identify itself to the etcd server using a client certificate
    and key.
  id: xccdf_org.ssgproject.content_rule_api_server_etcd_cert
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."etcd-certfile"'
    The output should return a configured certificate file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-etcd-cert
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-etcd-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38258"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-etcd-cert
    uid: 529beca2-3561-4923-9589-2f0e4cfcdeb4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the etcd Certificate Key for the API Server
    etcd is a highly-available key-value store used by OpenShift deployments
    for persistent storage of all REST API objects. These objects are sensitive
    in nature and should be protected by client authentication. This requires the
    API Server to identify itself to the etcd server using a client certificate
    and key.
  id: xccdf_org.ssgproject.content_rule_api_server_etcd_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."etcd-keyfile"'
    The output should return a configured certificate key file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-etcd-key
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-api-server-etcd-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38317"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-etcd-key
    uid: 100f08cf-d9cf-4c83-84b3-8feba39ed813
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the --kubelet-https argument is set to true
    Connections from the kube-apiserver to kubelets could potentially carry
    sensitive data such as secrets and keys. It is thus important to use
    in-transit encryption for any communication between the apiserver and
    kubelets.
  id: xccdf_org.ssgproject.content_rule_api_server_https_for_kubelet_conn
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["kubelet-https"]'
    The output should return true, or no output at all.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-https-for-kubelet-conn
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-api-server-https-for-kubelet-conn
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38338"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-https-for-kubelet-conn
    uid: 1e5389a8-0b9b-4d26-9a1d-2bc97feaf7cf
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Use of the Insecure Bind Address
    If the API Server is bound to an insecure address the installation would
    be susceptible to unauthented and unencrypted access to the master node(s).
    The API Server does not perform authentication checking for insecure
    binds and the traffic is generally not encrypted.
  id: xccdf_org.ssgproject.content_rule_api_server_insecure_bind_address
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["insecure-bind-address"]'
    The output should be empty.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-insecure-bind-address
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-api-server-insecure-bind-address
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38430"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-insecure-bind-address
    uid: 16880f2f-91d3-4d68-b518-78db9f78690d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Prevent Insecure Port Access
    Configuring the API Server on an insecure port would allow unauthenticated
    and unencrypted access to your master node(s). It is assumed firewall rules
    will be configured to ensure this port is not reachable from outside
    the cluster, however as a defense in depth measure, OpenShift should not
    be configured to use insecure ports.
  id: xccdf_org.ssgproject.content_rule_api_server_insecure_port
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["insecure-port"]'
    The output should return 0.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-insecure-port
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-insecure-port
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38251"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-insecure-port
    uid: 702d53e1-364e-48d4-a28c-9d8f1c028a88
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the kubelet Certificate Authority for the API Server
    Connections from the API Server to the kubelet are used for fetching logs
    for pods, attaching (through kubectl) to running pods, and using the kubelet
    port-forwarding functionality. These connections terminate at the kubelet
    HTTPS endpoint. By default, the API Server does not verify the kubelet serving
    certificate, which makes the connection subject to man-in-the-middle attacks,
    and unsafe to run over untrusted and/or public networks.
  id: xccdf_org.ssgproject.content_rule_api_server_kubelet_certificate_authority
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."kubelet-certificate-authority"'
    The output should return a configured certificate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-kubelet-certificate-authority
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-api-server-kubelet-certificate-authority
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38444"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-kubelet-certificate-authority
    uid: e7c14e5f-0066-4c77-ab76-15a2ea176ccf
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the kubelet Certificate File for the API Server
    By default the API Server does not authenticate itself to the kublet's
    HTTPS endpoints. Requests from the API Server are treated anonymously.
    Configuring certificate-based kubelet authentication ensures that the
    API Server authenticates itself to kubelets when submitting requests.
  id: xccdf_org.ssgproject.content_rule_api_server_kubelet_client_cert
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["kubelet-client-certificate"]'
    The output should return /etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-kubelet-client-cert
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-api-server-kubelet-client-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38479"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-kubelet-client-cert
    uid: eaa6cf6d-1ad1-4282-81fc-9aa0ed31d4cb
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the kubelet Certificate Key for the API Server
    By default the API Server does not authenticate itself to the kubelet's
    HTTPS endpoints. Requests from the API Server are treated anonymously.
    Configuring certificate-based kubelet authentication ensures that the
    API Server authenticates itself to kubelets when submitting requests.
  id: xccdf_org.ssgproject.content_rule_api_server_kubelet_client_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["kubelet-client-key"]'
    The output should return /etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-kubelet-client-key
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-api-server-kubelet-client-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38497"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-kubelet-client-key
    uid: f425751c-1234-4ec1-bc17-0bd40910a172
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure all admission control plugins are enabled
    Several hardening controls depend on certain API server admission plugins
    being enabled. Checking that no admission control plugins are disabled
    helps assert that all the critical admission control plugins are indeed
    enabled and providing the security benefits required.
  id: xccdf_org.ssgproject.content_rule_api_server_no_adm_ctrl_plugins_disabled
  instructions: |-
    To verify that the list of disabled admission plugins is empty, run the following command:
    $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."disable-admission-plugins"'
    There should be no output.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-no-adm-ctrl-plugins-disabled
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-api-server-no-adm-ctrl-plugins-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38458"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-no-adm-ctrl-plugins-disabled
    uid: 1f8b5523-aad4-497a-be40-de237c938ec6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure the openshift-oauth-apiserver service uses TLS
    Connections between the kube-apiserver and the extension
    openshift-oauth-apiserver could potentially carry sensitive data such
    as secrets and keys. It is important to use in-transit encryption
    for any communication between the kube-apiserver and the extension
    openshift-apiserver.
  id: xccdf_org.ssgproject.content_rule_api_server_oauth_https_serving_cert
  instructions: |-
    Run the following command:
    $ oc -n openshift-oauth-apiserver describe secret serving-cert
    Verify that the serving-cert for the openshift-apiserver is type
    kubernetes.io/tls and that returned Data includes tls.crt
    and tls.key.
          Is it the case that The openshift-apiserver serving-cert is not set to type
    <tt>kubernetes.io/tls</tt> and that returned Data does not include <tt>tls.crt</tt>
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-oauth-https-serving-cert
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-api-server-oauth-https-serving-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38396"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-oauth-https-serving-cert
    uid: cc76f8ca-7d1d-4401-a604-c489547c61e5
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure the openshift-oauth-apiserver service uses TLS
    Connections between the kube-apiserver and the extension
    openshift-apiserver could potentially carry sensitive data such
    as secrets and keys. It is important to use in-transit encryption
    for any communication between the kube-apiserver and the extension
    openshift-apiserver.
  id: xccdf_org.ssgproject.content_rule_api_server_openshift_https_serving_cert
  instructions: |-
    Run the following command:
    $ oc -n openshift-apiserver describe secret serving-cert
    Verify that the serving-cert for the openshift-apiserver is type
    kubernetes.io/tls and that returned Data includes tls.crt
    and tls.key.
          Is it the case that The openshift-apiserver serving-cert is not set to type
    <tt>kubernetes.io/tls</tt> and that returned Data does not include <tt>tls.crt</tt>
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-openshift-https-serving-cert
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-api-server-openshift-https-serving-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38446"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-openshift-https-serving-cert
    uid: c6fb26f0-f87b-419e-ad10-430541a8960b
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Profiling is protected by RBAC\nProfiling allows for the identification of specific performance bottlenecks.\nIt generates a significant amount of program data that could potentially be\nexploited to uncover system and program details. \nTo ensure the collected data is not exploited, profiling endpoints are secured\nvia RBAC (see cluster-debugger role). By default, the profiling endpoints are\naccessible only by users bound to cluster-admin or cluster-debugger role.\nProfiling can not be disabled."
  id: xccdf_org.ssgproject.content_rule_api_server_profiling_protected_by_rbac
  instructions: |-
    To verify that the cluster-debugger role is configured correctly,
    run the following command:
    $ oc get clusterroles cluster-debugger -o jsonpath='{.rules[0].nonResourceURLs}'
    and verify that the /metrics path is included there.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-profiling-protected-by-rbac
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-profiling-protected-by-rbac
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38250"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-profiling-protected-by-rbac
    uid: c0e13bd4-8abf-4cc2-a099-822730372ef1
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the API Server Minimum Request Timeout
    Setting global request timout allows extending the API Server request
    timeout limit to a duration appropriate to the user's connection speed.  By
    default, it is set to 1800 seconds which might not be suitable for some
    environments. Setting the limit too low may result in excessive timeouts,
    and a limit that is too large may exhaust the API Server resources making
    it prone to Denial-of-Service attack. It is recommended to set this limit
    as appropriate and change the default limit of 1800 seconds only if needed.
  id: xccdf_org.ssgproject.content_rule_api_server_request_timeout
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["min-request-timeout"]'
    The output should return .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-request-timeout
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-api-server-request-timeout
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38380"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-request-timeout
    uid: 748aa138-f13d-4485-aac4-026a57b5d1f3
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the service-account-lookup argument is set to true
    If service-account-lookup is not enabled, the apiserver
    only verifies that the authentication token is valid, and
    does not validate that the service account token mentioned
    in the request is actually present in etcd. This allows
    using a service account token even after the corresponding
    service account is deleted. This is an example of time of
    check to time of use security issue.
  id: xccdf_org.ssgproject.content_rule_api_server_service_account_lookup
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -o json | \
        jq -r '.data["config.yaml"]' | \
        jq -r '.apiServerArguments["service-account-lookup"]'
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-service-account-lookup
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-api-server-service-account-lookup
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38489"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-service-account-lookup
    uid: 1f113c22-28bb-4837-b02f-988c21d01bb3
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Service Account Public Key for the API Server
    By default if no service-account-key-file is specified
    to the apiserver, it uses the private key from the TLS serving
    certificate to verify service account tokens. To ensure that the
    keys for service account tokens are rotated as needed, a
    separate public/private key pair should be used for signing service
    account tokens.
  id: xccdf_org.ssgproject.content_rule_api_server_service_account_public_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .serviceAccountPublicKeyFiles
    The output should return configured certificate key file(s).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-service-account-public-key
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-service-account-public-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38256"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-service-account-public-key
    uid: 6a3359fc-a2fb-41c2-892a-0fdd4912cc35
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Certificate for the API Server
    API Server communication contains sensitive parameters that should remain
    encrypted in transit. Configure the API Server to serve only HTTPS
    traffic.
  id: xccdf_org.ssgproject.content_rule_api_server_tls_cert
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."tls-cert-file"'
    The output should return /etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-tls-cert
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-api-server-tls-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38419"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-tls-cert
    uid: 6a3945f9-4f64-4514-ac04-1a3c2bfba1ed
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Certificate Key for the API Server
    API Server communication contains sensitive parameters that should remain
    encrypted in transit. Configure the API Server to serve only HTTPS
    traffic.
  id: xccdf_org.ssgproject.content_rule_api_server_tls_private_key
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments."tls-private-key-file"'
    The output should return /etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-tls-private-key
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-api-server-tls-private-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38460"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-tls-private-key
    uid: c7c3e294-8873-4624-b95f-2cb1b1467253
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Token-based Authentication
    The token-based authentication utilizes static tokens to authenticate
    requests to the API Server. The tokens are stored in clear-text in a file
    on the API Server, and cannot be revoked or rotated without restarting the
    API Server.
  id: xccdf_org.ssgproject.content_rule_api_server_token_auth
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments' | grep "token-auth-file"
    The output should be empty as OpenShift does not support token-based authentication at all.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: api-server-token-auth
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-api-server-token-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38259"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-api-server-token-auth
    uid: e45b6f1d-4925-4a84-badd-85f84c97d3e9
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that Audit Log Forwarding Is Enabled
    Retaining logs ensures the ability to go back in time to investigate or correlate any events.
    Offloading audit logs from the cluster ensures that an attacker that has access to the cluster will not be able to
    tamper with the logs because of the logs being stored off-site.
  id: xccdf_org.ssgproject.content_rule_audit_log_forwarding_enabled
  instructions: |-
    Run the following command:
    oc get clusterlogforwarders instance -n openshift-logging -ojson | jq -r '.spec.pipelines[].inputRefs | contains(["audit"])'
    The output should return true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-log-forwarding-enabled
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-audit-log-forwarding-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38361"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-audit-log-forwarding-enabled
    uid: db987569-a9bf-470c-8dfc-a2d88905fe2c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the cluster's audit profile is properly set
    Logging is an important detective control for all systems, to detect potential
    unauthorised access.
  id: xccdf_org.ssgproject.content_rule_audit_profile_set
  instructions: |-
    Run the following command to retrieve the current audit profile:
    $ oc get apiservers cluster -ojsonpath='{.spec.audit.profile}'
    Make sure the profile returned matches the one that should be used.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-profile-set
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-audit-profile-set
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38470"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-audit-profile-set
    uid: 6872f348-8c1b-41f8-a59d-f4b9e2e173be
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Classification Banner on OpenShift Console
    Displays to users organization-defined system use notification message or banner before granting access to the system that provides privacy and security notices consistent with applicable federal laws
  id: xccdf_org.ssgproject.content_rule_classification_banner
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: classification-banner
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-classification-banner
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38493"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-classification-banner
    uid: d452e4a6-be2e-4402-914b-9b1c21b1ffd9
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that Compliance Operator is scanning the cluster
    Vulnerability scanning and risk management are important detective controls
    for all systems, to detect potential flaws and unauthorised access.
  id: xccdf_org.ssgproject.content_rule_compliancesuite_exists
  instructions: |-
    Run the following command to retrieve the compliancesuites in the system:
    $ oc get compliancesuites --all-namespaces
    Make sure the Compliance Operator is installed and there are
    scans running in the system.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: compliancesuite-exists
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-compliancesuite-exists
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38414"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-compliancesuite-exists
    uid: 753bba0c-c7a4-4347-9cf5-2da320f2e8f7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the CNI in use supports Network Policies
    Kubernetes network policies are enforced by the CNI plugin in use. As such
    it is important to ensure that the CNI plugin supports both Ingress and
    Egress network policies.
  id: xccdf_org.ssgproject.content_rule_configure_network_policies
  instructions: |-
    Verify on OpenShift that the NetworkPolicy plugin is being used:
    $ oc explain networkpolicy
    The resulting output should be an explanation of the NetworkPolicy resource.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-network-policies
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-configure-network-policies
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38303"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-configure-network-policies
    uid: 49aadb20-b6c0-4cf4-bc04-386d8abdb787
  severity: high
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that application Namespaces have Network Policies defined.
    Running different applications on the same Kubernetes cluster creates a risk of one
    compromised application attacking a neighboring application. Network segmentation is
    important to ensure that containers can communicate only with those they are supposed
    to. When a network policy is introduced to a given namespace, all traffic not allowed
    by the policy is denied. However, if there are no network policies in a namespace all
    traffic will be allowed into and out of the pods in that namespace.
  id: xccdf_org.ssgproject.content_rule_configure_network_policies_namespaces
  instructions: |-
    Verify on OpenShift namespaces that network policies are in use:
    $ oc get networkpolicy --all-namespaces
    Ensure that each namespace defined in the cluster has at least one NetworkPolicy.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-network-policies-namespaces
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-configure-network-policies-namespaces
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38494"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-configure-network-policies-namespaces
    uid: 84b50a8a-a80d-44f9-845d-0d33d4359b7a
  severity: high
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Controller insecure port argument is unset
    The Controller Manager API service is used for health and metrics
    information and is available without authentication or encryption. As such, it
    should only be bound to a localhost interface to minimize the cluster's
    attack surface.
  id: xccdf_org.ssgproject.content_rule_controller_insecure_port_disabled
  instructions: |-
    To verify that port is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson |   jq -r '.data["config.yaml"]' | jq '.extendedArguments["port"][]'
    Verify that it's disabled (the value is 0).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-insecure-port-disabled
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-controller-insecure-port-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38468"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-controller-insecure-port-disabled
    uid: 71138048-e950-4218-8ec9-576e73354faa
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the RotateKubeletServerCertificate argument is set
    Enabling kubelet certificate rotation causes the kubelet to both request
    a serving certificate after bootstrapping its client credentials and rotate the
    certificate as its existing credentials expire. This automated periodic rotation
    ensures that there are no downtimes due to expired certificates and thus
    addressing the availability in the C/I/A security triad.
  id: xccdf_org.ssgproject.content_rule_controller_rotate_kubelet_server_certs
  instructions: |-
    To verify that RotateKubeletServerCertificate is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["feature-gates"]'
    The output should return RotateKubeletServerCertificate=true.
          Is it the case that <tt>RotateKubeletServerCertificate</tt> argument is set to <tt>false</tt> in the
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-rotate-kubelet-server-certs
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-controller-rotate-kubelet-server-certs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38308"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-controller-rotate-kubelet-server-certs
    uid: a9804677-41b8-43e4-b340-9005faad5288
  severity: medium
  status: PASS
  warnings:
  - This recommendation only applies if you let kubelets get their certificates from the API Server. In case your certificates come from an outside Certificate Authority/tool (e.g. Vault) then you need to take care of rotation yourself
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Controller secure-port argument is set
    The Controller Manager API service is used for health and metrics
    information and is available without authentication or encryption. As such, it
    should only be bound to a localhost interface to minimize the cluster's
    attack surface.
  id: xccdf_org.ssgproject.content_rule_controller_secure_port
  instructions: |-
    To verify that secure-port is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson |   jq -r '.data["config.yaml"]' | jq '.extendedArguments["secure-port"][]'
    Verify that it's using an appropriate port (the value is not 0).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-secure-port
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-controller-secure-port
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38264"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-controller-secure-port
    uid: f3175006-0203-4841-9c6b-5cec10a139b7
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Service Account Certificate Authority Key for the Controller Manager
    Service accounts authenticate to the API using tokens signed by a private RSA
    key. The authentication layer verifies the signature using a matching public RSA key.
    Configuring the certificate authority file ensures that the API server's signing
    certificates are validated.
  id: xccdf_org.ssgproject.content_rule_controller_service_account_ca
  instructions: |-
    To verify that root-ca-file is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["root-ca-file"]'
    The output should return a configured certificate authority file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-service-account-ca
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-controller-service-account-ca
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38305"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-controller-service-account-ca
    uid: 7f78a14c-487b-4000-b2fd-d0092f47059a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Service Account Private Key for the Controller Manager
    By default if no private key file is specified to the
    API Server, the API Server uses the private key from the TLS serving
    certificate to verify service account tokens. To ensure that the keys
    for service account tokens could be rotated as needed, a separate
    public/private key pair should be used for signing service account
    tokens.
  id: xccdf_org.ssgproject.content_rule_controller_service_account_private_key
  instructions: |-
    To verify that service-account-private-key-file is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["service-account-private-key-file"]'
    The output should return a configured private key file.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-service-account-private-key
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-controller-service-account-private-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38255"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-controller-service-account-private-key
    uid: 17c4afcb-f245-43b1-a2f1-44d30f855c9b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that use-service-account-credentials is enabled
    The controller manager creates a service account per controller in
    kube-system namespace, generates an API token and credentials for it,
    then builds a dedicated API client with that service account credential
    for each controller loop to use. Setting the
    use-service-account-credentials to true runs each
    control loop within the contoller manager using a separate service
    account credential. When used in combination with RBAC, this ensures
    that the control loops run with the minimum permissions required to
    perform their intended tasks.
  id: xccdf_org.ssgproject.content_rule_controller_use_service_account
  instructions: |-
    To verify that service-account-credentials is configured correctly,
    run the following command:
    $ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["use-service-account-credentials"]'
    The value of use-service-account-credentials should be true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: controller-use-service-account
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-controller-use-service-account
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38260"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-controller-use-service-account
    uid: fedb3d67-d94d-401f-8b35-b854739fb36c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable etcd Self-Signed Certificates
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection. Using self-signed
    certificates ensures that the certificates are never validated
    against a certificate authority and could lead to compromised
    and invalidated data.
  id: xccdf_org.ssgproject.content_rule_etcd_auto_tls
  instructions: |-
    Run the following command:
    $ oc get cm/etcd-pod -n openshift-etcd -o yaml
    The etcd pod configuration contained in the configmap should not
    contain the --auto-tls=true flag.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-auto-tls
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-etcd-auto-tls
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38369"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-auto-tls
    uid: 5d293977-218a-410f-a468-4d7e48dfad88
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Client Certificate Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_cert_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep -E "\-\-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.crt"
    Verify that there is a certificate configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-cert-file
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-etcd-cert-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38434"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-cert-file
    uid: e2c1eaa5-7ed5-467c-b47e-c003f28d5236
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable The Client Certificate Authentication
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_client_cert_auth
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-client-cert-auth="
    The parameter should be set to true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-client-cert-auth
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-moderate-etcd-client-cert-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38247"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-client-cert-auth
    uid: 07789203-aabf-43be-857c-0f05e57df1d5
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Key File Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_key_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.key"
    Verify that there is a private key configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-key-file
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-etcd-key-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38276"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-key-file
    uid: 1d1cfab5-a61c-4f7a-ad08-1bb9d14e36f6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable etcd Peer Self-Signed Certificates
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection. Using self-signed
    certificates ensures that the certificates are never validated
    against a certificate authority and could lead to compromised
    and invalidated data.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_auto_tls
  instructions: |-
    Run the following command:
    $ oc get cm/etcd-pod -n openshift-etcd -o yaml
    The etcd pod configuration contained in the configmap should not
    contain the --peer-auto-tls=true flag.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-auto-tls
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-etcd-peer-auto-tls
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38282"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-peer-auto-tls
    uid: 700ea458-6079-4407-babe-69e8ac8660b7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Peer Client Certificate Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_cert_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.crt"
    Verify that there is a certificate configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-cert-file
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-etcd-peer-cert-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38463"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-peer-cert-file
    uid: 9ed9cf9c-8b76-4164-a1d7-ebd4d4999bb9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable The Peer Client Certificate Authentication
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_client_cert_auth
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-peer-client-cert-auth="
    The parameter should be set to true.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-client-cert-auth
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-etcd-peer-client-cert-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38390"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-peer-client-cert-auth
    uid: 4e6f02d2-2365-4192-a043-9983cb445c50
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The etcd Peer Key File Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_etcd_peer_key_file
  instructions: |-
    Run the following command:
    oc get -nopenshift-etcd cm etcd-pod -oyaml | grep "\-\-peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.key"
    Verify that there is a private key configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: etcd-peer-key-file
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-etcd-peer-key-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38324"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-etcd-peer-key-file
    uid: 1f505332-e0f0-4b27-9101-afc64330fb7d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns The Worker Proxy Kubeconfig File
    The kubeconfig file for kube-proxy provides permissions to the kube-proxy service.
    The proxy kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.

    The file is provided via a ConfigMap mount, so the kubelet itself makes sure that the
    file permissions are appropriate for the container taking it into use.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_proxy_kubeconfig
  instructions: |-
    Run the following command:

     $ for i in $(oc get pods -n openshift-sdn -l app=sdn -oname)
       do
          oc exec -n openshift-sdn $i -- stat -Lc %U:%G /config/kube-proxy-config.yaml
       done

    The output should be root:root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-proxy-kubeconfig
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-file-groupowner-proxy-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38271"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-file-groupowner-proxy-kubeconfig
    uid: fb43d5cb-ee01-4ad0-ad8f-0babf34ff157
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that File Integrity Operator is scanning the cluster
    File integrity scanning able to detect potential and unauthorised access.
  id: xccdf_org.ssgproject.content_rule_file_integrity_exists
  instructions: |-
    Run the following command to retrieve the fileintegrity objects in the system:
    $ oc get fileintegrities --all-namespaces
    Make sure the File Integrity Operator is installed and there exists
    at least one active FileIntegrity object in the cluster.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-integrity-exists
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-file-integrity-exists
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38487"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-file-integrity-exists
    uid: bc910396-b9c0-4803-8238-f93d55723dc8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify User Who Owns The Worker Proxy Kubeconfig File
    The kubeconfig file for kube-proxy provides permissions to the kube-proxy service.
    The proxy kubeconfig file contains information about the administrative configuration of the
    OpenShift cluster that is configured on the system. Protection of this file is
    critical for OpenShift security.

    The file is provided via a ConfigMap mount, so the kubelet itself makes sure that the
    file permissions are appropriate for the container taking it into use.
  id: xccdf_org.ssgproject.content_rule_file_owner_proxy_kubeconfig
  instructions: |-
    Run the following command:

     $ for i in $(oc get pods -n openshift-sdn -l app=sdn -oname)
       do
          oc exec -n openshift-sdn $i -- stat -Lc %U:%G /config/kube-proxy-config.yaml
       done

    The output should be root:root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-proxy-kubeconfig
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-file-owner-proxy-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38273"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-file-owner-proxy-kubeconfig
    uid: 20368dac-ed31-4873-adc2-4e1eea5df1f4
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on the Worker Proxy Kubeconfig File
    The kube-proxy kubeconfig file controls various parameters of the kube-proxy
    service in the worker node. If used, you should restrict its file permissions
    to maintain the integrity of the file. The file should be writable by only
    the administrators on the system.

    The kube-proxy runs with the kubeconfig parameters configured as
    a Kubernetes ConfigMap instead of a file. In this case, there is no proxy
    kubeconfig file. But appropriate permissions still need to be set in the
    ConfigMap mount.
  id: xccdf_org.ssgproject.content_rule_file_permissions_proxy_kubeconfig
  instructions: |-
    Run the following command:
    $ oc get -nopenshift-sdn ds sdn -ojson | jq -r '.spec.template.spec.volumes[] | select(.configMap.name == "sdn-config") | .configMap.defaultMode'
    The output should return a value of 420.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-proxy-kubeconfig
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-file-permissions-proxy-kubeconfig
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38386"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-file-permissions-proxy-kubeconfig
    uid: 6e4545de-7c05-450c-8c8a-7a1fb27875ff
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the cluster was installed with FIPS mode enabled
    Use of weak or untested encryption algorithms undermines the purposes of utilizing encryption to
    protect data. The system must implement cryptographic modules adhering to the higher
    standards approved by the federal government since this provides assurance they have been tested
    and validated.
  id: xccdf_org.ssgproject.content_rule_fips_mode_enabled
  instructions: |-
    Run the following command to retrieve if the FIPS flag is enabled:
    $ oc get machineconfig 99-master-fips -o jsonpath={.spec.fips}
    Make sure that the result is 'true'.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: fips-mode-enabled
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-fips-mode-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38326"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-fips-mode-enabled
    uid: 7985694d-d9fe-479e-9116-4e485014a748
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Apply Security Context to Your Pods and Containers
    A security context defines the operating system security settings (uid, gid,
    capabilities, SELinux role, etc..) applied to a container. When designing your
    containers and pods, make sure that you configure the security context for your
    pods, containers, and volumes. A security context is a property defined in the
    deployment yaml. It controls the security parameters that will be assigned to
    the pod/container/volume. There are two levels of security context: pod level
    security context, and container level security context.
  id: xccdf_org.ssgproject.content_rule_general_apply_scc
  instructions: |-
    Review the pod definitions in your cluster and verify that you have security
    contexts defined as appropriate.  OpenShift's Security Context Constraint
    feature is on by default in OpenShift 4 and applied to all pods deployed. SCC
    selection is determined by a combination of the values in the securityContext
    and the rolebindings for the account deploying the pod.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-apply-scc
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-general-apply-scc
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38423"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-general-apply-scc
    uid: c170939a-123d-4946-bf4a-e3240f436795
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Manage Image Provenance Using ImagePolicyWebhook
    Image Policy ensures that only approved container images are allowed to be ran on the OpenShift platform.
  id: xccdf_org.ssgproject.content_rule_general_configure_imagepolicywebhook
  instructions: |-
    To ensure that an image policy is configured, review the output
    returned from the following command:
    $ oc get image.config.openshift.io/cluster -o yaml
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-configure-imagepolicywebhook
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-general-configure-imagepolicywebhook
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38465"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-general-configure-imagepolicywebhook
    uid: 29ccec87-0d76-4dcd-af2e-216537c6b359
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    The default namespace should not be used
    Resources in a Kubernetes cluster should be segregated by namespace, to allow
    for security controls to be applied at that level and to make it easier to
    manage resources.
  id: xccdf_org.ssgproject.content_rule_general_default_namespace_use
  instructions: |-
    Run the following command to list objects in the default namespace:
    $ oc get all -n default
    The only entries there should be system-managed resources such as the
    kubernetes and openshift service.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-default-namespace-use
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-general-default-namespace-use
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38346"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-general-default-namespace-use
    uid: acd91013-d97f-4e35-8658-54e0cdc0533c
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Seccomp Profile Pod Definitions
    Seccomp (secure computing mode) is used to restrict the set of system calls
    applications can make, allowing cluster administrators greater control over
    the security of workloads running in the cluster. Kubernetes disables
    seccomp profiles by default for historical reasons. You should enable it to
    ensure that the workloads have restricted actions available within the
    container.
  id: xccdf_org.ssgproject.content_rule_general_default_seccomp_profile
  instructions: |-
    In OpenShift 4, CRI-O is the supported runtime. CRI-O runs unconfined by
    default in order to meet CRI conformance criteria.  On RHEL CoreOS, the
    default seccomp policy is associated with CRI-O and stored in
    /etc/crio/seccomp.json.  The default profile is applied when the user asks
    for the runtime/default profile via annotation to the pod and when the
    associated SCC allows use of the specified seccomp profile.

    Configuration of allowable seccomp profiles is managed through OpenShift
    Security Context Constraints.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-default-seccomp-profile
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-general-default-seccomp-profile
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38437"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-general-default-seccomp-profile
    uid: c145ecad-45ba-472b-9458-23afa919cf17
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Create administrative boundaries between resources using namespaces
    Limiting the scope of user permissions can reduce the impact of mistakes or
    malicious activities. A Kubernetes namespace allows you to partition created
    resources into logically named groups. Resources created in one namespace can
    be hidden from other namespaces. By default, each resource created by a user
    in Kubernetes cluster runs in a default namespace, called default. You can
    create additional namespaces and attach resources and users to them. You can
    use Kubernetes Authorization plugins to create policies that segregate access
    to namespace resources between different users.
  id: xccdf_org.ssgproject.content_rule_general_namespaces_in_use
  instructions: |-
    OpenShift projects wrap Kubernetes namespaces and are used by default in
    OpenShift 4.  Run the following command and review the namespaces created in
    the cluster.  $ oc get namespaces Ensure that the namespaces are
    the ones you need and are adequately administered.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: general-namespaces-in-use
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-general-namespaces-in-use
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38342"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-general-namespaces-in-use
    uid: 0eacd67d-9d0e-4fc5-aba2-62c8fdec83af
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The kubelet Client Certificate Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_tls_cert
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["kubelet-client-certificate"]'
    Verify that a client certificate is configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-tls-cert
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-kubelet-configure-tls-cert
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38363"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-kubelet-configure-tls-cert
    uid: fa6145ab-2126-4dc6-926d-cef4a0e33990
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure That The kubelet Server Key Is Correctly Set
    Without cryptographic integrity protections, information can be
    altered by unauthorized users without detection.
  id: xccdf_org.ssgproject.content_rule_kubelet_configure_tls_key
  instructions: |-
    Run the following command on the kubelet node(s):
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["kubelet-client-key"]'
    Verify that a client certificate is configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-configure-tls-key
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-kubelet-configure-tls-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38373"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-kubelet-configure-tls-key
    uid: 12816ef9-4a56-47b0-ad10-f1622577ad09
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    kubelet - Disable the Read-Only Port
    OpenShift disables the read-only port (10255) on all nodes by setting the
    read-only port kubelet flag to 0. This ensures only
    authenticated connections are able to receive information about the OpenShift
    system.
  id: xccdf_org.ssgproject.content_rule_kubelet_disable_readonly_port
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["kubelet-read-only-port"]'
    The output should be 0.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kubelet-disable-readonly-port
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-kubelet-disable-readonly-port
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38367"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-kubelet-disable-readonly-port
    uid: ab70caef-9381-4fa0-a31c-3a222a1ee1ba
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure OAuth tokens to expire after a set period of inactivity
    Terminating an idle session within a short time period reduces the window
    of opportunity for unauthorized personnel to take control of a session
    that has been left unattended.
  id: xccdf_org.ssgproject.content_rule_oauth_or_oauthclient_inactivity_timeout
  instructions: |-
    To check if the OAuth server timeout is configured, run the following command:
    oc get oauth cluster -ojsonpath='{.spec.tokenConfig.accessTokenInactivityTimeout}'
    the output should return a timeout value.

    To check if the OAuth client timeout is configured, run the following command:
    oc get oauthclients -ojson | jq -r '.items[] | { accessTokenInactivityTimeoutSeconds: .accessTokenInactivityTimeoutSeconds}'
    the output should return a timeout value per client.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: oauth-or-oauthclient-inactivity-timeout
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-oauth-or-oauthclient-inactivity-timeout
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38266"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-oauth-or-oauthclient-inactivity-timeout
    uid: 35da870f-07f5-4588-bfe5-b44b468f33b0
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Allowed registries are configured
    Allowed registries should be configured to restrict the registries that the
    OpenShift container runtime can access, and all other registries should be
    blocked.
  id: xccdf_org.ssgproject.content_rule_ocp_allowed_registries
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-allowed-registries
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-ocp-allowed-registries
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38427"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-ocp-allowed-registries
    uid: 35372190-dd7e-41e1-94fa-ad276c1a0ac2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Allowed registries for import are configured\nAllowed registries for import should be specified to limit the registries \nfrom which users may import images."
  id: xccdf_org.ssgproject.content_rule_ocp_allowed_registries_for_import
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-allowed-registries-for-import
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-ocp-allowed-registries-for-import
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38248"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-ocp-allowed-registries-for-import
    uid: 39ffdbc6-1cda-4d3a-a7c5-4a29b661e7da
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the OpenShift API Server Maximum Retained Audit Logs
    OpenShift automatically rotates the log files. Retaining old log files ensures
    OpenShift Operators will have sufficient log data available for carrying out
    any investigation or correlation. For example, if the audit log size is set to
    100 MB and the number of retained log files is set to 10, OpenShift Operators
    would have approximately 1 GB of log data to use during analysis.
  id: xccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxbackup
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxbackup"][0]'
    The output should return a value of 10 or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-api-server-audit-log-maxbackup
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-ocp-api-server-audit-log-maxbackup
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38398"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-ocp-api-server-audit-log-maxbackup
    uid: 05eae0fd-7a5e-4a67-a029-9e3cc06110f4
  severity: low
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure OpenShift API Server Maximum Audit Log Size
    OpenShift automatically rotates log files. Retaining old log files ensures that
    OpenShift Operators have sufficient log data available for carrying out any
    investigation or correlation. If you have set file size of 100 MB and the number of
    old log files to keep as 10, there would be approximately 1 GB of log data
    available for use in analysis.
  id: xccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxsize
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-maxsize"]'
    The output should return a value of ["100"] or as appropriate.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-api-server-audit-log-maxsize
    creationTimestamp: "2021-07-13T16:56:44Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:44Z"
    name: ocp4-moderate-ocp-api-server-audit-log-maxsize
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38449"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-ocp-api-server-audit-log-maxsize
    uid: 36146ee9-696e-4351-be4c-e40595bf1b73
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Do Not Use htpasswd-based IdP\n\n              \nWith any authentication mechanism the ability to revoke credentials if they\nare compromised or no longer required, is a key control. Kubernetes client\ncertificate authentication does not allow for this due to a lack of support\nfor certificate revocation.\n\n              \nOpenShift's built-in OAuth server allows credential revocation by relying on\nthe Identity provider, as well as giving the administrators the ability to\nrevoke any tokens given to a specific user.\n\n              \nIn addition, using an external Identity provider allows for setting up notifications\non account creation or deletion, multi-factor authentication, disabling inactive\naccounts or other features required by different compliance standards.\n\n            "
  id: xccdf_org.ssgproject.content_rule_ocp_idp_no_htpasswd
  instructions: "Run the following command to list the identity providers configured:\n$ oc get oauths cluster -ojsonpath='{.spec.identityProviders}' | jq \nMake sure that there exists at least one item referenced by the above path\nand that the value is not HTPasswd"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-idp-no-htpasswd
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-ocp-idp-no-htpasswd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38254"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-ocp-idp-no-htpasswd
    uid: 8264204a-bd7c-4eb7-93dc-1e59c6caffd6
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Only Use LDAP-based IdPs with TLS
    Transmitting authentication tokens as clear-text may leak them to
    an attacker.
  id: xccdf_org.ssgproject.content_rule_ocp_no_ldap_insecure
  instructions: "Run the following command to list the identity providers configured:\n$ oc get oauths cluster -ojsonpath='{.spec.identityProviders}' | jq \nIf any of the IDP providers' type is LDAP, make sure the insecure\nflag is not set or is set to false."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ocp-no-ldap-insecure
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-ocp-no-ldap-insecure
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38486"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-ocp-no-ldap-insecure
    uid: 826db830-0bea-49f2-a9ec-f9e771a7ef41
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure the Audit Log Path
    Auditing of the API Server is not enabled by default. Auditing the API Server
    provides a security-relevant chronological set of records documenting the sequence
    of activities that have affected the system by users, administrators, or other
    system components.
  id: xccdf_org.ssgproject.content_rule_openshift_api_server_audit_log_path
  instructions: |-
    Run the following command:
    $ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["audit-log-path"]'
    The output should return a valid audit log path.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: openshift-api-server-audit-log-path
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-openshift-api-server-audit-log-path
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38440"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-openshift-api-server-audit-log-path
    uid: 3f5a8517-8dfb-4082-a2e0-5ef028dac6c9
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Profiling is protected by RBAC
    Profiling allows for the identification of specific performance bottlenecks.
    It generates a significant amount of program data that could potentially be
    exploited to uncover system and program details. If you are not experiencing
    any bottlenecks and do not need the profiler for troubleshooting purposes, it
    is recommended to turn it off to reduce the potential attack surface. To
    ensure the collected data is not exploited, profiling endpoints are secured
    via RBAC (see cluster-debugger role). By default, the profiling endpoints are
    accessible only by users bound to cluster-admin or cluster-debugger role.
    Profiling can not be disabled.
  id: xccdf_org.ssgproject.content_rule_rbac_debug_role_protects_pprof
  instructions: |-
    To verify that the cluster-debugger role is configured correctly,
    run the following command:
    $ oc get clusterroles cluster-debugger -o jsonpath='{.rules[0].nonResourceURLs}'
    and verify that the /debug/pprof path is included there.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-debug-role-protects-pprof
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-rbac-debug-role-protects-pprof
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38425"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-rbac-debug-role-protects-pprof
    uid: 4d0c4a3c-84a6-4f1e-a5f1-4fda303bd7fd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the cluster-admin role is only used where required
    Kubernetes provides a set of default roles where RBAC is used. Some of these
    roles such as cluster-admin provide wide-ranging privileges which should
    only be applied where absolutely necessary. Roles such as cluster-admin
    allow super-user access to perform any action on any resource. When used in
    a ClusterRoleBinding, it gives full control over every resource in the
    cluster and in all namespaces. When used in a RoleBinding, it gives full
    control over every resource in the rolebinding's namespace, including the
    namespace itself.
  id: xccdf_org.ssgproject.content_rule_rbac_limit_cluster_admin
  instructions: |-
    Review users and groups bound to cluster-admin and decide whether they
    require such access. Consider creating least-privilege roles for users and
    service accounts. Obtain a list of the users who have access to the
    cluster-admin role by reviewing the clusterrolebinding output for each role
    binding that has access to the cluster-admin role. To do this, run the
    following command:
    $ oc get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].kind | grep cluster-admin

    Care should be taken before removing any clusterrolebindings from the
    environment to ensure they are not required for operation of the cluster.
    Specifically, modifications should not be made to the default
    clusterrolebindings including those with the "system:" prefix.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-limit-cluster-admin
    creationTimestamp: "2021-07-13T16:56:41Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:41Z"
    name: ocp4-moderate-rbac-limit-cluster-admin
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38393"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-rbac-limit-cluster-admin
    uid: 3b575800-14fe-4dbf-b1ce-e2f1dccadc34
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to Kubernetes Secrets
    Inappropriate access to secrets stored within the Kubernetes
    cluster can allow for an attacker to gain additional access to
    the Kubernetes cluster or external resources whose credentials
    are stored as secrets.
  id: xccdf_org.ssgproject.content_rule_rbac_limit_secrets_access
  instructions: |-
    To review the policy rules assigned to roles in all namespaces, run
    the following command:
    $ for ns in $(oc get projects -ojsonpath='{.items[*].metadata.name}'); do oc describe roles -n$ns; done
    To review the policy rules assigned to cluster roles, run the following
    command:
    $ for i in $(oc get clusterroles -o jsonpath='{.items[*].metadata.name}'); do oc describe clusterrole ${i}; done
    Review the output and ensure that only authorized roles have access to the
    secrets resource or all resources using a wildcard.
    To filter clusterroles that have assigned access to the secrets resources for clustrroles, run:
    $ oc get clusterroles -ojson | jq -r '.items[] | {name: .metadata.name, rules: .rules} | select(.rules[]?.resources | try contains(["secrets"])) | .name' | sort | uniq
    and similarly to filter namespace/role pairs:
    $ for ns in $(oc get projects -ojsonpath='{.items[*].metadata.name}'); do oc get roles -n$ns -ojson | jq -r '.items[] | {name: .metadata.name, namespace: .metadata.namespace, rules: .rules} | select(.rules[]?.resources | try contains(["secrets"])) | "\(.namespace)/\(.name)"' | sort | uniq; done
    note that the two commands above do not show roles and/or clusterroles with wildcard access to any resources.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-limit-secrets-access
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-rbac-limit-secrets-access
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38310"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-rbac-limit-secrets-access
    uid: 0a8f7de9-8b0c-4a53-9254-dce42aba1165
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Minimize Access to Pod Creation
    The ability to create pods in a cluster opens up the cluster
    for privilege escalation.
  id: xccdf_org.ssgproject.content_rule_rbac_pod_creation_access
  instructions: |-
    To review the pod creation privileges in roles, run the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    Review the output, and for any role/clusterrole defining create permissions
    for pods that are NOT an OpenShift "system:" or other system-provided
    role/clusterrole, determine if the users bound to the role truly have the
    need to create pods.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-pod-creation-access
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-rbac-pod-creation-access
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38301"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-rbac-pod-creation-access
    uid: c67653d2-d079-4b86-b683-90f9ce9560c0
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Minimize Wildcard Usage in Cluster and Local Roles
    The principle of least privilege recommends that users are
    provided only the access required for their role and nothing
    more. The use of wildcard rights grants is likely to provide
    excessive rights to the Kubernetes API.
  id: xccdf_org.ssgproject.content_rule_rbac_wildcard_use
  instructions: |-
    To review the wildcard usage in roles, run the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    Review the output, and for any role/clusterrole specifying a wildcard
    resource that is NOT an OpenShift "system:" or other system-provided
    role/clusterrole, determine if the wildcard access can be replaced with
    specific resources.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: rbac-wildcard-use
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-rbac-wildcard-use
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38472"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-rbac-wildcard-use
    uid: dc2d66a9-50ee-4142-ad69-cc904fe0ed7c
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that all OpenShift Routes prefer TLS
    Using clear-text in communications coming to or from outside
    the cluster's network may leak sensitive information.
  id: xccdf_org.ssgproject.content_rule_routes_protected_by_tls
  instructions: |-
    Run the following command to retrieve the compliancesuites in the system:
    $ oc get routes --all-namespaces
    Make sure that every route object has either Disable or Redirect
    in the .spec.tls.insecureEdgeTerminationPolicy setting.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: routes-protected-by-tls
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-routes-protected-by-tls
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38442"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-routes-protected-by-tls
    uid: 23640888-9f4d-47e8-b493-6dca45059232
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Drop Container Capabilities
    By default, containers run with a default set of capabilities as assigned
    by the Container Runtime which can include dangerous or highly privileged
    capabilities. Capabilities should be dropped unless absolutely critical for
    the container to run software as added capabilities that are not required
    allow for malicious containers or attackers.
  id: xccdf_org.ssgproject.content_rule_scc_drop_container_capabilities
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that do not list any requiredDropCapabilities, examine the
    associated rolebindings to account for the users that are bound to the role.
    Review each SCC and determine that all capabilities are either
    completely disabled as a list entry under requiredDropCapabilities,
    or that all the un-required capabilities are dropped for containers and SCCs.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-drop-container-capabilities
    creationTimestamp: "2021-07-13T16:56:37Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:37Z"
    name: ocp4-moderate-scc-drop-container-capabilities
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38246"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-drop-container-capabilities
    uid: a4683d25-a2f5-4b4b-a1d1-2b1615526f9b
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Container Capabilities
    By default, containers run with a default set of capabilities as assigned
    by the Container Runtime which can include dangerous or highly privileged
    capabilities. Capabilities should be dropped unless absolutely critical for
    the container to run software as added capabilities that are not required
    allow for malicious containers or attackers.
  id: xccdf_org.ssgproject.content_rule_scc_limit_container_allowed_capabilities
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that do not list an explicit allowedCapabilities, examine the
    associated rolebindings to account for the users that are bound to the role.
    Review each SCC and determine that only required capabilities are either
    completely added as a list entry under allowedCapabilities,
    or that all the un-required capabilities are dropped for containers and SCCs.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-container-allowed-capabilities
    creationTimestamp: "2021-07-13T16:56:43Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:43Z"
    name: ocp4-moderate-scc-limit-container-allowed-capabilities
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38432"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-container-allowed-capabilities
    uid: 6e0adf86-de12-409d-9b96-65af9856fb49
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to the Host IPC Namespace
    A container running in the host's IPC namespace can use IPC
    to interact with processes outside the container potentially
    allowing an attacker to exploit a host process thereby enabling an
    attacker to exploit other services.
  id: xccdf_org.ssgproject.content_rule_scc_limit_ipc_namespace
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowHostIPC set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowHostIPC, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowHostIPC is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-ipc-namespace
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-scc-limit-ipc-namespace
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38409"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-ipc-namespace
    uid: f6ee8ba2-6cde-4f32-9621-819a0c675710
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Use of the CAP_NET_RAW
    By default, containers run with a default set of capabilities as assigned
    by the Container Runtime which can include dangerous or highly privileged
    capabilities. If the CAP_NET_RAW is enabled, it may be misused
    by malicious containers or attackers.
  id: xccdf_org.ssgproject.content_rule_scc_limit_net_raw_capability
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that do not have NET_RAW or ALL set under requiredDropCapabilities.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that do not drop NET_RAW or ALL, examine the
    associated rolebindings to account for the users that are bound to the role.
    Review each SCC and determine that either NET_RAW or ALL
    is either included as a list entry under requiredDropCapabilities,
    or that either NET_RAW or ALL is only enabled to a small
    set of containers and SCCs.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-net-raw-capability
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-scc-limit-net-raw-capability
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38490"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-net-raw-capability
    uid: a16213cd-4c3e-4733-83b8-8d7328342811
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to the Host Network Namespace
    A container running in the host's network namespace could
    access the host network traffic to and from other pods
    potentially allowing an attacker to exploit pods and network
    traffic.
  id: xccdf_org.ssgproject.content_rule_scc_limit_network_namespace
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowHostNetwork set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowHostNetwork, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowHostNetwork is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-network-namespace
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-scc-limit-network-namespace
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38485"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-network-namespace
    uid: e6862577-e1f9-48c3-8f46-a6cd11214968
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Containers Ability to Escalate Privileges
    Privileged containers have access to more of the Linux Kernel
    capabilities and devices. If a privileged container were
    compromised, an attacker would have full access to the container
    and host.
  id: xccdf_org.ssgproject.content_rule_scc_limit_privilege_escalation
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowPrivilegeEscalation set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowPrivilegeEscalation, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowPrivilegeEscalation is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-privilege-escalation
    creationTimestamp: "2021-07-13T16:56:39Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:39Z"
    name: ocp4-moderate-scc-limit-privilege-escalation
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38320"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-privilege-escalation
    uid: a8a1b0f2-f18a-4a0a-b23f-ca9d0c275427
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Privileged Container Use
    Privileged containers have access to all Linux Kernel
    capabilities and devices. If a privileged container were
    compromised, an attacker would have full access to the container
    and host.
  id: xccdf_org.ssgproject.content_rule_scc_limit_privileged_containers
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowPrivilegedContainer set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowPrivilegedContainer, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowPrivilegedContainer is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-privileged-containers
    creationTimestamp: "2021-07-13T16:56:42Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:42Z"
    name: ocp4-moderate-scc-limit-privileged-containers
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38400"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-privileged-containers
    uid: 9b36647c-212f-4257-a99d-4a3f4fb2ccb1
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Access to the Host Process ID Namespace
    A container running in the host's PID namespace can inspect
    processes running outside the container which can be used to
    escalate privileges outside of the container.
  id: xccdf_org.ssgproject.content_rule_scc_limit_process_id_namespace
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowHostPID set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowHostPID, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowHostPID is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-process-id-namespace
    creationTimestamp: "2021-07-13T16:56:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:46Z"
    name: ocp4-moderate-scc-limit-process-id-namespace
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38498"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-process-id-namespace
    uid: 3c80f4a3-c95a-4cce-bc12-42ac4ca5114b
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Container Running As Root User
    Privileged containers have access to all Linux Kernel
    capabilities and devices. If a privileged container were
    compromised, an attacker would have full access to the container
    and host.
  id: xccdf_org.ssgproject.content_rule_scc_limit_root_containers
  instructions: |-
    Inspect each SCC returned from running the following command:
    $ oc get scc
    Review each SCC for those that have allowPrivilegedContainer set to true.
    Next, examine the outputs of the following commands:
    $ oc describe roles --all-namespaces
    $ oc describe clusterroles
    For any role/clusterrole that reference the
    securitycontextconstraints resource with the resourceNames
    of the SCCs that have allowPrivilegedContainer, examine the associated
    rolebindings to account for the users that are bound to the role. Review the
    account to determine if allowPrivilegedContainer is truly required.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scc-limit-root-containers
    creationTimestamp: "2021-07-13T16:56:38Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:38Z"
    name: ocp4-moderate-scc-limit-root-containers
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38252"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scc-limit-root-containers
    uid: 15d26843-6223-4a3e-938f-78fa02e1ab49
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that the bind-address parameter is not used
    In OpenShift 4, The Kubernetes Scheduler operator manages and updates the
    Kubernetes Scheduler deployed on top of OpenShift. By default, the operator
    exposes metrics via metrics service. The metrics are collected from the
    Kubernetes Scheduler operator. Profiling data is sent to healthzPort,
    the port of the localhost healthz endpoint. Changing this value may disrupt
    components that monitor the kubelet health.
  id: xccdf_org.ssgproject.content_rule_scheduler_no_bind_address
  instructions: |-
    Run the following command:
    oc get -nopenshift-kube-scheduler cm kube-scheduler-pod -ojson | jq -r '.data["pod.yaml"]' | jq -r | grep bind-address
    The output should be empty
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: scheduler-no-bind-address
    creationTimestamp: "2021-07-13T16:56:45Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:45Z"
    name: ocp4-moderate-scheduler-no-bind-address
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38488"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-scheduler-no-bind-address
    uid: cb834533-7309-4fa2-a0d3-b8876c95cdff
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Consider external secret storage
    Kubernetes supports secrets as first-class objects, but care needs to be
    taken to ensure that access to secrets is carefully limited. Using an
    external secrets provider can ease the management of access to secrets,
    especially where secrets are used across both Kubernetes and non-Kubernetes
    environments.
  id: xccdf_org.ssgproject.content_rule_secrets_consider_external_storage
  instructions: |-
    Review the cluster configuration and determine if an appropriate secrets
    manager has been configured.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: secrets-consider-external-storage
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-secrets-consider-external-storage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38351"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-secrets-consider-external-storage
    uid: 8d9fbbd2-ab3f-4d7e-a893-07086b9d564c
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Do Not Use Environment Variables with Secrets
    Environment variables are subject and very susceptible to
    malicious hijacking methods by an adversary, as such,
    environment variables should never be used for secrets.
  id: xccdf_org.ssgproject.content_rule_secrets_no_environment_variables
  instructions: |-
    To find workloads that use environment variables for secrets, run the following:
    $ oc get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.namespace} {.metadata.name} {"\n"}{end}' -A
    Review the output and ensure that workloads that can mount secrets as data
    volumes use that instead of environment variables.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: secrets-no-environment-variables
    creationTimestamp: "2021-07-13T16:56:40Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: ocp4-moderate
      compliance.openshift.io/suite: nist-moderate-and-cis-platform
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6a401606-7011-47e6-97f8-3f9a6d19652b"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:56:40Z"
    name: ocp4-moderate-secrets-no-environment-variables
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: ocp4-moderate
      uid: 6a401606-7011-47e6-97f8-3f9a6d19652b
    resourceVersion: "38332"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/ocp4-moderate-secrets-no-environment-variables
    uid: e937e3e0-4c93-4545-9ddf-bf15579ef8ba
  severity: medium
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Only Root Has UID 0
    An account has root authority if it has a UID of 0. Multiple accounts
    with a UID of 0 afford more opportunity for potential intruders to
    guess a password for a privileged account. Proper configuration of
    sudo is recommended to afford multiple system administrators
    access to root privileges in an accountable manner.
  id: xccdf_org.ssgproject.content_rule_accounts_no_uid_except_zero
  instructions: |-
    To list all password file entries for accounts with UID 0, run the
    following command:
    $ awk -F: '($3 == \"0\") {print}' /etc/passwd
    This should print only one line, for the user root.

    If there is a finding, change the UID of the failing (non-root) user. If
    the account is associated with the system commands or applications the UID
    should be changed to one greater than 0 but less than
    1000. Otherwise assign a UID of greater than 1000 that
    has not already been assigned.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: accounts-no-uid-except-zero
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-accounts-no-uid-except-zero
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40815"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-accounts-no-uid-except-zero
    uid: c89fdf92-ad3e-4fa9-aea0-59436285d5c8
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - chmod
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_chmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    chmod system call, run the following command:
    preserve$ sudo grep "chmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-chmod
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-chmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41101"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-chmod
    uid: daea8bbe-6ca0-4a1b-bf77-d5c3de87c786
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - chown
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_chown
  instructions: |-
    To determine if the system is configured to audit calls to the
    chown system call, run the following command:
    preserve$ sudo grep "chown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-chown
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-chown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40070"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-chown
    uid: 899c81a8-bb96-4363-9346-73c9ac3f896c
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchmod
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmod system call, run the following command:
    preserve$ sudo grep "fchmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchmod
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-fchmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40843"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-fchmod
    uid: e2158163-ddfa-4297-80cd-6c29387f2bfc
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchmodat
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchmodat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmodat system call, run the following command:
    preserve$ sudo grep "fchmodat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchmodat
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-fchmodat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40543"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-fchmodat
    uid: 9a816cc6-c494-4ff5-9711-9c13ddff505d
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchown
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchown system call, run the following command:
    preserve$ sudo grep "fchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchown
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-fchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40728"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-fchown
    uid: 28505f3a-e2e6-4752-92ad-9af712259676
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchownat
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchownat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchownat system call, run the following command:
    preserve$ sudo grep "fchownat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchownat
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-fchownat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40054"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-fchownat
    uid: 9faf7aa2-5119-4a30-a279-c7befcd189d7
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fremovexattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fremovexattr system call, run the following command:
    preserve$ sudo grep "fremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fremovexattr
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-fremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40442"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-fremovexattr
    uid: bdf70f17-bf2f-4902-acf2-c7de4284955e
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fsetxattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fsetxattr system call, run the following command:
    preserve$ sudo grep "fsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fsetxattr
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-fsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40242"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-fsetxattr
    uid: 80ed3b22-0888-4076-b402-52cf4aa0b6ca
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - lchown
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_lchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    lchown system call, run the following command:
    preserve$ sudo grep "lchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-lchown
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-lchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40499"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-lchown
    uid: 0277e5e0-6103-4eb7-9926-edae477206fb
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - lremovexattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_lremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lremovexattr system call, run the following command:
    preserve$ sudo grep "lremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-lremovexattr
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-lremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40028"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-lremovexattr
    uid: d47e46f4-1a4d-4ca5-9429-bb14cf4b2e67
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - lsetxattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_lsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lsetxattr system call, run the following command:
    preserve$ sudo grep "lsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-lsetxattr
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-lsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40081"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-lsetxattr
    uid: 85c77057-6b9c-4568-ab04-3a974bdeae12
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - removexattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_removexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    removexattr system call, run the following command:
    preserve$ sudo grep "removexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-removexattr
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-removexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40820"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-removexattr
    uid: 2af4e6d6-ffca-4840-975a-194e17fdbc07
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - setxattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_setxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    setxattr system call, run the following command:
    preserve$ sudo grep "setxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-setxattr
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-audit-rules-dac-modification-setxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40914"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-dac-modification-setxattr
    uid: 6556419e-3a0c-411c-aa03-74baef08ae4b
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/group
    Creation of groups through direct edition of /etc/group could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_group_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-group-open
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-audit-rules-etc-group-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40078"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-group-open
    uid: 70f5ac70-35f1-438d-8913-7bdc5aa7213f
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&03 -F path=/etc/group -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/group
    Creation of groups through direct edition of /etc/group could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_group_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-group-open-by-handle-at
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-etc-group-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40405"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-group-open-by-handle-at
    uid: 474e56ed-0655-401f-b7b7-b493b49172a7
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/group -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/group
    Creation of groups through direct edition of /etc/group could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_group_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-group-openat
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-audit-rules-etc-group-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40216"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-group-openat
    uid: a489aa71-00c5-4a80-819b-886b169f97df
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/group -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/gshadow
    Creation of users through direct edition of /etc/gshadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_gshadow_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-gshadow-open
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-audit-rules-etc-gshadow-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40114"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-gshadow-open
    uid: 6a8220ad-e5b2-4f50-bc26-5c5692be6721
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&03 -F path=/etc/gshadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/gshadow
    Creation of users through direct edition of /etc/gshadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_gshadow_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-gshadow-open-by-handle-at
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-audit-rules-etc-gshadow-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40304"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-gshadow-open-by-handle-at
    uid: c8f2f17b-bbfd-43c5-8df1-c97891cae07c
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/gshadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/gshadow
    Creation of users through direct edition of /etc/gshadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_gshadow_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-gshadow-openat
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-audit-rules-etc-gshadow-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40974"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-gshadow-openat
    uid: 85ac8ed4-b870-4a9c-a12a-d431e88dcb5e
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/gshadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/passwd
    Creation of users through direct edition of /etc/passwd could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_passwd_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-passwd-open
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-etc-passwd-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40412"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-passwd-open
    uid: 35cddc43-e052-4492-a726-7d76ae203e02
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&03 -F path=/etc/passwd -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/passwd
    Creation of users through direct edition of /etc/passwd could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_passwd_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-passwd-open-by-handle-at
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-etc-passwd-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40437"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-passwd-open-by-handle-at
    uid: 954b355b-6cb5-4bd9-92e4-db60bb5b9a95
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/passwd -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/passwd
    Creation of users through direct edition of /etc/passwd could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_passwd_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-passwd-openat
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-audit-rules-etc-passwd-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40715"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-passwd-openat
    uid: 4bdf8085-9335-49fe-bfa9-68c7b506437b
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/passwd -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/shadow
    Creation of users through direct edition of /etc/shadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_shadow_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-shadow-open
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-audit-rules-etc-shadow-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40325"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-shadow-open
    uid: 5a4a087c-c19a-40d0-95ff-cc8fd289f8cf
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open,openat,open_by_handle_at -F a1&03 -F path=/etc/shadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/shadow
    Creation of users through direct edition of /etc/shadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_shadow_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-shadow-open-by-handle-at
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-master-audit-rules-etc-shadow-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41116"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-shadow-open-by-handle-at
    uid: 1779811b-a9fe-414a-a4e5-a4ee7f00d976
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open,openat,open_by_handle_at -F a2&03 -F path=/etc/shadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/shadow
    Creation of users through direct edition of /etc/shadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_shadow_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-shadow-openat
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-audit-rules-etc-shadow-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40721"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-etc-shadow-openat
    uid: 36488761-c976-4e9f-9435-3a43ea08fc6f
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open,openat,open_by_handle_at -F a2&03 -F path=/etc/shadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run chcon
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_chcon
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/bin/chcon" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:

    -a always,exit -F path=/usr/bin/chcon -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-chcon
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-audit-rules-execution-chcon
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40343"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-execution-chcon
    uid: aad9e884-7036-4268-aa3d-00b4a67fc8fd
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run restorecon
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_restorecon
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/restorecon" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/restorecon -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-restorecon
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-audit-rules-execution-restorecon
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40142"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-execution-restorecon
    uid: de31d1d5-a734-4600-90c4-1224caca1e26
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run semanage
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_semanage
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/semanage" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/semanage -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-semanage
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-audit-rules-execution-semanage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40994"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-execution-semanage
    uid: d17b3d5a-90fc-4624-9d24-2decdf2d99e5
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run setfiles
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_setfiles
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/setfiles" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/setfiles -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-setfiles
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-audit-rules-execution-setfiles
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40603"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-execution-setfiles
    uid: 459cda67-c5f0-453e-9b8f-6a994da8f4a3
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run setsebool
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_setsebool
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/setsebool" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/setsebool -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-setsebool
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-audit-rules-execution-setsebool
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41036"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-execution-setsebool
    uid: 81e867b1-9d6d-4e2e-83b9-8f6f529bc031
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run seunshare
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_seunshare
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/seunshare" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/seunshare -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-seunshare
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-audit-rules-execution-seunshare
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40136"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-execution-seunshare
    uid: 54bb2578-5bdb-421b-a960-fa16b6980f0b
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - rename
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_rename
  instructions: |-
    To determine if the system is configured to audit calls to the
    rename system call, run the following command:
    preserve$ sudo grep "rename" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-rename
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-audit-rules-file-deletion-events-rename
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40272"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-file-deletion-events-rename
    uid: d241027b-fd61-471c-9b15-f5ce464380f2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - renameat
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_renameat
  instructions: |-
    To determine if the system is configured to audit calls to the
    renameat system call, run the following command:
    preserve$ sudo grep "renameat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-renameat
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-audit-rules-file-deletion-events-renameat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40151"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-file-deletion-events-renameat
    uid: f0242609-59fe-49ce-909f-02d341d06ae3
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - rmdir
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_rmdir
  instructions: |-
    To determine if the system is configured to audit calls to the
    rmdir system call, run the following command:
    preserve$ sudo grep "rmdir" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-rmdir
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-audit-rules-file-deletion-events-rmdir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40894"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-file-deletion-events-rmdir
    uid: 1f510c9b-58d8-4e16-860a-c4697cdd5d38
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - unlink
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_unlink
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlink system call, run the following command:
    preserve$ sudo grep "unlink" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-unlink
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-audit-rules-file-deletion-events-unlink
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40685"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-file-deletion-events-unlink
    uid: 1c6680d1-1645-4cc8-ad1d-09fc07ee643a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - unlinkat
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_unlinkat
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlinkat system call, run the following command:
    preserve$ sudo grep "unlinkat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-unlinkat
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-audit-rules-file-deletion-events-unlinkat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40889"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-file-deletion-events-unlinkat
    uid: 20d0de00-27f3-46fc-a1f3-c854094f0580
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Make the auditd Configuration Immutable
    Making the audit configuration immutable prevents accidental as
    well as malicious modification of the audit rules, although it may be
    problematic if legitimate changes are needed during system
    operation
  id: xccdf_org.ssgproject.content_rule_audit_rules_immutable
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-immutable
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-immutable
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40566"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-immutable
    uid: b26d215c-8f9a-4df2-a955-24f453f36986
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Kernel Module Unloading - delete_module
    The removal of kernel modules can be used to alter the behavior of
    the kernel and potentially introduce malicious code into kernel space. It is important
    to have an audit trail of modules that have been introduced into the kernel.
  id: xccdf_org.ssgproject.content_rule_audit_rules_kernel_module_loading_delete
  instructions: |-
    To determine if the system is configured to audit calls to the
    delete_module system call, run the following command:
    preserve$ sudo grep "delete_module" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-kernel-module-loading-delete
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-kernel-module-loading-delete
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39998"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-kernel-module-loading-delete
    uid: 15d7609a-38cd-48a3-bac4-54d758984e0d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Kernel Module Loading and Unloading - finit_module
    The addition/removal of kernel modules can be used to alter the behavior of
    the kernel and potentially introduce malicious code into kernel space. It is important
    to have an audit trail of modules that have been introduced into the kernel.
  id: xccdf_org.ssgproject.content_rule_audit_rules_kernel_module_loading_finit
  instructions: |-
    To determine if the system is configured to audit calls to the
    finit_module system call, run the following command:
    preserve$ sudo grep "finit_module" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-kernel-module-loading-finit
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-audit-rules-kernel-module-loading-finit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40848"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-kernel-module-loading-finit
    uid: 81087362-8271-4dd8-92a9-9a5913041f71
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Kernel Module Loading - init_module
    The addition of kernel modules can be used to alter the behavior of
    the kernel and potentially introduce malicious code into kernel space. It is important
    to have an audit trail of modules that have been introduced into the kernel.
  id: xccdf_org.ssgproject.content_rule_audit_rules_kernel_module_loading_init
  instructions: |-
    To determine if the system is configured to audit calls to the
    init_module system call, run the following command:
    preserve$ sudo grep "init_module" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-kernel-module-loading-init
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-audit-rules-kernel-module-loading-init
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40263"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-kernel-module-loading-init
    uid: 9e70faba-7bac-4392-adf2-0cedb7c0bb69
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Logon and Logout Events - faillock
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_login_events_faillock
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/var/run/faillock\|-w /var/run/faillock"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-login-events-faillock
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-login-events-faillock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39963"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-login-events-faillock
    uid: e57ffd0f-b2ee-4b62-8285-ae1f44d6a7a3
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Logon and Logout Events - lastlog
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_login_events_lastlog
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/var/log/lastlog\|-w /var/log/lastlog"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-login-events-lastlog
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-audit-rules-login-events-lastlog
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40592"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-login-events-lastlog
    uid: ca59becf-6871-48d1-bdc3-a9969006ad6d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Logon and Logout Events - tallylog
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_login_events_tallylog
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/var/log/tallylog\|-w /var/log/tallylog"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-login-events-tallylog
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-audit-rules-login-events-tallylog
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40645"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-login-events-tallylog
    uid: 9bace014-a04d-4e28-b18a-8704e03325cc
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Mandatory Access Controls
    The system's mandatory access policy (SELinux) should not be
    arbitrarily changed by anything other than administrator action. All changes to
    MAC policy should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_mac_modification
  instructions: |-
    To determine if the system is configured to audit changes to its SELinux
    configuration files, run the following command:
    $ sudo auditctl -l | grep "dir=/etc/selinux"
    If the system is configured to watch for changes to its SELinux
    configuration, a line should be returned (including
    perm=wa indicating permissions that are watched).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-mac-modification
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-audit-rules-mac-modification
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40066"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-mac-modification
    uid: e7d7e5ba-a5d9-45e1-ad9d-b941b7addfc7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Exporting to Media (successful)
    The unauthorized exportation of data to external media could result in an information leak
    where classified information, Privacy Act information, and intellectual property could be lost. An audit
    trail should be created each time a filesystem is mounted to help identify and guard against information
    loss.
  id: xccdf_org.ssgproject.content_rule_audit_rules_media_export
  instructions: |-
    To verify that auditing is configured for all media exportation events, run the following command:
    $ sudo auditctl -l | grep syscall | grep mount
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-media-export
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-audit-rules-media-export
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41081"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-media-export
    uid: 29c32bc5-2ad4-4818-866c-dd2437eb95f8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Network Environment
    The network environment should not be modified by anything other
    than administrator action. Any change to network parameters should be
    audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_networkconfig_modification
  instructions: |-
    To determine if the system is configured to audit changes to its network configuration,
    run the following command:
    auditctl -l | egrep '(/etc/issue|/etc/issue.net|/etc/hosts|/etc/sysconfig/network)'
    If the system is configured to watch for network configuration changes, a line should be returned for
    each file specified (and perm=wa should be indicated for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-networkconfig-modification
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-audit-rules-networkconfig-modification
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40492"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-networkconfig-modification
    uid: 1868ffc5-ed04-4156-914f-ce96caa72469
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - at
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_at
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep '\bat\b' /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-at
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40883"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-at
    uid: d5ebe49e-3bce-4647-86b4-4108b5f816af
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - chage
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_chage
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep chage /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-chage
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-chage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40560"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-chage
    uid: d4468216-b76c-4e21-ac2a-ce4d60139cc3
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - chsh
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_chsh
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep chsh /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-chsh
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-chsh
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40678"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-chsh
    uid: 412ad9cd-2429-4226-8dd4-077816198cd9
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - crontab
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_crontab
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep crontab /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-crontab
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-crontab
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41106"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-crontab
    uid: 9db0c6bc-ce49-4307-b43a-138949a4d884
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - gpasswd
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_gpasswd
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep gpasswd /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-gpasswd
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-gpasswd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40555"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-gpasswd
    uid: 161098f9-ffc9-4c39-90ff-39ec065a5112
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - mount
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_mount
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep mount /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-mount
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-mount
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40259"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-mount
    uid: 581d93bd-87e2-456a-a3d2-b0d78b176385
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - newgidmap
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_newgidmap
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep newgidmap /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-newgidmap
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-newgidmap
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40095"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-newgidmap
    uid: 3c6c1b8e-6bc4-4eb3-a043-37019436dfaf
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - newgrp
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_newgrp
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep newgrp /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-newgrp
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-newgrp
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40597"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-newgrp
    uid: 4d0f072a-8be8-4ab3-b40e-1da31aced9d2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - newuidmap
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_newuidmap
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep newuidmap /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-newuidmap
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-newuidmap
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40448"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-newuidmap
    uid: eda64cad-51d8-4e02-9be6-2f0a554f3796
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - pam_timestamp_check
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_pam_timestamp_check
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep pam_timestamp_check /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-pam-timestamp-check
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-pam-timestamp-check
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40572"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-pam-timestamp-check
    uid: 947d9920-15b2-42a6-8f57-6b8171f80253
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - passwd
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_passwd
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep passwd /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-passwd
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-passwd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40422"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-passwd
    uid: 0bd4cff7-cdba-453b-998c-75062acbe861
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - postdrop
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_postdrop
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep postdrop /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-postdrop
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-postdrop
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40984"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-postdrop
    uid: 319f7064-0250-4dca-832d-dd8c8a7c1088
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - postqueue
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_postqueue
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep postqueue /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-postqueue
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-postqueue
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40191"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-postqueue
    uid: 72427310-b85a-473d-b6f9-2c49f8a93eb1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - pt_chown
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_pt_chown
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep pt_chown /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-pt-chown
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-pt-chown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41061"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-pt-chown
    uid: 3842a22a-3f3f-4057-8dc1-1e2380c11443
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - ssh-keysign
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_ssh_keysign
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep ssh-keysign /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-ssh-keysign
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-ssh-keysign
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40929"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-ssh-keysign
    uid: ee13d488-6212-4453-a30b-71378a2ee051
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - su
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_su
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep su /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-su
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-su
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40006"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-su
    uid: 80dc9d23-61c0-49d8-9ea3-42e266ff2f11
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - sudo
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_sudo
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep sudo /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-sudo
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-sudo
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40329"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-sudo
    uid: 5bdeaaec-8e11-49af-8574-95a3d6f0a590
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - sudoedit
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_sudoedit
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep sudoedit /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-sudoedit
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-sudoedit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40130"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-sudoedit
    uid: f723b27c-c7fe-4f9b-8284-e4ea008a01a1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - umount
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_umount
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep umount /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-umount
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-umount
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39987"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-umount
    uid: a7c839d3-3a45-4215-9ee2-d2b6e9211bb1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - unix_chkpwd
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_unix_chkpwd
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep unix_chkpwd /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-unix-chkpwd
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-unix-chkpwd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40549"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-unix-chkpwd
    uid: c95ae434-56d9-4ff6-869f-91e6187084e6
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - userhelper
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_userhelper
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep userhelper /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-userhelper
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-userhelper
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41012"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-userhelper
    uid: 736b689c-ec69-47a0-ae13-7074370e5f63
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - usernetctl
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_usernetctl
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep usernetctl /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-usernetctl
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-audit-rules-privileged-commands-usernetctl
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40989"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-privileged-commands-usernetctl
    uid: caa316cb-0a0a-48ae-815a-ab893a55e0cb
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Process and Session Initiation Information
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_session_events
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-session-events
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-audit-rules-session-events
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40201"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-session-events
    uid: 8bc64179-063d-440e-8af6-8d748abf8014
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects System Administrator Actions
    The actions taken by system administrators should be audited to keep a record
    of what was executed on the system, as well as, for accountability purposes.
  id: xccdf_org.ssgproject.content_rule_audit_rules_sysadmin_actions
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/etc/sudoers\|watch=/etc/sudoers.d\|-w /etc/sudoers\|-w /etc/sudoers.d"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-sysadmin-actions
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-audit-rules-sysadmin-actions
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40877"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-sysadmin-actions
    uid: dc60b485-306f-4e29-9a0e-184b96a6a940
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record attempts to alter time through adjtimex
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_adjtimex
  instructions: |-
    To determine if the system is configured to audit calls to the
    adjtimex system call, run the following command:
    preserve$ sudo grep "adjtimex" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-adjtimex
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-audit-rules-time-adjtimex
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40086"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-time-adjtimex
    uid: f6a4726a-e866-4b84-8655-79f842498d90
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Time Through clock_settime
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_clock_settime
  instructions: |-
    To determine if the system is configured to audit calls to the
    clock_settime system call, run the following command:
    preserve$ sudo grep "clock_settime" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-clock-settime
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-audit-rules-time-clock-settime
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40336"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-time-clock-settime
    uid: 505f8642-d0a8-4b66-af3a-fae01727b9ce
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record attempts to alter time through settimeofday
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_settimeofday
  instructions: |-
    To determine if the system is configured to audit calls to the
    settimeofday system call, run the following command:
    preserve$ sudo grep "settimeofday" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-settimeofday
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-audit-rules-time-settimeofday
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41086"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-time-settimeofday
    uid: 88312a98-a701-4e4e-8b05-550727ae5ddf
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Time Through stime
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_stime
  instructions: |-
    If the system is not configured to audit time changes, this is a finding.
    If the system is 64-bit only, this is not applicable
    ocil: |
    To determine if the system is configured to audit calls to the
    stime system call, run the following command:
    preserve$ sudo grep "stime" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-stime
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-audit-rules-time-stime
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41068"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-time-stime
    uid: 7a20d65c-3e55-4ddd-bd42-56c911b91e34
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter the localtime File
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_watch_localtime
  instructions: |-
    To determine if the system is configured to audit attempts to
    alter time via the /etc/localtime file, run the following
    command:
    $ sudo auditctl -l | grep "watch=/etc/localtime"
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-watch-localtime
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-audit-rules-time-watch-localtime
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40577"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-time-watch-localtime
    uid: a7a544a7-77f8-48ab-a10c-0e38af449e08
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - chmod
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_chmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    chmod system call, run the following command:
    preserve$ sudo grep "chmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-chmod
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-chmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39992"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-chmod
    uid: d6e24fec-85a7-4644-a92e-4da3841871b4
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - chown
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_chown
  instructions: |-
    To determine if the system is configured to audit calls to the
    chown system call, run the following command:
    preserve$ sudo grep "chown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-chown
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-chown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40692"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-chown
    uid: fca17ebf-9bd0-4349-8b40-b6254fcac59b
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - creat
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    creat system call, run the following command:
    preserve$ sudo grep "creat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-creat
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40221"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-creat
    uid: f0126b64-7d11-49d7-8a83-e9b87e914a50
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fchmod
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmod system call, run the following command:
    preserve$ sudo grep "fchmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchmod
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40245"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchmod
    uid: f34d01ad-c9c6-44be-9452-6c6f0c1abd62
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fchmodat
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchmodat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmodat system call, run the following command:
    preserve$ sudo grep "fchmodat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchmodat
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchmodat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40454"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchmodat
    uid: a046faaa-0721-4339-8eef-29f876d6a6f5
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - fchown
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchown system call, run the following command:
    preserve$ sudo grep "fchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchown
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40933"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchown
    uid: e424cd09-bc0f-4641-89f3-c54a3afb44ba
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - fchownat
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchownat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchownat system call, run the following command:
    preserve$ sudo grep "fchownat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchownat
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchownat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40279"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fchownat
    uid: d01addab-714d-4de5-8e49-2eed4b987540
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fremovexattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fremovexattr system call, run the following command:
    preserve$ sudo grep "fremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fremovexattr
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40275"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fremovexattr
    uid: 361ba4f7-c16f-4053-aad6-f0940764a972
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fsetxattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fsetxattr system call, run the following command:
    preserve$ sudo grep "fsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fsetxattr
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39990"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-fsetxattr
    uid: ec860612-b774-4d90-8232-b0989d332742
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - ftruncate
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_ftruncate
  instructions: |-
    To determine if the system is configured to audit calls to the
    ftruncate system call, run the following command:
    preserve$ sudo grep "ftruncate" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-ftruncate
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-ftruncate
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41090"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-ftruncate
    uid: a42f192e-ad94-4580-8e73-268679781b1e
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - lchown
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_lchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    lchown system call, run the following command:
    preserve$ sudo grep "lchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-lchown
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-lchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40521"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-lchown
    uid: 624457cc-453b-44aa-96d2-5c547354174b
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - lremovexattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_lremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lremovexattr system call, run the following command:
    preserve$ sudo grep "lremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-lremovexattr
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-lremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40473"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-lremovexattr
    uid: 1ceb32cd-5309-474c-b889-69193cbc1fe7
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - lsetxattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_lsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lsetxattr system call, run the following command:
    preserve$ sudo grep "lsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-lsetxattr
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-lsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39970"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-lsetxattr
    uid: 9ba2f0e4-4d37-4ed4-8cb7-c0a72dd696f9
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - open
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40373"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open
    uid: 6bff5c6d-d100-4f1a-84a0-c281f2f0d3ea
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - open_by_handle_at
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40610"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at
    uid: a63946f1-7379-465b-908a-280f0365a6fa
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Creation Attempts to Files - open_by_handle_at O_CREAT
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at_o_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at-o-creat
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40527"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-creat
    uid: 48a211e9-527b-438e-ae30-2178f33637fe
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&0100 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-create
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Modification Attempts to Files - open_by_handle_at O_TRUNC_WRITE
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at_o_trunc_write
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at-o-trunc-write
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-trunc-write
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40957"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-trunc-write
    uid: 07ee3788-df09-41eb-813f-1792a07992fe
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&01003 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-modification
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Unauthorized Access Attempts To open_by_handle_at Are Ordered Correctly
    The more specific rules cover a subset of events covered by the less specific rules.
    By ordering them from more specific to less specific, it is assured that the less specific
    rule will not catch events better recorded by the more specific rule.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at_rule_order
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at-rule-order
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at-rule-order
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40126"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-by-handle-at-rule-order
    uid: ccc4137f-e866-4ec8-b237-0fb58138e5b8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Creation Attempts to Files - open O_CREAT
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_o_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-o-creat
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-o-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40346"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-o-creat
    uid: 5a65ca0c-7b79-41fa-a16b-1f8b4c4bbc9b
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&0100 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-create
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Modification Attempts to Files - open O_TRUNC_WRITE
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_o_trunc_write
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-o-trunc-write
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-o-trunc-write
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40761"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-o-trunc-write
    uid: 043a70c6-70c5-4486-a3b4-fb0ba527ae63
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&01003 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-modification
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Rules For Unauthorized Attempts To open Are Ordered Correctly
    The more specific rules cover a subset of events covered by the less specific rules.
    By ordering them from more specific to less specific, it is assured that the less specific
    rule will not catch events better recorded by the more specific rule.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_rule_order
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-rule-order
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-rule-order
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40061"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-open-rule-order
    uid: 6b8cfb2e-573a-40b1-944b-0662a3d570a8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - openat
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40033"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat
    uid: 41d9200c-41f4-4261-aa6b-021b0f8ef813
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Creation Attempts to Files - openat O_CREAT
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat_o_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat-o-creat
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat-o-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40309"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat-o-creat
    uid: 3874c192-ce2c-4545-88e1-fd8e5101a8f1
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&0100 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-create
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Modification Attempts to Files - openat O_TRUNC_WRITE
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat_o_trunc_write
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat-o-trunc-write
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat-o-trunc-write
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40504"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat-o-trunc-write
    uid: 7f618920-45fa-44be-9943-43468b4cc128
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&01003 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-modification
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Rules For Unauthorized Attempts To openat Are Ordered Correctly
    The more specific rules cover a subset of events covered by the less specific rules.
    By ordering them from more specific to less specific, it is assured that the less specific
    rule will not catch events better recorded by the more specific rule.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat_rule_order
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat-rule-order
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat-rule-order
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41075"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-openat-rule-order
    uid: ce401eeb-d35d-459f-9994-d9a18650d0c1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - removexattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_removexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    removexattr system call, run the following command:
    preserve$ sudo grep "removexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-removexattr
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-removexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40827"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-removexattr
    uid: 485b4788-2d8a-413b-9c04-8c2b5431d11a
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - rename
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_rename
  instructions: |-
    To determine if the system is configured to audit calls to the
    rename system call, run the following command:
    preserve$ sudo grep "rename" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-rename
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-rename
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40514"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-rename
    uid: eb52a761-3ead-44f1-8128-8703d2e7cd4c
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - renameat
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_renameat
  instructions: |-
    To determine if the system is configured to audit calls to the
    renameat system call, run the following command:
    preserve$ sudo grep "renameat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-renameat
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-renameat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40616"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-renameat
    uid: 778816ed-9929-416c-b156-a2e0b9874d3b
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - setxattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_setxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    setxattr system call, run the following command:
    preserve$ sudo grep "setxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-setxattr
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-setxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40230"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-setxattr
    uid: 969c7980-a99b-4010-9ab7-ff230224b643
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - truncate
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_truncate
  instructions: |-
    To determine if the system is configured to audit calls to the
    truncate system call, run the following command:
    preserve$ sudo grep "truncate" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-truncate
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-truncate
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40145"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-truncate
    uid: c8fdb799-21e8-433d-83e4-e0fef0699b19
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - unlink
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_unlink
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlink system call, run the following command:
    preserve$ sudo grep "unlink" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-unlink
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-unlink
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40121"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-unlink
    uid: d242173d-5ee1-41ca-90b6-08ce0fc6c83e
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - unlinkat
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_unlinkat
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlinkat system call, run the following command:
    preserve$ sudo grep "unlinkat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-unlinkat
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-unlinkat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40651"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-unsuccessful-file-modification-unlinkat
    uid: 01ae52c5-9ccf-4fd2-a712-9bd24d20ab52
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/group
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_group
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/group)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-group
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-audit-rules-usergroup-modification-group
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40674"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-usergroup-modification-group
    uid: 505b58ac-53ac-4da5-a4c6-75f5bf250f02
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/gshadow
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_gshadow
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/gshadow)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-gshadow
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-audit-rules-usergroup-modification-gshadow
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40166"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-usergroup-modification-gshadow
    uid: fff591e7-91c7-41df-a624-3a82fac023bb
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/security/opasswd
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_opasswd
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/security/opasswd)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-opasswd
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-audit-rules-usergroup-modification-opasswd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40509"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-usergroup-modification-opasswd
    uid: 3b1f1229-6182-4d66-8ebe-8c80b38006e2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/passwd
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_passwd
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/passwd)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-passwd
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-audit-rules-usergroup-modification-passwd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40290"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-usergroup-modification-passwd
    uid: b5a9ccff-a920-4aac-9d63-e29ad4e3c9cd
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/shadow
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_shadow
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/shadow)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-shadow
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-audit-rules-usergroup-modification-shadow
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40021"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-audit-rules-usergroup-modification-shadow
    uid: 051dd04d-541f-4faf-87f3-d0f7c8105a26
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Disk Error Action on Disk Error
    Taking appropriate action in case of disk errors will minimize the possibility of
    losing audit records.
  id: xccdf_org.ssgproject.content_rule_auditd_data_disk_error_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to either log to syslog,
    switch to single-user mode, execute a script,
    or halt when the disk errors:
    disk_error_action single
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-disk-error-action
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-auditd-data-disk-error-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40375"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-disk-error-action
    uid: 08e7349e-ac9c-4d89-bab7-3005e51f0698
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Disk Full Action when Disk Space Is Full
    Taking appropriate action in case of a filled audit storage volume will minimize
    the possibility of losing audit records.
  id: xccdf_org.ssgproject.content_rule_auditd_data_disk_full_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to either log to syslog,
    switch to single-user mode, execute a script,
    or halt when the disk is out of space:
    disk_full_action single
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-disk-full-action
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-auditd-data-disk-full-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40631"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-disk-full-action
    uid: b2bc6cc1-015b-495c-80ed-4834a4764b62
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd admin_space_left Action on Low Disk Space
    Administrators should be made aware of an inability to record
    audit records. If a separate partition or logical volume of adequate size
    is used, running low on space for audit records should never occur.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_admin_space_left_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to either suspend, switch to single user mode,
    or halt when disk space has run low:
    admin_space_left_action single
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-admin-space-left-action
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-auditd-data-retention-admin-space-left-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40365"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-admin-space-left-action
    uid: 75fe4c35-a8a8-431a-a5a9-28f0eceb2b86
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd flush priority
    Audit data should be synchronously written to disk to ensure
    log integrity. These parameters assure that all audit event data is fully
    synchronized with the log files on the disk.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_flush
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to synchronize audit event data
    with the log files on the disk:
    $ sudo grep flush /etc/audit/auditd.conf
    flush = DATA
    Acceptable values are DATA, and SYNC. The setting is
    case-insensitive.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-flush
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-auditd-data-retention-flush
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40361"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-flush
    uid: bf076b0d-be94-4719-ab56-60791b592f0b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Max Log File Size
    The total storage for audit log files must be large enough to retain
    log information over the period required. This is a function of the maximum
    log file size and the number of logs retained.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_max_log_file
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine how much data the system will retain in each audit log file:
    $ sudo grep max_log_file /etc/audit/auditd.conf
    max_log_file = 6
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-max-log-file
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-auditd-data-retention-max-log-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40657"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-max-log-file
    uid: 19fc5f58-ec0f-4e98-b71d-84292647d056
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd max_log_file_action Upon Reaching Maximum Log Size
    Automatically rotating logs (by setting this to rotate)
    minimizes the chances of the system unexpectedly running out of disk space by
    being overwhelmed with log data. However, for systems that must never discard
    log data, or which use external processes to transfer it and reclaim space,
    keep_logs can be employed.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_max_log_file_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to rotate logs when they reach their
    maximum size:
    $ sudo grep max_log_file_action /etc/audit/auditd.conf
    max_log_file_action rotate
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-max-log-file-action
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-auditd-data-retention-max-log-file-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40382"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-max-log-file-action
    uid: cdc082d8-9085-46c5-9df8-9a3a0694bd12
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Number of Logs Retained
    The total storage for audit log files must be large enough to retain
    log information over the period required. This is a function of the maximum log
    file size and the number of logs retained.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_num_logs
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine how many logs the system is configured to retain after rotation:
    $ sudo grep num_logs /etc/audit/auditd.conf
    num_logs = 5
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-num-logs
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-auditd-data-retention-num-logs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40945"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-num-logs
    uid: 8104fbb4-050a-4659-8b08-c789459cedbd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd space_left on Low Disk Space
    Notifying administrators of an impending disk space problem may allow them to
    take corrective action prior to any disruption.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_space_left
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured correctly:
    space_left SIZE_in_MB
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-space-left
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-auditd-data-retention-space-left
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40036"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-space-left
    uid: 3a31c941-3e1d-4b21-a776-8d77edcf4721
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd space_left Action on Low Disk Space
    Notifying administrators of an impending disk space problem may
    allow them to take corrective action prior to any disruption.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_space_left_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to email the administrator when
    disk space is starting to run low:
    $ sudo grep space_left_action /etc/audit/auditd.conf
    space_left_action
    Acceptable values are email, suspend, single, and halt.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-space-left-action
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-auditd-data-retention-space-left-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41046"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-data-retention-space-left-action
    uid: d6beab00-3f2d-4d69-b3cc-da2eaf415a5b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set number of records to cause an explicit flush to audit logs
    If option freq isn't set to 50, the flush to disk
    may happen after higher number of records, increasing the danger
    of audit loss.
  id: xccdf_org.ssgproject.content_rule_auditd_freq
  instructions: |-
    To verify that Audit Daemon is configured to flush to disk after
    every 50 records, run the following command:
    $ sudo grep freq /etc/audit/auditd.conf
    The output should return the following:
    freq = 50
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-freq
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-auditd-freq
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40773"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-freq
    uid: 9bf8bbfe-f30e-42d1-823e-1bafb4e7cd0a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Include Local Events in Audit Logs
    If option local_events isn't set to yes only events from
    network will be aggregated.
  id: xccdf_org.ssgproject.content_rule_auditd_local_events
  instructions: |-
    To verify that Audit Daemon is configured to include local events, run the
    following command:
    $ sudo grep local_events /etc/audit/auditd.conf
    The output should return the following:
    local_events = yes
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-local-events
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-auditd-local-events
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41042"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-local-events
    uid: 2d3cca66-8684-4fb8-820e-411a747cc786
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Resolve information before writing to audit logs
    If option log_format isn't set to ENRICHED, the
    audit records will be stored in a format exactly as the kernel sends them.
  id: xccdf_org.ssgproject.content_rule_auditd_log_format
  instructions: |-
    To verify that Audit Daemon is configured to resolve all uid, gid, syscall,
    architecture, and socket address information before writing the event to disk,
    run the following command:
    $ sudo grep log_format /etc/audit/auditd.conf
    The output should return the following:
    log_format = ENRICHED
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-log-format
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-auditd-log-format
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40250"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-log-format
    uid: f5ce6046-9363-498a-bd14-3184f4a327c9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set hostname as computer node name in audit logs
    If option name_format is left at its default value of
    none, audit events from different computers may be hard
    to distinguish.
  id: xccdf_org.ssgproject.content_rule_auditd_name_format
  instructions: |-
    To verify that Audit Daemon is configured to record the hostname
    in audit events, run the following command:
    $ sudo grep name_format /etc/audit/auditd.conf
    The output should return the following:
    name_format = hostname
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-name-format
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-auditd-name-format
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40939"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-name-format
    uid: 46fce63c-2b30-4cec-99e7-4768a66291d7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Write Audit Logs to the Disk
    If write_logs isn't set to yes, the Audit logs will
    not be written to the disk.
  id: xccdf_org.ssgproject.content_rule_auditd_write_logs
  instructions: |-
    To verify that Audit Daemon is configured to write logs to the disk, run the
    following command:
    $ sudo grep write_logs /etc/audit/auditd.conf
    The output should return the following:
    write_logs = yes
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-write-logs
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-auditd-write-logs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40833"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-auditd-write-logs
    uid: 736144cc-1369-4c71-b165-23e718f964c1
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Modify the System Login Banner
    Display of a standardized and approved use notification before granting
    access to the operating system ensures privacy and security notification
    verbiage used is consistent with applicable federal laws, Executive Orders,
    directives, policies, regulations, standards, and guidance.

    System use notifications are required only for access via login interfaces
    with human users and are not required when such human interfaces do not
    exist.
  id: xccdf_org.ssgproject.content_rule_banner_etc_issue
  instructions: |-
    To check if the system login banner is compliant,
    run the following command:
    $ cat /etc/issue
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: banner-etc-issue
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-banner-etc-issue
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40924"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-banner-etc-issue
    uid: f66e2aeb-7395-43eb-8efa-1e508c787d8d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Booting from USB Devices in Boot Firmware
    Booting a system from a USB device would allow an attacker to
    circumvent any security measures provided by the operating system. Attackers
    could mount partitions and modify the configuration of the OS.
  id: xccdf_org.ssgproject.content_rule_bios_disable_usb_boot
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: bios-disable-usb-boot
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-bios-disable-usb-boot
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40661"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-bios-disable-usb-boot
    uid: 55066da2-4f4b-407a-92a1-5f858eec317c
  severity: unknown
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable chrony daemon from acting as server
    Minimizing the exposure of the server functionality of the chrony
    daemon diminishes the attack surface.
  id: xccdf_org.ssgproject.content_rule_chronyd_client_only
  instructions: |-
    To verify that port has been set properly, perform the following:
    $ grep '\bport\b' /etc/chrony.conf
    The output should return
    port 0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-client-only
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-chronyd-client-only
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40909"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-chronyd-client-only
    uid: f02486f4-aba2-41a3-9203-df2d1f679e6b
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable network management of chrony daemon
    Not exposing the management interface of the chrony daemon on
    the network diminishes the attack space.
  id: xccdf_org.ssgproject.content_rule_chronyd_no_chronyc_network
  instructions: |-
    To verify that cmdport has been set properly, perform the following:
    $ grep '\bcmdport\b' /etc/chrony.conf
    The output should return
    cmdport 0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-no-chronyc-network
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-chronyd-no-chronyc-network
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40159"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-chronyd-no-chronyc-network
    uid: 95cabc90-93d6-426b-8a34-6f1998bdb81a
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Time Service Maxpoll Interval
    Inaccurate time stamps make it more difficult to correlate
    events and can lead to an inaccurate analysis. Determining the correct
    time a particular event occurred on a system is critical when conducting
    forensic analysis and investigating system events. Sources outside the
    configured acceptable allowance (drift) may be inaccurate.
  id: xccdf_org.ssgproject.content_rule_chronyd_or_ntpd_set_maxpoll
  instructions: |-
    To verify that maxpoll has been set properly, perform the following:
    $ sudo grep maxpoll /etc/ntp.conf /etc/chrony.conf
    The output should return
    maxpoll .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-or-ntpd-set-maxpoll
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-chronyd-or-ntpd-set-maxpoll
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40357"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-chronyd-or-ntpd-set-maxpoll
    uid: 5e19a304-8c7c-4e97-ad87-9742e01af4ab
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Specify Additional Remote NTP Servers
    Specifying additional NTP servers increases the availability of
    accurate time data, in the event that one of the specified servers becomes
    unavailable. This is typical for a system acting as an NTP server for
    other systems.
  id: xccdf_org.ssgproject.content_rule_chronyd_or_ntpd_specify_multiple_servers
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-or-ntpd-specify-multiple-servers
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-chronyd-or-ntpd-specify-multiple-servers
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40394"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-chronyd-or-ntpd-specify-multiple-servers
    uid: f2d39de7-2761-45d1-b635-aac9e87555b6
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Specify a Remote NTP Server
    Synchronizing with an NTP server makes it possible to collate system
    logs from multiple sources or correlate computer events with real time events.
  id: xccdf_org.ssgproject.content_rule_chronyd_or_ntpd_specify_remote_server
  instructions: |-
    To verify that a remote NTP service is configured for time synchronization,
    open the following file:
    /etc/chrony.conf in the case the system in question is
    configured to use the chronyd as the NTP daemon (default setting)/etc/ntp.conf in the case the system in question is configured
    to use the ntpd as the NTP daemon
    In the file, there should be a section similar to the following:
    server ntpserver
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-or-ntpd-specify-remote-server
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-chronyd-or-ntpd-specify-remote-server
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40998"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-chronyd-or-ntpd-specify-remote-server
    uid: 141d9e99-7897-446d-91b5-6ab48864e5f7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure System Cryptography Policy
    Centralized cryptographic policies simplify applying secure ciphers across an operating system and
    the applications that run on that operating system. Use of weak or untested encryption algorithms
    undermines the purposes of utilizing encryption to protect data.
  id: xccdf_org.ssgproject.content_rule_configure_crypto_policy
  instructions: |-
    To verify that cryptography policy has been configured correctly, run the
    following command:
    $ update-crypto-policies --show
    The output should return .
    Run the command to check if the policy is correctly applied:
    $ update-crypto-policies --is-applied
    The output should be The configured policy is applied.
    Moreover, check if settings for selected crypto policy are as expected.
    List all libraries for which it holds that their crypto policies do not have symbolic link in /etc/crypto-policies/back-ends.
    $ ls -l /etc/crypto-policies/back-ends/ | grep '^[^l]' | tail -n +2 | awk -F' ' '{print $NF}' | awk -F'.' '{print $1}' | sort
    Subsequently, check if matching libraries have drop in files in the /etc/crypto-policies/local.d directory.
    $ ls /etc/crypto-policies/local.d/ | awk -F'-' '{print $1}' | uniq | sort
    Outputs of two previous commands should match.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-crypto-policy
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-master-configure-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41096"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-configure-crypto-policy
    uid: cb090daf-654d-4dde-9c22-b016ccc2dd26
  severity: high
  status: FAIL
  warnings:
  - The system needs to be rebooted for these changes to take effect.
  - System Crypto Modules must be provided by a vendor that undergoes FIPS-140 certifications. FIPS-140 is applicable to all Federal agencies that use cryptographic-based security systems to protect sensitive information in computer and telecommunication systems (including voice systems) as defined in Section 5131 of the Information Technology Management Reform Act of 1996, Public Law 104-106. This standard shall be used in designing and implementing cryptographic modules that Federal departments and agencies operate or are operated for them under contract. See *https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.140-2.pdf* To meet this, the system has to have cryptographic software provided by a vendor that has undergone this certification. This means providing documentation, test results, design information, and independent third party review by an accredited lab. While open source software is capable of meeting this, it does not meet FIPS-140 unless the vendor submits to this process.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Kerberos to use System Crypto Policy
    Overriding the system crypto policy makes the behavior of Kerberos violate expectations,
    and makes system configuration more fragmented.
  id: xccdf_org.ssgproject.content_rule_configure_kerberos_crypto_policy
  instructions: |-
    Check that the symlink exists and target the correct kerberos crypto policy, with the following command:
    file /etc/krb5.conf.d/crypto-policies
    If command ouput shows the following line, kerberos is configured to use the system-wide crypto policy.
    /etc/krb5.conf.d/crypto-policies: symbolic link to /etc/crypto-policies/back-ends/krb5.config
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-kerberos-crypto-policy
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-configure-kerberos-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40805"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-configure-kerberos-crypto-policy
    uid: 45998d3f-7721-4c31-b715-55a8db2f3f2b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure OpenSSL library to use System Crypto Policy
    Overriding the system crypto policy makes the behavior of the Java runtime violates expectations,
    and makes system configuration more fragmented.
  id: xccdf_org.ssgproject.content_rule_configure_openssl_crypto_policy
  instructions: |-
    To verify that OpenSSL uses the system crypto policy, check out that the OpenSSL config file
    /etc/pki/tls/openssl.cnf contains the [ crypto_policy ] section with the
    .include /etc/crypto-policies/back-ends/opensslcnf.config directive:
    grep '\.include\s* /etc/crypto-policies/back-ends/opensslcnf.config$' /etc/pki/tls/openssl.cnf.
          Is it the case that the OpenSSL config file doesn't contain the whole section,
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-openssl-crypto-policy
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-configure-openssl-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40864"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-configure-openssl-crypto-policy
    uid: 0089b56a-0dde-4c85-8aaa-77c150089f5d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure SSH to use System Crypto Policy
    Overriding the system crypto policy makes the behavior of the SSH service violate expectations,
    and makes system configuration more fragmented.
  id: xccdf_org.ssgproject.content_rule_configure_ssh_crypto_policy
  instructions: |-
    Check that the CRYPTO_POLICY variable is not set or is commented in the
    /etc/sysconfig/sshd.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-ssh-crypto-policy
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-configure-ssh-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41031"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-configure-ssh-crypto-policy
    uid: dded4961-007b-47fa-8d2d-cb11822fd2d6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Log USBGuard daemon audit events using Linux Audit
    Using the Linux Audit logging allows for centralized trace
    of events.
  id: xccdf_org.ssgproject.content_rule_configure_usbguard_auditbackend
  instructions: |-
    To verify that Linux Audit logging si enabled for the USBGuard daemon,
    run the following command:
    $ sudo grep AuditBackend /etc/usbguard/usbguard-daemon.conf
    The output should be
    AuditBackend=LinuxAudit
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-usbguard-auditbackend
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-configure-usbguard-auditbackend
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40162"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-configure-usbguard-auditbackend
    uid: 8c99dedf-4745-4bfa-a83d-9bc34b3dfe52
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable core dump backtraces
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data
    and is generally useful only for developers or system operators trying to
    debug problems.

    Enabling core dumps on production systems is not recommended,
    however there may be overriding operational requirements to enable advanced
    debuging. Permitting temporary enablement of core dumps during such situations
    should be reviewed through local needs and policy.
  id: xccdf_org.ssgproject.content_rule_coredump_disable_backtraces
  instructions: |-
    To verify that logging core dump backtraces is disabled, run the
    following command:
    $ grep ProcessSizeMax /etc/systemd/coredump.conf
    The output should be:
    ProcessSizeMax=0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coredump-disable-backtraces
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-coredump-disable-backtraces
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40350"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coredump-disable-backtraces
    uid: 0b8be7a1-8e77-45e4-b11e-1fbe7d449181
  severity: medium
  status: FAIL
  warnings:
  - If the /etc/systemd/coredump.conf file does not already contain the [Coredump] section, the value will not be configured correctly.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable storing core dump
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data
    and is generally useful only for developers or system operators trying to
    debug problems. Enabling core dumps on production systems is not recommended,
    however there may be overriding operational requirements to enable advanced
    debuging. Permitting temporary enablement of core dumps during such situations
    should be reviewed through local needs and policy.
  id: xccdf_org.ssgproject.content_rule_coredump_disable_storage
  instructions: |-
    To verify that storing core dumps are disabled, run the following command:
    $ grep Storage /etc/systemd/coredump.conf
    The output should be:
    Storage=none
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coredump-disable-storage
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-coredump-disable-storage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40252"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coredump-disable-storage
    uid: 4146f010-7a06-4538-b506-bfe00bcb03c5
  severity: medium
  status: FAIL
  warnings:
  - If the /etc/systemd/coredump.conf file does not already contain the [Coredump] section, the value will not be configured correctly.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Extend Audit Backlog Limit for the Audit Daemon
    audit_backlog_limit sets the queue length for audit events awaiting transfer
    to the audit daemon. Until the audit daemon is up and running, all log messages
    are stored in this queue.  If the queue is overrun during boot process, the action
    defined by audit failure flag is taken.
  id: xccdf_org.ssgproject.content_rule_coreos_audit_backlog_limit_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    audit=1, then auditing is enabled at boot time.

    To ensure audit_backlog_limit=8192 is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-audit-backlog-limit-kernel-argument
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-coreos-audit-backlog-limit-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39965"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-audit-backlog-limit-kernel-argument
    uid: db71ee47-7324-435f-b43e-10092f501d32
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Auditing for Processes Which Start Prior to the Audit Daemon
    Each process on the system carries an "auditable" flag which indicates whether
    its activities can be audited. Although auditd takes care of enabling
    this for all processes which launch after it does, adding the kernel argument
    ensures it is set for every process during boot.
  id: xccdf_org.ssgproject.content_rule_coreos_audit_option
  instructions: |-
    Inspect the form of BLS (Boot Loader Specification) options lines for the Linux operating system
    in /boot/loader/entries/*.conf. If they include audit=1, then auditing
    is enabled at boot time.
    # grep 'options.*audit=1.*' /boot/loader/entires/*.conf
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-audit-option
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-coreos-audit-option
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40268"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-audit-option
    uid: cc6ed3ac-2862-490c-af3c-6120ca133bfd
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify that Interactive Boot is Disabled
    Using interactive boot, the console user could disable auditing, firewalls,
    or other services, weakening system security.
  id: xccdf_org.ssgproject.content_rule_coreos_disable_interactive_boot
  instructions: |-
    Inspect /proc/cmdline for any instances of
    systemd.confirm_spawn=(1|yes|true|on) in the kernel boot arguments.
    Presence of a systemd.confirm_spawn=(1|yes|true|on) indicates
    that interactive boot is enabled at boot time.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-disable-interactive-boot
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-coreos-disable-interactive-boot
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40238"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-disable-interactive-boot
    uid: faf80fb7-811e-4d8d-a9c7-eb63cc8828b2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure SELinux Not Disabled in the kernel arguments
    Disabling a major host protection feature, such as SELinux, at boot time prevents
    it from confining system services at boot time.  Further, it increases
    the chances that it will remain off during system operation.
  id: xccdf_org.ssgproject.content_rule_coreos_enable_selinux_kernel_argument
  instructions: |-
    Inspect /proc/cmdline for any instances of selinux=0
    in the kernel boot arguments.  Presence of selinux=0 indicates
    that SELinux is disabled at boot time.

    If it would be disabled anywhere, make sure to enable it via a
    MachineConfig object.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-enable-selinux-kernel-argument
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-coreos-enable-selinux-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40786"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-enable-selinux-kernel-argument
    uid: 9a10fc2f-2b1c-4550-a811-8ae4d249d4c4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Support for USB via Bootloader Configuration
    Disabling the USB subsystem within the Linux kernel at system boot will
    protect against potentially malicious USB devices, although it is only practical
    in specialized systems.
  id: xccdf_org.ssgproject.content_rule_coreos_nousb_kernel_argument
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-nousb-kernel-argument
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-coreos-nousb-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40962"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-nousb-kernel-argument
    uid: 9a979bb5-a65a-4e37-a340-3780755b51cd
  severity: medium
  status: FAIL
  warnings:
  - Disabling all kernel support for USB will cause problems for systems with USB-based keyboards, mice, or printers. This configuration is infeasible for systems which require USB devices, which is common.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable page allocator poisoning
    Poisoning writes an arbitrary value to freed pages, so any modification or
    reference to that page after being freed or before being initialized will be
    detected and prevented.
    This prevents many types of use-after-free vulnerabilities at little performance cost.
    Also prevents leak of data and detection of corrupted memory.
  id: xccdf_org.ssgproject.content_rule_coreos_page_poison_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    page_poison=1, then page poisoning is enabled at boot time.

    To ensure page_poison=1 is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-page-poison-kernel-argument
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-coreos-page-poison-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40465"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-page-poison-kernel-argument
    uid: 10ebcaa4-83b5-4deb-a420-7be44017db23
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Page-Table Isolation (KPTI)
    Kernel page-table isolation is a kernel feature that mitigates
    the Meltdown security vulnerability and hardens the kernel
    against attempts to bypass kernel address space layout
    randomization (KASLR).
  id: xccdf_org.ssgproject.content_rule_coreos_pti_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    pti=on, then Kernel page-table isolation is enabled at boot time.

    To ensure pti=on is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-pti-kernel-argument
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-coreos-pti-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40838"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-pti-kernel-argument
    uid: 9d0670b5-2843-4c70-bf7b-674cd5b1cc9a
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable vsyscalls
    Virtual Syscalls provide an opportunity of attack for a user who has control
    of the return instruction pointer.
  id: xccdf_org.ssgproject.content_rule_coreos_vsyscall_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    vsyscall=none, then virtual syscalls are not enabled at boot time.

    To ensure vsyscall=none is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-vsyscall-kernel-argument
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: INFO
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-coreos-vsyscall-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40286"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-coreos-vsyscall-kernel-argument
    uid: 4797a1f3-6e38-4eb9-8e40-8a52db26e562
  severity: medium
  status: INFO
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Access Events to Audit Log Directory
    Attempts to read the logs should be recorded, suspicious access to audit log files could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.'
  id: xccdf_org.ssgproject.content_rule_directory_access_var_log_audit
  instructions: |-
    To determine if the system is configured to audit accesses to
    /var/log/audit directory, run the following command:
    preserve$ sudo grep "dir=/var/log/audit" /etc/audit/audit.rules
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: directory-access-var-log-audit
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-directory-access-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39968"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-directory-access-var-log-audit
    uid: cd3a9548-b1f1-4650-ad59-9c4dac3aff20
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    System Audit Logs Must Have Mode 0750 or Less Permissive
    If users can write to audit logs, audit trails can be modified or destroyed.
  id: xccdf_org.ssgproject.content_rule_directory_permissions_var_log_audit
  instructions: |-
    Run the following command to check the mode of the system audit logs:
    $ sudo ls -ld /var/log/audit
    Audit log directories must be mode 0700 or less permissive.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: directory-permissions-var-log-audit
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-directory-permissions-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40854"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-directory-permissions-var-log-audit
    uid: 5dbf924b-6d51-4018-9d11-7c31f0912acc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Ctrl-Alt-Del Burst Action
    A locally logged-in user who presses Ctrl-Alt-Del, when at the console,
    can reboot the system. If accidentally pressed, as could happen in
    the case of mixed OS environment, this can create the risk of short-term
    loss of availability of systems due to unintentional reboot.
  id: xccdf_org.ssgproject.content_rule_disable_ctrlaltdel_burstaction
  instructions: |-
    To ensure the system is configured to ignore the Ctrl-Alt-Del setting,
    enter the following command:
    $ sudo grep -i ctrlaltdelburstaction /etc/systemd/system.conf
    The output should return:
    CtrlAltDelBurstAction=none
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: disable-ctrlaltdel-burstaction
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-disable-ctrlaltdel-burstaction
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40188"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-disable-ctrlaltdel-burstaction
    uid: 71fa27aa-119b-445e-a638-8d462c6e6201
  severity: high
  status: FAIL
  warnings:
  - Disabling the Ctrl-Alt-Del key sequence in /etc/init/control-alt-delete.conf DOES NOT disable the Ctrl-Alt-Del key sequence if running in runlevel 6 (e.g. in GNOME, KDE, etc.)! The Ctrl-Alt-Del key sequence will only be disabled if running in the non-graphical runlevel 3.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Ctrl-Alt-Del Reboot Activation
    A locally logged-in user who presses Ctrl-Alt-Del, when at the console,
    can reboot the system. If accidentally pressed, as could happen in
    the case of mixed OS environment, this can create the risk of short-term
    loss of availability of systems due to unintentional reboot.
  id: xccdf_org.ssgproject.content_rule_disable_ctrlaltdel_reboot
  instructions: |-
    To ensure the system is configured to mask the Ctrl-Alt-Del sequence, Check
    that the ctrl-alt-del.target is masked and not active with the following
    command:
    sudo systemctl status ctrl-alt-del.target
    The output should indicate that the target is masked and not active. It
    might resemble following output:
    ctrl-alt-del.target
    Loaded: masked (/dev/null; bad)
    Active: inactive (dead)
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: disable-ctrlaltdel-reboot
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-disable-ctrlaltdel-reboot
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40869"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-disable-ctrlaltdel-reboot
    uid: 6729a0c8-8e54-4df1-8a91-b79c717be9f4
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Core Dumps for All Users
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data and is generally useful
    only for developers trying to debug problems.
  id: xccdf_org.ssgproject.content_rule_disable_users_coredumps
  instructions: |-
    To verify that core dumps are disabled for all users, run the following command:
    $ grep core /etc/security/limits.conf
    The output should be:
    *     hard   core    0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: disable-users-coredumps
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-disable-users-coredumps
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40799"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-disable-users-coredumps
    uid: 080a0262-737b-4f7b-9d31-3d21e7b8900d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable FIPS Mode
    Use of weak or untested encryption algorithms undermines the purposes of utilizing encryption to
    protect data. The operating system must implement cryptographic modules adhering to the higher
    standards approved by the federal government since this provides assurance they have been tested
    and validated.
  id: xccdf_org.ssgproject.content_rule_enable_fips_mode
  instructions: |-
    To verify that FIPS is enabled properly, run the following command:
    fips-mode-setup --check
    The output should contain the following:
    FIPS mode is enabled.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: enable-fips-mode
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-enable-fips-mode
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40302"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-enable-fips-mode
    uid: f25950a4-d3b8-4b1a-b609-49b5e4f7a2b2
  severity: high
  status: FAIL
  warnings:
  - The system needs to be rebooted for these changes to take effect.
  - System Crypto Modules must be provided by a vendor that undergoes FIPS-140 certifications. FIPS-140 is applicable to all Federal agencies that use cryptographic-based security systems to protect sensitive information in computer and telecommunication systems (including voice systems) as defined in Section 5131 of the Information Technology Management Reform Act of 1996, Public Law 104-106. This standard shall be used in designing and implementing cryptographic modules that Federal departments and agencies operate or are operated for them under contract. See *https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.140-2.pdf* To meet this, the system has to have cryptographic software provided by a vendor that has undergone this certification. This means providing documentation, test results, design information, and independent third party review by an accredited lab. While open source software is capable of meeting this, it does not meet FIPS-140 unless the vendor submits to this process.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Logrotate Runs Periodically
    Log files that are not properly rotated run the risk of growing so large
    that they fill up the /var/log partition. Valuable logging information could be lost
    if the /var/log partition becomes full.
  id: xccdf_org.ssgproject.content_rule_ensure_logrotate_activated
  instructions: |-
    To determine the status and frequency of logrotate, run the following command:
    $ sudo grep logrotate /var/log/cron*
    If logrotate is configured properly, output should include references to
    /etc/cron.daily.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ensure-logrotate-activated
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-ensure-logrotate-activated
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40105"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-ensure-logrotate-activated
    uid: e09f21a4-1446-4d48-80bb-6af6a11351a9
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns SSH Server config file
    Service configuration files enable or disable features of their respective
    services that if configured incorrectly can lead to insecure and vulnerable
    configurations. Therefore, service configuration files should be owned by the
    correct group to prevent unauthorized changes.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_sshd_config
  instructions: |-
    To check the group ownership of /etc/ssh/sshd_config,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/ssh/sshd_config
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-sshd-config
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-file-groupowner-sshd-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40111"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-groupowner-sshd-config
    uid: 8e6589e5-576a-4b41-bf24-8a781b3b421e
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Owner on SSH Server config file
    Service configuration files enable or disable features of their respective
    services that if configured incorrectly can lead to insecure and vulnerable
    configurations. Therefore, service configuration files should be owned by the
    correct group to prevent unauthorized changes.
  id: xccdf_org.ssgproject.content_rule_file_owner_sshd_config
  instructions: |-
    To check the ownership of /etc/ssh/sshd_config,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/ssh/sshd_config
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-sshd-config
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-file-owner-sshd-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41028"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-owner-sshd-config
    uid: 5205ff2d-b8ed-48d1-bfbb-106e18990ae3
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    System Audit Logs Must Be Owned By Root
    Unauthorized disclosure of audit records can reveal system and configuration data to
    attackers, thus compromising its confidentiality.
  id: xccdf_org.ssgproject.content_rule_file_ownership_var_log_audit
  instructions: "To properly set the owner of /var/log/audit, run the command:\n$ sudo chown root /var/log/audit \n\nTo properly set the owner of /var/log/audit/*, run the command:\n$ sudo chown root /var/log/audit/*"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-ownership-var-log-audit
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-file-ownership-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40212"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-ownership-var-log-audit
    uid: a92740c1-f23f-4623-8fe4-314e19cf6998
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on SSH Server config file
    Service configuration files enable or disable features of their respective
    services that if configured incorrectly can lead to insecure and vulnerable
    configurations. Therefore, service configuration files should be owned by the
    correct group to prevent unauthorized changes.
  id: xccdf_org.ssgproject.content_rule_file_permissions_sshd_config
  instructions: |-
    To check the permissions of /etc/ssh/sshd_config,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/ssh/sshd_config
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-sshd-config
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-file-permissions-sshd-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39985"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-permissions-sshd-config
    uid: 437b8c06-0cb0-4701-a0a1-4305fce6b8c9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on SSH Server Private *_key Key Files
    If an unauthorized user obtains the private SSH host key file, the host could be
    impersonated.
  id: xccdf_org.ssgproject.content_rule_file_permissions_sshd_private_key
  instructions: |-
    To check the permissions of /etc/ssh/*_key,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/ssh/*_key
    If properly configured, the output should indicate the following permissions:
    -rw-r-----
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-sshd-private-key
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-file-permissions-sshd-private-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41056"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-permissions-sshd-private-key
    uid: 75b81f1a-7825-48c0-9c67-83c2e9b0d770
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on SSH Server Public *.pub Key Files
    If a public host key file is modified by an unauthorized user, the SSH service
    may be compromised.
  id: xccdf_org.ssgproject.content_rule_file_permissions_sshd_pub_key
  instructions: |-
    To check the permissions of /etc/ssh/*.pub,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/ssh/*.pub
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-sshd-pub-key
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-file-permissions-sshd-pub-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40178"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-permissions-sshd-pub-key
    uid: 20a48e7b-c224-4d3e-b21f-38dd987ccd75
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    System Audit Logs Must Have Mode 0640 or Less Permissive
    If users can write to audit logs, audit trails can be modified or destroyed.
  id: xccdf_org.ssgproject.content_rule_file_permissions_var_log_audit
  instructions: |-
    Run the following command to check the mode of the system audit logs:
    $ sudo ls -l /var/log/audit
    Audit logs must be mode 0640 or less permissive.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-var-log-audit
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-file-permissions-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40981"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-file-permissions-var-log-audit
    uid: 61075de0-effb-4815-a76d-e801505bb04e
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable ATM Support
    Disabling ATM protects the system against exploitation of any
    flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_atm_disabled
  instructions: |-
    If the system is configured to prevent the loading of the atm kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r atm /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-atm-disabled
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-master-kernel-module-atm-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41122"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-atm-disabled
    uid: 60be6a50-33ab-4358-ba63-70fb0e1486f0
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Bluetooth Kernel Module
    If Bluetooth functionality must be disabled, preventing the kernel
    from loading the kernel module provides an additional safeguard against its
    activation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_bluetooth_disabled
  instructions: |-
    If the system is configured to prevent the loading of the bluetooth kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r bluetooth /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-bluetooth-disabled
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-kernel-module-bluetooth-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40294"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-bluetooth-disabled
    uid: c260561e-a968-48d6-8edf-e7b2564412a2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable CAN Support
    Disabling CAN protects the system against exploitation of any
    flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_can_disabled
  instructions: |-
    If the system is configured to prevent the loading of the can kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r can /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-can-disabled
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-kernel-module-can-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40074"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-can-disabled
    uid: 9765834d-5602-470e-9d70-0b74faa1b04c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of cramfs
    Removing support for unneeded filesystem types reduces the local attack surface
    of the server.
  id: xccdf_org.ssgproject.content_rule_kernel_module_cramfs_disabled
  instructions: |-
    If the system is configured to prevent the loading of the cramfs kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r cramfs /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-cramfs-disabled
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-kernel-module-cramfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40538"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-cramfs-disabled
    uid: f88a1f1d-9231-422e-b1f9-be4c8bac647b
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable IEEE 1394 (FireWire) Support
    Disabling FireWire protects the system against exploitation of any
    flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_firewire-core_disabled
  instructions: |-
    If the system is configured to prevent the loading of the firewire-core kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r firewire-core /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-firewire-core-disabled
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-master-kernel-module-firewire-core-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40532"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-firewire-core-disabled
    uid: 687c70c4-82b2-4f29-9eb8-b7ae48b80216
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of freevxfs
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_freevxfs_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-freevxfs-disabled
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-kernel-module-freevxfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40319"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-freevxfs-disabled
    uid: ba742b8e-e8c5-4a8c-9d0b-9c3794f666ef
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of hfs
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_hfs_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-hfs-disabled
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-kernel-module-hfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40009"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-hfs-disabled
    uid: 89f6ad29-2741-41f5-9476-a5009fb1b54e
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of hfsplus
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_hfsplus_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-hfsplus-disabled
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-master-kernel-module-hfsplus-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41110"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-hfsplus-disabled
    uid: 241e9a40-9e75-46e5-8a39-fb6445cc8ed3
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of jffs2
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_jffs2_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-jffs2-disabled
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-kernel-module-jffs2-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40459"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-jffs2-disabled
    uid: 2189ae20-0c77-4039-b1aa-4426c5ca6759
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable SCTP Support
    Disabling SCTP protects
    the system against exploitation of any flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_sctp_disabled
  instructions: |-
    If the system is configured to prevent the loading of the sctp kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r sctp /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-sctp-disabled
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-kernel-module-sctp-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40004"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-sctp-disabled
    uid: 18cb06aa-3f46-4dde-983e-de687554e44c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of squashfs
    Removing support for unneeded filesystem types reduces the local attack
    surface of the system.
  id: xccdf_org.ssgproject.content_rule_kernel_module_squashfs_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-squashfs-disabled
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-kernel-module-squashfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40904"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-squashfs-disabled
    uid: 5f945f9d-b318-4247-99b5-9cea4d30e36b
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable TIPC Support
    Disabling TIPC protects
    the system against exploitation of any flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_tipc_disabled
  instructions: |-
    If the system is configured to prevent the loading of the tipc kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r tipc /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-tipc-disabled
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-kernel-module-tipc-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40207"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-tipc-disabled
    uid: d80aca92-154e-4540-b16f-bf00cac75eb5
  severity: medium
  status: FAIL
  warnings:
  - This configuration baseline was created to deploy the base operating system for general purpose workloads. When the operating system is configured for certain purposes, such as a node in High Performance Computing cluster, it is expected that the tipc kernel module will be loaded.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of udf
    Removing support for unneeded filesystem types reduces the local
    attack surface of the system.
  id: xccdf_org.ssgproject.content_rule_kernel_module_udf_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-udf-disabled
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-kernel-module-udf-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40174"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-udf-disabled
    uid: e97491d5-0853-425c-ade6-a868f028d3a7
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Modprobe Loading of USB Storage Driver
    USB storage devices such as thumb drives can be used to introduce
    malicious software.
  id: xccdf_org.ssgproject.content_rule_kernel_module_usb-storage_disabled
  instructions: |-
    If the system is configured to prevent the loading of the usb-storage kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r usb-storage /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-usb-storage-disabled
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-kernel-module-usb-storage-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40707"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-kernel-module-usb-storage-disabled
    uid: 10c8305f-6175-4a9b-be26-d341394dbf89
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Direct root Logins Not Allowed
    Disabling direct root logins ensures proper accountability and multifactor
    authentication to privileged accounts. Users will first login, then escalate
    to privileged (root) access via su / sudo. This is required for FISMA Low
    and FISMA Moderate systems.
  id: xccdf_org.ssgproject.content_rule_no_direct_root_logins
  instructions: |-
    To ensure root may not directly login to the system over physical consoles,
    run the following command:
    cat /etc/securetty
    If any output is returned, this is a finding.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-direct-root-logins
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-no-direct-root-logins
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40969"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-no-direct-root-logins
    uid: 019b11e7-94b3-4f9a-84fb-1192d26d0a54
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Prevent Login to Accounts With Empty Password
    If an account has an empty password, anyone could log in and
    run commands with the privileges of that account. Accounts with
    empty passwords should never be used in operational environments.
  id: xccdf_org.ssgproject.content_rule_no_empty_passwords
  instructions: |-
    To verify that null passwords cannot be used, run the following command:

    $ grep nullok /etc/pam.d/system-auth

    If this produces any output, it may be possible to log into accounts
    with empty passwords. Remove any instances of the nullok option to
    prevent logins with empty passwords.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-empty-passwords
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-no-empty-passwords
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40050"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-no-empty-passwords
    uid: 69ac036d-7052-4665-b6d8-f1358dce7b4d
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Verify No netrc Files Exist\nUnencrypted passwords for remote FTP servers may be stored in .netrc\nfiles. "
  id: xccdf_org.ssgproject.content_rule_no_netrc_files
  instructions: |-
    To check the system for the existence of any .netrc files,
    run the following command:
    $ sudo find /home -xdev -name .netrc
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-netrc-files
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-no-netrc-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40486"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-no-netrc-files
    uid: 46dddf86-0495-47c9-8674-bffce86eb2fa
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that System Accounts Do Not Run a Shell Upon Login
    Ensuring shells are not given to system accounts upon login makes it more
    difficult for attackers to make use of system accounts.
  id: xccdf_org.ssgproject.content_rule_no_shelllogin_for_systemaccounts
  instructions: |-
    To obtain a listing of all users, their UIDs, and their shells, run the
    command: $ awk -F: '{print $1 ":" $3 ":" $7}' /etc/passwd Identify
    the system accounts from this listing. These will primarily be the accounts
    with UID numbers less than UID_MIN, other than root. Value of the UID_MIN
    directive is set in /etc/login.defs configuration file. In the default
    configuration UID_MIN is set to 1000.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-shelllogin-for-systemaccounts
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-no-shelllogin-for-systemaccounts
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40470"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-no-shelllogin-for-systemaccounts
    uid: a1f4438d-a409-47e0-9f94-4651a077bb25
  severity: medium
  status: PASS
  warnings:
  - Do not perform the steps in this section on the root account. Doing so might cause the system to become inaccessible.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Prevent user from disabling the screen lock
    Not listing tmux among permitted shells
    prevents malicious program running as user
    from lowering security by disabling the screen lock.
  id: xccdf_org.ssgproject.content_rule_no_tmux_in_shells
  instructions: |-
    To verify that tmux is not listed as allowed shell on the system
    run the following command:
    $ grep 'tmux$' /etc/shells
    The output should be empty.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-tmux-in-shells
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-no-tmux-in-shells
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40637"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-no-tmux-in-shells
    uid: 5e1d2d06-d891-4e2d-9e68-2918ba48674f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure the audit Subsystem is Installed
    The auditd service is an access monitoring and accounting daemon, watching system calls to audit any access, in comparison with potential local access control policy such as SELinux policy.
  id: xccdf_org.ssgproject.content_rule_package_audit_installed
  instructions: Is it the case that the package is not installed?
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-audit-installed
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-package-audit-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39976"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-package-audit-installed
    uid: d26872dc-96e0-4501-a5a1-c68f80ab6e47
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Install iptables Package
    iptables controls the Linux kernel network packet filtering
    code. iptables allows system operators to set up firewalls and IP
    masquerading, etc.
  id: xccdf_org.ssgproject.content_rule_package_iptables_installed
  instructions: 'Run the following command to determine if the iptables package is installed: $ rpm -q iptables'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-iptables-installed
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-package-iptables-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40795"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-package-iptables-installed
    uid: 785ab3f2-5fab-4b6e-a7c3-674d7420d448
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Install sudo Package
    sudo is a program designed to allow a system administrator to give
    limited root privileges to users and log root activity. The basic philosophy
    is to give as few privileges as possible but still allow system users to
    get their work done.
  id: xccdf_org.ssgproject.content_rule_package_sudo_installed
  instructions: 'Run the following command to determine if the sudo package is installed: $ rpm -q sudo'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-sudo-installed
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-master-package-sudo-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40482"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-package-sudo-installed
    uid: 482eeaa7-2fd3-4262-9db8-b2e292d1cb26
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Install usbguard Package
    usbguard is a software framework that helps to protect
    against rogue USB devices by implementing basic whitelisting/blacklisting
    capabilities based on USB device attributes.
  id: xccdf_org.ssgproject.content_rule_package_usbguard_installed
  instructions: 'Run the following command to determine if the usbguard package is installed: $ rpm -q usbguard'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-usbguard-installed
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-package-usbguard-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40743"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-package-usbguard-installed
    uid: 24a56f81-7f6d-4115-bb21-20ba09a2f877
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Require Authentication for Single User Mode
    This prevents attackers with physical access from trivially bypassing security
    on the machine and gaining root access. Such accesses are further prevented
    by configuring the bootloader password.
  id: xccdf_org.ssgproject.content_rule_require_singleuser_auth
  instructions: |-
    To check if authentication is required for single-user mode, run the following command:
    $ grep sulogin /usr/lib/systemd/system/rescue.service
    The output should be similar to the following, and the line must begin with
    ExecStart and /usr/lib/systemd/systemd-sulogin-shell.
        ExecStart=-/usr/lib/systemd/systemd-sulogin-shell rescue
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: require-singleuser-auth
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-require-singleuser-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40810"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-require-singleuser-auth
    uid: 4a02c24f-bbc5-478d-8250-888d946243db
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure No Daemons are Unconfined by SELinux
    Daemons which run with the unconfined_service_t context may cause AVC denials,
    or allow privileges that the daemon does not require.
  id: xccdf_org.ssgproject.content_rule_selinux_confinement_of_daemons
  instructions: |-
    Ensure there are no unconfined daemons running on the system,
    the following command should produce no output:
    $ sudo ps -eZ | grep "unconfined_service_t"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: selinux-confinement-of-daemons
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-selinux-confinement-of-daemons
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40697"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-selinux-confinement-of-daemons
    uid: 603d9437-20d6-45f0-8e30-467da02b752d
  severity: medium
  status: FAIL
  warnings:
  - Automatic remediation of this control is not available. Remediation can be achieved by amending SELinux policy or stopping the unconfined daemons as outlined above.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure SELinux Policy
    Setting the SELinux policy to targeted or a more specialized policy
    ensures the system will confine processes that are likely to be
    targeted for exploitation, such as network or system services.

    Note: During the development or debugging of SELinux modules, it is common to
    temporarily place non-production systems in permissive mode. In such
    temporary cases, SELinux policies should be developed, and once work
    is completed, the system should be reconfigured to
    .
  id: xccdf_org.ssgproject.content_rule_selinux_policytype
  instructions: |-
    Check the file /etc/selinux/config and ensure the following line appears:
    SELINUXTYPE=
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: selinux-policytype
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-selinux-policytype
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40090"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-selinux-policytype
    uid: 5b673cb6-5504-4bb1-a1fe-0b58cae8fcde
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure SELinux State is Enforcing
    Setting the SELinux state to enforcing ensures SELinux is able to confine
    potentially compromised processes to the security policy, which is designed to
    prevent them from causing damage to the system or further elevating their
    privileges.
  id: xccdf_org.ssgproject.content_rule_selinux_state
  instructions: |-
    Check the file /etc/selinux/config and ensure the following line appears:
    SELINUX=
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: selinux-state
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-selinux-state
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40014"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-selinux-state
    uid: 1a9feb9a-454f-4717-b6bf-667330a35c70
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable auditd Service
    Without establishing what type of events occurred, it would be difficult
    to establish, correlate, and investigate the events leading up to an outage or attack.
    Ensuring the auditd service is active ensures audit records
    generated by the kernel are appropriately recorded.

    Additionally, a properly configured audit subsystem ensures that actions of
    individual system users can be uniquely traced to those users so they
    can be held accountable for their actions.
  id: xccdf_org.ssgproject.content_rule_service_auditd_enabled
  instructions: |-
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    auditd service:
    $ systemctl is-active auditd
    If the service is running, it should return the following: active
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-auditd-enabled
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-service-auditd-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40323"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-auditd-enabled
    uid: efdd8bac-01a2-48e8-80fd-d9560e0b8d6f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable the Automounter
    Disabling the automounter permits the administrator to
    statically control filesystem mounting through /etc/fstab.

    Additionally, automatically mounting filesystems permits easy introduction of
    unknown devices, thereby facilitating malicious activity.
  id: xccdf_org.ssgproject.content_rule_service_autofs_disabled
  instructions: |-
    To check that the autofs service is disabled in system boot configuration,
    You'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Subsequently,run the following command:
    $ systemctl is-enabled autofs
    Output should indicate the autofs service has either not been installed,
    or has been disabled at all runlevels, as shown in the example below:
    $ systemctl is-enabled autofs disabled

    Run the following command to verify autofs is not active (i.e. not running) through current runtime configuration:
    $ systemctl is-active autofs

    If the service is not running the command will return the following output:
    inactive

    The service will also be masked, to check that the autofs is masked, run the following command:
    $ systemctl show autofs | grep "LoadState\|UnitFileState"

    If the service is masked the command will return the following outputs:

    LoadState=masked

    UnitFileState=masked
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-autofs-disabled
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-service-autofs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40899"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-autofs-disabled
    uid: ba3f54e4-d948-4019-9dfe-92173cf85d21
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Bluetooth Service
    Disabling the bluetooth service prevents the system from attempting
    connections to Bluetooth devices, which entails some security risk.
    Nevertheless, variation in this risk decision may be expected due to the
    utility of Bluetooth connectivity and its limited range.
  id: xccdf_org.ssgproject.content_rule_service_bluetooth_disabled
  instructions: |-
    To check that the bluetooth service is disabled in system boot configuration,
    You'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Subsequently,run the following command:
    $ systemctl is-enabled bluetooth
    Output should indicate the bluetooth service has either not been installed,
    or has been disabled at all runlevels, as shown in the example below:
    $ systemctl is-enabled bluetooth disabled

    Run the following command to verify bluetooth is not active (i.e. not running) through current runtime configuration:
    $ systemctl is-active bluetooth

    If the service is not running the command will return the following output:
    inactive

    The service will also be masked, to check that the bluetooth is masked, run the following command:
    $ systemctl show bluetooth | grep "LoadState\|UnitFileState"

    If the service is masked the command will return the following outputs:

    LoadState=masked

    UnitFileState=masked
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-bluetooth-disabled
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-service-bluetooth-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40103"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-bluetooth-disabled
    uid: f006bded-96c1-4d01-9d89-8d7137608ef3
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the NTP Daemon
    Enabling some of chronyd or ntpd services ensures
    that the NTP daemon will be running and that the system will synchronize its
    time to any servers specified. This is important whether the system is
    configured to be a client (and synchronize only its own clock) or it is also
    acting as an NTP server to other systems.  Synchronizing time is essential for
    authentication services such as Kerberos, but it is also important for
    maintaining accurate logs and auditing possible security breaches.

    The chronyd and ntpd NTP daemons offer all of the
    functionality of ntpdate, which is now deprecated. Additional
    information on this is available at

        http://support.ntp.org/bin/view/Dev/DeprecatingNtpdate
  id: xccdf_org.ssgproject.content_rule_service_chronyd_or_ntpd_enabled
  instructions: |-
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    chronyd service:
    $ systemctl is-active chronyd
    If the service is running, it should return the following: active

    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    ntpd service:
    $ systemctl is-active ntpd
    If the service is running, it should return the following: active
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-chronyd-or-ntpd-enabled
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-service-chronyd-or-ntpd-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40182"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-chronyd-or-ntpd-enabled
    uid: ba525036-f6a6-4f01-b2be-f721032f63cd
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable debug-shell SystemD Service
    This prevents attackers with physical access from trivially bypassing security
    on the machine through valid troubleshooting configurations and gaining root
    access when the system is rebooted.
  id: xccdf_org.ssgproject.content_rule_service_debug-shell_disabled
  instructions: |-
    To check that the debug-shell service is disabled in system boot configuration,
    You'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Subsequently,run the following command:
    $ systemctl is-enabled debug-shell
    Output should indicate the debug-shell service has either not been installed,
    or has been disabled at all runlevels, as shown in the example below:
    $ systemctl is-enabled debug-shell disabled

    Run the following command to verify debug-shell is not active (i.e. not running) through current runtime configuration:
    $ systemctl is-active debug-shell

    If the service is not running the command will return the following output:
    inactive

    The service will also be masked, to check that the debug-shell is masked, run the following command:
    $ systemctl show debug-shell | grep "LoadState\|UnitFileState"

    If the service is masked the command will return the following outputs:

    LoadState=masked

    UnitFileState=masked
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-debug-shell-disabled
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-service-debug-shell-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40128"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-debug-shell-disabled
    uid: 7affb955-6617-47ef-8cc0-c9e8f5d9730f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable acquiring, saving, and processing core dumps
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data
    and is generally useful only for developers trying to debug problems.
  id: xccdf_org.ssgproject.content_rule_service_systemd-coredump_disabled
  instructions: |-
    To verify that acquiring, saving, and processing core dumps is disabled, run the
    following command:
    $ systemctl status systemd-coredump.socket
    The output should be similar to:
    â— systemd-coredump.socket
       Loaded: masked (Reason: Unit systemd-coredump.socket is masked.)
       Active: inactive (dead) ...
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-systemd-coredump-disabled
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-service-systemd-coredump-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39995"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-systemd-coredump-disabled
    uid: 3c02c4cd-22aa-426d-bf48-171f1fac7e06
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the USBGuard Service
    The usbguard service must be running in order to
    enforce the USB device authorization policy for all USB devices.
  id: xccdf_org.ssgproject.content_rule_service_usbguard_enabled
  instructions: |-
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    usbguard service:
    $ systemctl is-active usbguard
    If the service is running, it should return the following: active
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-usbguard-enabled
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-master-service-usbguard-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40432"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-service-usbguard-enabled
    uid: c5045233-b6d0-4115-9bfb-8ea8ad00118f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable SSH Support for .rhosts Files
    SSH trust relationships mean a compromise on one host
    can allow an attacker to move trivially to other hosts.
  id: xccdf_org.ssgproject.content_rule_sshd_disable_rhosts
  instructions: |-
    To determine how the SSH daemon's IgnoreRhosts option is set, run the following command:
    $ sudo grep -i IgnoreRhosts /etc/ssh/sshd_config
    If no line, a commented line, or a line indicating the value yes is returned, then the required value is set.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-disable-rhosts
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-sshd-disable-rhosts
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40587"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sshd-disable-rhosts
    uid: c8f49db6-3d42-4ba6-b015-61f84101c931
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Users' SSH Access
    Specifying which accounts are allowed SSH access into the system reduces the
    possibility of unauthorized access to the system.
  id: xccdf_org.ssgproject.content_rule_sshd_limit_user_access
  instructions: |-
    To ensure sshd limits the users who can log in, run the following:
    $ sudo grep AllowUsers /etc/ssh/sshd_config
    If properly configured, the output should be a list of usernames allowed to log in
    to this system.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-limit-user-access
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-sshd-limit-user-access
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40791"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sshd-limit-user-access
    uid: 193c6512-4fb8-4d5b-bc59-070efd8f5c2b
  severity: unknown
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set SSH Idle Timeout Interval
    Terminating an idle ssh session within a short time period reduces the window of
    opportunity for unauthorized personnel to take control of a management session
    enabled on the console or console port that has been let unattended.
  id: xccdf_org.ssgproject.content_rule_sshd_set_idle_timeout
  instructions: |-
    Run the following command to see what the timeout interval is:
    $ sudo grep ClientAliveInterval /etc/ssh/sshd_config
    If properly configured, the output should be:
    ClientAliveInterval
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-set-idle-timeout
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-sshd-set-idle-timeout
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39974"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sshd-set-idle-timeout
    uid: 323a05f4-9a2c-4351-a677-a131341529b5
  severity: medium
  status: FAIL
  warnings:
  - SSH disconnecting idle clients will not have desired effect without also configuring ClientAliveCountMax in the SSH service configuration.
  - |-
    Following conditions may prevent the SSH session to time out:

    * Remote processes on the remote machine generates output. As the output has to be transferred over the network to the client, the timeout is reset every time such transfer happens.
    * Any scp or sftp activity by the same user to the host resets the timeout.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set SSH Client Alive Count Max to zero
    This ensures a user login will be terminated as soon as the ClientAliveInterval
    is reached.
  id: xccdf_org.ssgproject.content_rule_sshd_set_keepalive_0
  instructions: |-
    To ensure ClientAliveInterval is set correctly, run the following command:
    $ sudo grep ClientAliveCountMax /etc/ssh/sshd_config
    If properly configured, the output should be:
    ClientAliveCountMax 0

    In this case, the SSH idle timeout occurs precisely when
    the ClientAliveInterval is set.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-set-keepalive-0
    creationTimestamp: "2021-07-13T16:58:51Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:51Z"
    name: rhcos4-moderate-master-sshd-set-keepalive-0
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40197"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sshd-set-keepalive-0
    uid: 33e0789c-64c5-4483-b9ff-45da5b75b6f1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Enforce DAC on Hardlinks
    By enabling this kernel parameter, users can no longer create soft or hard links to
    files which they do not own. Disallowing such hardlinks mitigate vulnerabilities
    based on insecure file system accessed by privileged programs, avoiding an
    exploitation vector exploiting unsafe use of open() or creat().
  id: xccdf_org.ssgproject.content_rule_sysctl_fs_protected_hardlinks
  instructions: "The runtime status of the fs.protected_hardlinks kernel parameter can be queried\nby running the following command:\n$ sysctl fs.protected_hardlinks\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*fs.protected_hardlinks\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nfs.protected_hardlinks = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains fs.protected_hardlinks = 1, and that one assignment\nis returned when \n$ grep -r fs.protected_hardlinks /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-fs-protected-hardlinks
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-sysctl-fs-protected-hardlinks
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41008"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-fs-protected-hardlinks
    uid: f802a266-5d7e-4e68-bc55-89972ca9f4df
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Enforce DAC on Symlinks
    By enabling this kernel parameter, symbolic links are permitted to be followed
    only when outside a sticky world-writable directory, or when the UID of the
    link and follower match, or when the directory owner matches the symlink's owner.
    Disallowing such symlinks helps mitigate vulnerabilities based on insecure file system
    accessed by privileged programs, avoiding an exploitation vector exploiting unsafe use of
    open() or creat().
  id: xccdf_org.ssgproject.content_rule_sysctl_fs_protected_symlinks
  instructions: "The runtime status of the fs.protected_symlinks kernel parameter can be queried\nby running the following command:\n$ sysctl fs.protected_symlinks\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*fs.protected_symlinks\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nfs.protected_symlinks = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains fs.protected_symlinks = 1, and that one assignment\nis returned when \n$ grep -r fs.protected_symlinks /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-fs-protected-symlinks
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-master-sysctl-fs-protected-symlinks
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40919"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-fs-protected-symlinks
    uid: 794e6a7f-03b2-4cf2-8e6f-6015821d6f56
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable storing core dumps
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data and is generally useful
    only for developers trying to debug problems.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_core_pattern
  instructions: "The runtime status of the kernel.core_pattern kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.core_pattern\nThe output of the command should indicate a value of |/bin/false.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.core_pattern\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.core_pattern = |/bin/false\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.core_pattern = |/bin/false, and that one assignment\nis returned when \n$ grep -r kernel.core_pattern /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-core-pattern
    creationTimestamp: "2021-07-13T16:58:49Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:49Z"
    name: rhcos4-moderate-master-sysctl-kernel-core-pattern
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40110"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-core-pattern
    uid: 0a75e1fb-6fd5-40a3-9b46-1f974ccf8e62
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Restrict Access to Kernel Message Buffer
    Unprivileged access to the kernel syslog can expose sensitive kernel
    address information.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_dmesg_restrict
  instructions: "The runtime status of the kernel.dmesg_restrict kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.dmesg_restrict\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.dmesg_restrict\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.dmesg_restrict = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.dmesg_restrict = 1, and that one assignment\nis returned when \n$ grep -r kernel.dmesg_restrict /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-dmesg-restrict
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-sysctl-kernel-dmesg-restrict
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40754"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-dmesg-restrict
    uid: 673237c2-1e6f-4543-9f70-71a30e18ed5c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |
    Disable Kernel Image Loading
    Disabling kexec_load allows greater control of the kernel memory.
    It makes it impossible to load another kernel image after it has been disabled.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_kexec_load_disabled
  instructions: "The runtime status of the kernel.kexec_load_disabled kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.kexec_load_disabled\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.kexec_load_disabled\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.kexec_load_disabled = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.kexec_load_disabled = 1, and that one assignment\nis returned when \n$ grep -r kernel.kexec_load_disabled /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-kexec-load-disabled
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-sysctl-kernel-kexec-load-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39981"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-kexec-load-disabled
    uid: 28a2c649-e86c-4acb-8e57-d5c7e6f1b28a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Restrict Exposed Kernel Pointer Addresses Access
    Exposing kernel pointers (through procfs or seq_printf()) exposes
    kernel writeable structures that can contain functions pointers. If a write vulnereability occurs
    in the kernel allowing a write access to any of this structure, the kernel can be compromise. This
    option disallow any program withtout the CAP_SYSLOG capability from getting the kernel pointers addresses,
    replacing them with 0.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_kptr_restrict
  instructions: "The runtime status of the kernel.kptr_restrict kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.kptr_restrict\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.kptr_restrict\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.kptr_restrict = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.kptr_restrict = 1, and that one assignment\nis returned when \n$ grep -r kernel.kptr_restrict /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-kptr-restrict
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-sysctl-kernel-kptr-restrict
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40354"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-kptr-restrict
    uid: 0739b32a-28db-48c8-ab13-c250cebd5091
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disallow kernel profiling by unprivileged users
    Kernel profiling can reveal sensitive information about kernel behaviour.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_perf_event_paranoid
  instructions: "The runtime status of the kernel.perf_event_paranoid kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.perf_event_paranoid\nThe output of the command should indicate a value of 2.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.perf_event_paranoid\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.perf_event_paranoid = 2\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.perf_event_paranoid = 2, and that one assignment\nis returned when \n$ grep -r kernel.perf_event_paranoid /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-perf-event-paranoid
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-sysctl-kernel-perf-event-paranoid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40299"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-perf-event-paranoid
    uid: 5027d61e-5a63-49eb-9c02-7b507798467c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Access to Network bpf() Syscall From Unprivileged Processes
    Loading and accessing the packet filters programs and maps using the bpf()
    syscall has the potential of revealing sensitive information about the kernel state.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_unprivileged_bpf_disabled
  instructions: "The runtime status of the kernel.unprivileged_bpf_disabled kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.unprivileged_bpf_disabled\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.unprivileged_bpf_disabled\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.unprivileged_bpf_disabled = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.unprivileged_bpf_disabled = 1, and that one assignment\nis returned when \n$ grep -r kernel.unprivileged_bpf_disabled /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-unprivileged-bpf-disabled
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-master-sysctl-kernel-unprivileged-bpf-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40669"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-unprivileged-bpf-disabled
    uid: cc9cb9da-31ee-4582-8787-032eb4ba298d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |
    Restrict usage of ptrace to descendant processes
    Unrestricted usage of ptrace allows compromised binaries to run ptrace
    on another processes of the user. Like this, the attacker can steal
    sensitive information from the target processes (e.g. SSH sessions, web browser, ...)
    without any additional assistance from the user (i.e. without resorting to phishing).
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_yama_ptrace_scope
  instructions: "The runtime status of the kernel.yama.ptrace_scope kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.yama.ptrace_scope\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.yama.ptrace_scope\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.yama.ptrace_scope = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.yama.ptrace_scope = 1, and that one assignment\nis returned when \n$ grep -r kernel.yama.ptrace_scope /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-yama-ptrace-scope
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-sysctl-kernel-yama-ptrace-scope
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40332"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-kernel-yama-ptrace-scope
    uid: f87c3e13-d8f5-43c5-82bb-ad467da3bb9c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Harden the operation of the BPF just-in-time compiler
    When hardened, the extended Berkeley Packet Filter just-in-time compiler
    will randomize any kernel addresses in the BPF programs and maps,
    and will not expose the JIT addresses in /proc/kallsyms.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_core_bpf_jit_harden
  instructions: "The runtime status of the net.core.bpf_jit_harden kernel parameter can be queried\nby running the following command:\n$ sysctl net.core.bpf_jit_harden\nThe output of the command should indicate a value of 2.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.core.bpf_jit_harden\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.core.bpf_jit_harden = 2\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.core.bpf_jit_harden = 2, and that one assignment\nis returned when \n$ grep -r net.core.bpf_jit_harden /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-core-bpf-jit-harden
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-sysctl-net-core-bpf-jit-harden
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40735"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-core-bpf-jit-harden
    uid: d2e73133-b3ae-4fec-816b-831104f71362
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Accepting ICMP Redirects for All IPv4 Interfaces
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages modify the
    host's route table and are unauthenticated. An illicit ICMP redirect
    message could result in a man-in-the-middle attack.

    This feature of the IPv4 protocol has few legitimate uses. It should be
    disabled unless absolutely required."
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_accept_redirects
  instructions: "The runtime status of the net.ipv4.conf.all.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-accept-redirects
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-all-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40148"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-all-accept-redirects
    uid: d965bd62-80fd-4361-b33d-7b39c9293e7e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on all IPv4 Interfaces
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router,
    which can be used to bypass network security measures. This requirement
    applies only to the forwarding of source-routerd traffic, such as when IPv4
    forwarding is enabled and the system is functioning as a router.

    Accepting source-routed packets in the IPv4 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_accept_source_route
  instructions: "The runtime status of the net.ipv4.conf.all.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-accept-source-route
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-all-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40749"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-all-accept-source-route
    uid: 722071cb-8de0-47a8-bf0d-6619a87fae3f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Log Martian Packets on all IPv4 Interfaces
    The presence of "martian" packets (which have impossible addresses)
    as well as spoofed packets, source-routed packets, and redirects could be a
    sign of nefarious network activity. Logging these packets enables this activity
    to be detected.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_log_martians
  instructions: "The runtime status of the net.ipv4.conf.all.log_martians kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.log_martians\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.log_martians\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.log_martians = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.log_martians = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.log_martians /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-log-martians
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-all-log-martians
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40582"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-all-log-martians
    uid: f58f28b5-d31e-4327-a09c-e3f878079b62
  severity: unknown
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Use Reverse Path Filtering on all IPv4 Interfaces
    Enabling reverse path filtering drops packets with source addresses
    that should not have been able to be received on the interface they were
    received on. It should not be used on systems which are routers for
    complicated networks, but is helpful for end hosts and routers serving small
    networks.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_rp_filter
  instructions: "The runtime status of the net.ipv4.conf.all.rp_filter kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.rp_filter\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.rp_filter\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.rp_filter = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.rp_filter = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.rp_filter /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-rp-filter
    creationTimestamp: "2021-07-13T16:58:52Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:52Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-all-rp-filter
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40236"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-all-rp-filter
    uid: 2b94dea4-4739-43ae-b1cf-43a571991c10
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Secure ICMP Redirects on all IPv4 Interfaces
    Accepting "secure" ICMP redirects (from those gateways listed as
    default gateways) has few legitimate uses. It should be disabled unless it is
    absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_secure_redirects
  instructions: "The runtime status of the net.ipv4.conf.all.secure_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.secure_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.secure_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.secure_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.secure_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.secure_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-secure-redirects
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-all-secure-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40369"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-all-secure-redirects
    uid: 5b5a0c5d-c453-4c60-9d2a-181e7ac420be
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Sending ICMP Redirects on all IPv4 Interfaces
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages contain information
    from the system's route table possibly revealing portions of the network topology.

    The ability to send ICMP redirects is only appropriate for systems acting as routers.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_send_redirects
  instructions: "The runtime status of the net.ipv4.conf.all.send_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.send_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.send_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.send_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.send_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.send_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-send-redirects
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-all-send-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40951"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-all-send-redirects
    uid: e6ddb0b2-2fbd-4978-aefb-be5637116f97
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting ICMP Redirects by Default on IPv4 Interfaces
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages modify the
    host's route table and are unauthenticated. An illicit ICMP redirect
    message could result in a man-in-the-middle attack.
    This feature of the IPv4 protocol has few legitimate uses. It should
    be disabled unless absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_accept_redirects
  instructions: "The runtime status of the net.ipv4.conf.default.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-accept-redirects
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-default-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40312"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-default-accept-redirects
    uid: bf246cff-d338-4609-9bba-e0e19d487d4a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on IPv4 Interfaces by Default
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router,
    which can be used to bypass network security measures.

    Accepting source-routed packets in the IPv4 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required, such as when
    IPv4 forwarding is enabled and the system is legitimately functioning as a
    router.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_accept_source_route
  instructions: "The runtime status of the net.ipv4.conf.default.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-accept-source-route
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-default-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41016"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-default-accept-source-route
    uid: 3d7362bc-64ce-4df0-b906-de87fd9e8a12
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Paremeter to Log Martian Packets on all IPv4 Interfaces by Default
    The presence of "martian" packets (which have impossible addresses)
    as well as spoofed packets, source-routed packets, and redirects could be a
    sign of nefarious network activity. Logging these packets enables this activity
    to be detected.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_log_martians
  instructions: "The runtime status of the net.ipv4.conf.default.log_martians kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.log_martians\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.log_martians\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.log_martians = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.log_martians = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.log_martians /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-log-martians
    creationTimestamp: "2021-07-13T16:58:53Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:53Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-default-log-martians
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40282"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-default-log-martians
    uid: 9ea1f3c6-b810-4626-a044-fa95f4808e8a
  severity: unknown
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Use Reverse Path Filtering on all IPv4 Interfaces by Default
    Enabling reverse path filtering drops packets with source addresses
    that should not have been able to be received on the interface they were
    received on. It should not be used on systems which are routers for
    complicated networks, but is helpful for end hosts and routers serving small
    networks.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_rp_filter
  instructions: "The runtime status of the net.ipv4.conf.default.rp_filter kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.rp_filter\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.rp_filter\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.rp_filter = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.rp_filter = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.rp_filter /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-rp-filter
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-default-rp-filter
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40039"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-default-rp-filter
    uid: 290de827-c999-4a08-b647-2915014adb0b
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Kernel Parameter for Accepting Secure Redirects By Default
    Accepting "secure" ICMP redirects (from those gateways listed as
    default gateways) has few legitimate uses. It should be disabled unless it is
    absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_secure_redirects
  instructions: "The runtime status of the net.ipv4.conf.default.secure_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.secure_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.secure_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.secure_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.secure_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.secure_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-secure-redirects
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-default-secure-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41002"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-default-secure-redirects
    uid: 2fe2b176-979e-4373-a998-4b16b341d79d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Sending ICMP Redirects on all IPv4 Interfaces by Default
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages contain information
    from the system's route table possibly revealing portions of the network topology.

    The ability to send ICMP redirects is only appropriate for systems acting as routers.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_send_redirects
  instructions: "The runtime status of the net.ipv4.conf.default.send_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.send_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.send_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.send_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.send_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.send_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-send-redirects
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-conf-default-send-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40315"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-conf-default-send-redirects
    uid: 223d0119-dba7-48d5-bd36-4899fb6c0bee
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Ignore ICMP Broadcast Echo Requests on IPv4 Interfaces
    Responding to broadcast (ICMP) echoes facilitates network mapping
    and provides a vector for amplification attacks.

    Ignoring ICMP echo requests (pings) sent to broadcast or multicast
    addresses makes the system slightly more difficult to enumerate on the network.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_icmp_echo_ignore_broadcasts
  instructions: "The runtime status of the net.ipv4.icmp_echo_ignore_broadcasts kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.icmp_echo_ignore_broadcasts\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.icmp_echo_ignore_broadcasts\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.icmp_echo_ignore_broadcasts = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.icmp_echo_ignore_broadcasts /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-icmp-echo-ignore-broadcasts
    creationTimestamp: "2021-07-13T16:58:50Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:50Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-icmp-echo-ignore-broadcasts
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40170"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-icmp-echo-ignore-broadcasts
    uid: e9aa0b9a-a99b-4e90-ae0b-72234cba987c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Ignore Bogus ICMP Error Responses on IPv4 Interfaces
    Ignoring bogus ICMP error responses reduces
    log size, although some activity would not be logged.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_icmp_ignore_bogus_error_responses
  instructions: "The runtime status of the net.ipv4.icmp_ignore_bogus_error_responses kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.icmp_ignore_bogus_error_responses\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.icmp_ignore_bogus_error_responses\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.icmp_ignore_bogus_error_responses = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.icmp_ignore_bogus_error_responses /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-icmp-ignore-bogus-error-responses
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-icmp-ignore-bogus-error-responses
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41050"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-icmp-ignore-bogus-error-responses
    uid: 2395cee8-1487-44d8-9d21-33209fb42bc7
  severity: unknown
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Use TCP Syncookies on IPv4 Interfaces
    A TCP SYN flood attack can cause a denial of service by filling a
    system's TCP connection table with connections in the SYN_RCVD state.
    Syncookies can be used to track a connection when a subsequent ACK is received,
    verifying the initiator is attempting a valid connection and is not a flood
    source. This feature is activated when a flood condition is detected, and
    enables the system to continue servicing valid connection requests.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_tcp_syncookies
  instructions: "The runtime status of the net.ipv4.tcp_syncookies kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.tcp_syncookies\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.tcp_syncookies\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.tcp_syncookies = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.tcp_syncookies = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.tcp_syncookies /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-tcp-syncookies
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-sysctl-net-ipv4-tcp-syncookies
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39972"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv4-tcp-syncookies
    uid: 64abe338-8f93-4934-bb00-451f19933fb7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Accepting Router Advertisements on All IPv6 Interfaces
    An illicit router advertisement message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_all_accept_ra
  instructions: "The runtime status of the net.ipv6.conf.all.accept_ra kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.all.accept_ra\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.all.accept_ra\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.all.accept_ra = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.all.accept_ra = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.all.accept_ra /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-all-accept-ra
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-sysctl-net-ipv6-conf-all-accept-ra
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40099"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv6-conf-all-accept-ra
    uid: 8dad9528-e808-4792-aafe-0dfacb20c9a1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Accepting ICMP Redirects for All IPv6 Interfaces
    An illicit ICMP redirect message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_all_accept_redirects
  instructions: "The runtime status of the net.ipv6.conf.all.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.all.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.all.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.all.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.all.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.all.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-all-accept-redirects
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-sysctl-net-ipv6-conf-all-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40001"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv6-conf-all-accept-redirects
    uid: 18600f86-49a2-4e55-b107-541407205780
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on all IPv6 Interfaces
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router, which can
    be used to bypass network security measures. This requirement applies only to the
    forwarding of source-routerd traffic, such as when IPv6 forwarding is enabled and
    the system is functioning as a router.

    Accepting source-routed packets in the IPv6 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_all_accept_source_route
  instructions: "The runtime status of the net.ipv6.conf.all.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.all.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.all.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.all.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.all.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.all.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-all-accept-source-route
    creationTimestamp: "2021-07-13T16:58:47Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:47Z"
    name: rhcos4-moderate-master-sysctl-net-ipv6-conf-all-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40045"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv6-conf-all-accept-source-route
    uid: 616228a6-a5e2-42f1-836e-01b758512488
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Accepting Router Advertisements on all IPv6 Interfaces by Default
    An illicit router advertisement message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_default_accept_ra
  instructions: "The runtime status of the net.ipv6.conf.default.accept_ra kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.default.accept_ra\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.default.accept_ra\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.default.accept_ra = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.default.accept_ra = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.default.accept_ra /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-default-accept-ra
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-master-sysctl-net-ipv6-conf-default-accept-ra
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40860"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv6-conf-default-accept-ra
    uid: 7d19cbee-9ce2-4192-bb11-c168a0affeaa
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting ICMP Redirects by Default on IPv6 Interfaces
    An illicit ICMP redirect message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_default_accept_redirects
  instructions: "The runtime status of the net.ipv6.conf.default.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.default.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.default.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.default.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.default.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.default.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-default-accept-redirects
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-master-sysctl-net-ipv6-conf-default-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40780"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv6-conf-default-accept-redirects
    uid: 67bf7bf0-4dd7-4922-916a-af383b03f0aa
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on IPv6 Interfaces by Default
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router, which can
    be used to bypass network security measures. This requirement applies only to the
    forwarding of source-routerd traffic, such as when IPv6 forwarding is enabled and
    the system is functioning as a router.

    Accepting source-routed packets in the IPv6 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_default_accept_source_route
  instructions: "The runtime status of the net.ipv6.conf.default.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.default.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.default.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.default.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.default.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.default.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-default-accept-source-route
    creationTimestamp: "2021-07-13T16:58:46Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:46Z"
    name: rhcos4-moderate-master-sysctl-net-ipv6-conf-default-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "39977"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-sysctl-net-ipv6-conf-default-accept-source-route
    uid: 65147cb7-8c78-42ca-9ff4-e956b716ac18
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Authorize Human Interface Devices and USB hubs in USBGuard daemon
    Without allowing Human Interface Devices, it might not be possible
    to interact with the system. Without allowing hubs, it might not be possible to use any
    USB devices on the system.
  id: xccdf_org.ssgproject.content_rule_usbguard_allow_hid_and_hub
  instructions: |-
    To verify that USB Human Interface Devices and hubs will be authorized by the USBGuard daemon,
    run the following command:
    $ sudo grep allow /etc/usbguard/rules.conf
    The output lines should include
    allow with-interface match-all { 03:*:* 09:00:* }
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: usbguard-allow-hid-and-hub
    creationTimestamp: "2021-07-13T16:58:48Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:48Z"
    name: rhcos4-moderate-master-usbguard-allow-hid-and-hub
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40093"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-usbguard-allow-hid-and-hub
    uid: 78c2b2ef-2d97-4918-ad84-ba8ca43e0d02
  severity: medium
  status: FAIL
  warnings:
  - This rule should be understood primarily as a convenience administration feature. This rule ensures that if the USBGuard default rules.conf file is present, it will alter it so that USB human interface devices and hubs are allowed. However, if the rules.conf file is altered by system administrator, the rule does not check if USB human interface devices and hubs are allowed. This assumes that an administrator modified the file with some purpose in mind.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable WiFi or Bluetooth in BIOS
    Disabling wireless support in the BIOS prevents easy
    activation of the wireless interface, generally requiring administrators
    to reboot the system first.
  id: xccdf_org.ssgproject.content_rule_wireless_disable_in_bios
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: wireless-disable-in-bios
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-master-wireless-disable-in-bios
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "41022"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-wireless-disable-in-bios
    uid: f704db9a-cb59-4e25-a205-55d1a5707474
  severity: unknown
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Deactivate Wireless Network Interfaces
    The use of wireless networking can introduce many different attack vectors into
    the organization's network. Common attack vectors such as malicious association
    and ad hoc networks will allow an attacker to spoof a wireless access point
    (AP), allowing validated systems to connect to the malicious AP and enabling the
    attacker to monitor and record network traffic. These malicious APs can also
    serve to create a man-in-the-middle attack or be used to create a denial of
    service to valid network resources.
  id: xccdf_org.ssgproject.content_rule_wireless_disable_interfaces
  instructions: |-
    Verify that there are no wireless interfaces configured on the system
    with the following command:

    $ sudo nmcli device
    The output should contain the following:
    wifi disconnected
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: wireless-disable-interfaces
    creationTimestamp: "2021-07-13T16:58:54Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-master
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e112e811-21d2-4151-a3ec-66d68fbaab65"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:54Z"
    name: rhcos4-moderate-master-wireless-disable-interfaces
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-master
      uid: e112e811-21d2-4151-a3ec-66d68fbaab65
    resourceVersion: "40339"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-master-wireless-disable-interfaces
    uid: 178a23ae-ba34-4b07-9aa2-ccbab67f520d
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Only Root Has UID 0
    An account has root authority if it has a UID of 0. Multiple accounts
    with a UID of 0 afford more opportunity for potential intruders to
    guess a password for a privileged account. Proper configuration of
    sudo is recommended to afford multiple system administrators
    access to root privileges in an accountable manner.
  id: xccdf_org.ssgproject.content_rule_accounts_no_uid_except_zero
  instructions: |-
    To list all password file entries for accounts with UID 0, run the
    following command:
    $ awk -F: '($3 == \"0\") {print}' /etc/passwd
    This should print only one line, for the user root.

    If there is a finding, change the UID of the failing (non-root) user. If
    the account is associated with the system commands or applications the UID
    should be changed to one greater than 0 but less than
    1000. Otherwise assign a UID of greater than 1000 that
    has not already been assigned.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: accounts-no-uid-except-zero
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-accounts-no-uid-except-zero
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40419"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-accounts-no-uid-except-zero
    uid: 172cd44a-6e99-4948-b06e-63c381fa2e72
  severity: high
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - chmod
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_chmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    chmod system call, run the following command:
    preserve$ sudo grep "chmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-chmod
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-chmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40797"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-chmod
    uid: 95ba7224-187b-4e11-b156-2ed814bb97e1
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - chown
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_chown
  instructions: |-
    To determine if the system is configured to audit calls to the
    chown system call, run the following command:
    preserve$ sudo grep "chown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-chown
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-chown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41202"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-chown
    uid: 680db7c5-91d0-4304-95ee-d6019a80c040
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchmod
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmod system call, run the following command:
    preserve$ sudo grep "fchmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchmod
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-fchmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41464"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-fchmod
    uid: a8c8f8b7-8692-40d0-9678-d4ef4dbf84d5
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchmodat
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchmodat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmodat system call, run the following command:
    preserve$ sudo grep "fchmodat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchmodat
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-fchmodat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40907"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-fchmodat
    uid: 8a5a4ad6-3aad-4e0b-a7a8-ad43e88d8628
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchown
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchown system call, run the following command:
    preserve$ sudo grep "fchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchown
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-fchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40655"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-fchown
    uid: 11ef8996-cd66-4020-8d42-3bcdd21c28c6
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fchownat
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fchownat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchownat system call, run the following command:
    preserve$ sudo grep "fchownat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fchownat
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-fchownat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41053"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-fchownat
    uid: d44a39c4-5ebe-4715-a165-99c79f2188d3
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fremovexattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fremovexattr system call, run the following command:
    preserve$ sudo grep "fremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fremovexattr
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-fremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40484"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-fremovexattr
    uid: e00ec7ec-0293-4e28-bb02-08fa39ae4aea
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - fsetxattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_fsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fsetxattr system call, run the following command:
    preserve$ sudo grep "fsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-fsetxattr
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-fsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40682"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-fsetxattr
    uid: a1558da0-f26c-4475-81c1-f848c7e52e22
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - lchown
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_lchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    lchown system call, run the following command:
    preserve$ sudo grep "lchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-lchown
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-lchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40381"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-lchown
    uid: 9dff55c1-456a-4737-81b7-8765aa42c3ff
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - lremovexattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_lremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lremovexattr system call, run the following command:
    preserve$ sudo grep "lremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-lremovexattr
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-lremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40676"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-lremovexattr
    uid: aa9463f0-f408-4f84-804b-84916b3a18a7
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - lsetxattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_lsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lsetxattr system call, run the following command:
    preserve$ sudo grep "lsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-lsetxattr
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-lsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40564"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-lsetxattr
    uid: c64a1d32-8973-46fc-970b-9fd5850ee80f
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - removexattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_removexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    removexattr system call, run the following command:
    preserve$ sudo grep "removexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-removexattr
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-removexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41385"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-removexattr
    uid: 951eeb8b-df17-44e8-87cb-819f400ee733
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Discretionary Access Controls - setxattr
    The changing of file permissions could indicate that a user is attempting to
    gain access to information that would otherwise be disallowed. Auditing DAC modifications
    can facilitate the identification of patterns of abuse among both authorized and
    unauthorized users.
  id: xccdf_org.ssgproject.content_rule_audit_rules_dac_modification_setxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    setxattr system call, run the following command:
    preserve$ sudo grep "setxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-dac-modification-setxattr
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-dac-modification-setxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40846"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-dac-modification-setxattr
    uid: e68ffdae-3a28-4a14-9176-3b3ca34d967f
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/group
    Creation of groups through direct edition of /etc/group could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_group_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-group-open
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-audit-rules-etc-group-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41480"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-group-open
    uid: 8951bf58-8011-4f3a-ba2d-94cb410d0e97
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&03 -F path=/etc/group -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/group
    Creation of groups through direct edition of /etc/group could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_group_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-group-open-by-handle-at
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-audit-rules-etc-group-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40649"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-group-open-by-handle-at
    uid: 86aedc08-c2b6-4260-8f14-dbff2721456e
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/group -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/group
    Creation of groups through direct edition of /etc/group could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_group_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-group-openat
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-etc-group-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40867"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-group-openat
    uid: 278a6844-7070-452a-a4d6-2b59f3334d80
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/group -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/gshadow
    Creation of users through direct edition of /etc/gshadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_gshadow_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-gshadow-open
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-audit-rules-etc-gshadow-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40966"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-gshadow-open
    uid: 8cf633a7-6c44-4de5-be9c-36c2bc53197e
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&03 -F path=/etc/gshadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/gshadow
    Creation of users through direct edition of /etc/gshadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_gshadow_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-gshadow-open-by-handle-at
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-audit-rules-etc-gshadow-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41184"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-gshadow-open-by-handle-at
    uid: 479f5a70-0cba-403f-a61c-68264ca5169f
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/gshadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/gshadow
    Creation of users through direct edition of /etc/gshadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_gshadow_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-gshadow-openat
    creationTimestamp: "2021-07-13T16:58:55Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:55Z"
    name: rhcos4-moderate-worker-audit-rules-etc-gshadow-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40379"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-gshadow-openat
    uid: 923d3d75-6309-4958-b67d-af1dad38bb3c
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/gshadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/passwd
    Creation of users through direct edition of /etc/passwd could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_passwd_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-passwd-open
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-audit-rules-etc-passwd-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40927"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-passwd-open
    uid: 7638a525-1e3f-4029-a0fe-e7988261f623
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&03 -F path=/etc/passwd -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/passwd
    Creation of users through direct edition of /etc/passwd could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_passwd_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-passwd-open-by-handle-at
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-etc-passwd-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40420"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-passwd-open-by-handle-at
    uid: 398fcd63-72f4-4708-b44a-e7058fac055a
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/passwd -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/passwd
    Creation of users through direct edition of /etc/passwd could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_passwd_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-passwd-openat
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-etc-passwd-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40862"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-passwd-openat
    uid: 97cdecc0-f9cd-4a92-9516-950a6c2dc224
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&03 -F path=/etc/passwd -F auid>=1000 -F auid!=unset -F key=modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open syscall - /etc/shadow
    Creation of users through direct edition of /etc/shadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_shadow_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-shadow-open
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-audit-rules-etc-shadow-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41146"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-shadow-open
    uid: 5853b3f3-f56b-4cb5-83c2-4105797fd420
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open,openat,open_by_handle_at -F a1&03 -F path=/etc/shadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via open_by_handle_at syscall - /etc/shadow
    Creation of users through direct edition of /etc/shadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_shadow_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-shadow-open-by-handle-at
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-audit-rules-etc-shadow-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41120"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-shadow-open-by-handle-at
    uid: 42b8f834-9aab-4eb2-8710-9df117fa9f5d
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open,openat,open_by_handle_at -F a2&03 -F path=/etc/shadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information via openat syscall - /etc/shadow
    Creation of users through direct edition of /etc/shadow could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_etc_shadow_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-etc-shadow-openat
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-audit-rules-etc-shadow-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40547"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-etc-shadow-openat
    uid: b40cfbf9-dd93-4c30-868b-0c7cef56e7b4
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open,openat,open_by_handle_at -F a2&03 -F path=/etc/shadow -F auid>=1000 -F auid!=unset -F key=user-modify
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run chcon
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_chcon
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/bin/chcon" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:

    -a always,exit -F path=/usr/bin/chcon -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-chcon
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-audit-rules-execution-chcon
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41206"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-execution-chcon
    uid: 195bc2e0-0839-40f3-a349-75d30ab397a7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run restorecon
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_restorecon
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/restorecon" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/restorecon -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-restorecon
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-audit-rules-execution-restorecon
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40902"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-execution-restorecon
    uid: 5dc4ae6c-d46c-4e31-b671-6f57850837f8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run semanage
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_semanage
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/semanage" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/semanage -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-semanage
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-audit-rules-execution-semanage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41079"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-execution-semanage
    uid: a447ac0c-fae2-4b66-9363-6f7dac2285ac
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run setfiles
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_setfiles
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/setfiles" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/setfiles -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-setfiles
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-audit-rules-execution-setfiles
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41371"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-execution-setfiles
    uid: 8c47d720-3230-464a-8f7a-90e35facb664
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run setsebool
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_setsebool
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/setsebool" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/setsebool -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-setsebool
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-audit-rules-execution-setsebool
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40912"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-execution-setsebool
    uid: a45680e8-0100-452b-a3dd-c898435d9357
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Any Attempts to Run seunshare
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_execution_seunshare
  instructions: |-
    To verify that execution of the command is being audited, run the following command:
    $ sudo grep "path=/usr/sbin/seunshare" /etc/audit/audit.rules /etc/audit/rules.d/*
    The output should return something similar to:
    -a always,exit -F path=/usr/sbin/seunshare -F auid>=1000 -F auid!=unset -F key=privileged
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-execution-seunshare
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-audit-rules-execution-seunshare
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41309"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-execution-seunshare
    uid: 0b10038e-b5fe-40dc-b88c-998ada05ccd8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - rename
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_rename
  instructions: |-
    To determine if the system is configured to audit calls to the
    rename system call, run the following command:
    preserve$ sudo grep "rename" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-rename
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-audit-rules-file-deletion-events-rename
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41428"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-file-deletion-events-rename
    uid: 33084ee5-1bad-4e57-b78b-22171c856970
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - renameat
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_renameat
  instructions: |-
    To determine if the system is configured to audit calls to the
    renameat system call, run the following command:
    preserve$ sudo grep "renameat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-renameat
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-file-deletion-events-renameat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40490"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-file-deletion-events-renameat
    uid: 6e25c637-56e8-4526-9bd9-ae620c6c62d4
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - rmdir
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_rmdir
  instructions: |-
    To determine if the system is configured to audit calls to the
    rmdir system call, run the following command:
    preserve$ sudo grep "rmdir" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-rmdir
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-file-deletion-events-rmdir
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41346"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-file-deletion-events-rmdir
    uid: 2850bb94-5ed1-42cd-954b-7d2dacbe9e00
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - unlink
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_unlink
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlink system call, run the following command:
    preserve$ sudo grep "unlink" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-unlink
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-audit-rules-file-deletion-events-unlink
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41454"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-file-deletion-events-unlink
    uid: 5afafa67-61a7-41fe-affe-184b2a43957a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects File Deletion Events by User - unlinkat
    Auditing file deletions will create an audit trail for files that are removed
    from the system. The audit trail could aid in system troubleshooting, as well as, detecting
    malicious processes that attempt to delete log files to conceal their presence.
  id: xccdf_org.ssgproject.content_rule_audit_rules_file_deletion_events_unlinkat
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlinkat system call, run the following command:
    preserve$ sudo grep "unlinkat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-file-deletion-events-unlinkat
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-audit-rules-file-deletion-events-unlinkat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41044"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-file-deletion-events-unlinkat
    uid: 9c45cf74-f04d-463b-8ec6-b3b53c2cf614
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Make the auditd Configuration Immutable
    Making the audit configuration immutable prevents accidental as
    well as malicious modification of the audit rules, although it may be
    problematic if legitimate changes are needed during system
    operation
  id: xccdf_org.ssgproject.content_rule_audit_rules_immutable
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-immutable
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-immutable
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40452"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-immutable
    uid: cf6a228a-7894-429a-8b28-2d05af44afd1
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Kernel Module Unloading - delete_module
    The removal of kernel modules can be used to alter the behavior of
    the kernel and potentially introduce malicious code into kernel space. It is important
    to have an audit trail of modules that have been introduced into the kernel.
  id: xccdf_org.ssgproject.content_rule_audit_rules_kernel_module_loading_delete
  instructions: |-
    To determine if the system is configured to audit calls to the
    delete_module system call, run the following command:
    preserve$ sudo grep "delete_module" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-kernel-module-loading-delete
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-audit-rules-kernel-module-loading-delete
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41190"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-kernel-module-loading-delete
    uid: cd215cbc-a2b6-450c-8857-3c88da15db4d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Kernel Module Loading and Unloading - finit_module
    The addition/removal of kernel modules can be used to alter the behavior of
    the kernel and potentially introduce malicious code into kernel space. It is important
    to have an audit trail of modules that have been introduced into the kernel.
  id: xccdf_org.ssgproject.content_rule_audit_rules_kernel_module_loading_finit
  instructions: |-
    To determine if the system is configured to audit calls to the
    finit_module system call, run the following command:
    preserve$ sudo grep "finit_module" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-kernel-module-loading-finit
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-audit-rules-kernel-module-loading-finit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41401"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-kernel-module-loading-finit
    uid: 1655ea79-1e5b-4981-9b93-57a0ac4f05de
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Kernel Module Loading - init_module
    The addition of kernel modules can be used to alter the behavior of
    the kernel and potentially introduce malicious code into kernel space. It is important
    to have an audit trail of modules that have been introduced into the kernel.
  id: xccdf_org.ssgproject.content_rule_audit_rules_kernel_module_loading_init
  instructions: |-
    To determine if the system is configured to audit calls to the
    init_module system call, run the following command:
    preserve$ sudo grep "init_module" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-kernel-module-loading-init
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-audit-rules-kernel-module-loading-init
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40740"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-kernel-module-loading-init
    uid: 7a32e4c9-2799-4d29-94b5-85e6db978474
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Logon and Logout Events - faillock
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_login_events_faillock
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/var/run/faillock\|-w /var/run/faillock"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-login-events-faillock
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-audit-rules-login-events-faillock
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40813"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-login-events-faillock
    uid: 4dc2546d-5b3b-41a9-9736-eab606d8e253
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Logon and Logout Events - lastlog
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_login_events_lastlog
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/var/log/lastlog\|-w /var/log/lastlog"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-login-events-lastlog
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-audit-rules-login-events-lastlog
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41425"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-login-events-lastlog
    uid: 2c9c3449-56a3-4aab-9fa7-e45ab9266dc2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Logon and Logout Events - tallylog
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_login_events_tallylog
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/var/log/tallylog\|-w /var/log/tallylog"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-login-events-tallylog
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-audit-rules-login-events-tallylog
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40979"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-login-events-tallylog
    uid: bbe496e0-cd0b-46d9-91d3-3375703980e2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Mandatory Access Controls
    The system's mandatory access policy (SELinux) should not be
    arbitrarily changed by anything other than administrator action. All changes to
    MAC policy should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_mac_modification
  instructions: |-
    To determine if the system is configured to audit changes to its SELinux
    configuration files, run the following command:
    $ sudo auditctl -l | grep "dir=/etc/selinux"
    If the system is configured to watch for changes to its SELinux
    configuration, a line should be returned (including
    perm=wa indicating permissions that are watched).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-mac-modification
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-audit-rules-mac-modification
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41295"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-mac-modification
    uid: c4c05404-58ab-4817-8688-568890915327
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on Exporting to Media (successful)
    The unauthorized exportation of data to external media could result in an information leak
    where classified information, Privacy Act information, and intellectual property could be lost. An audit
    trail should be created each time a filesystem is mounted to help identify and guard against information
    loss.
  id: xccdf_org.ssgproject.content_rule_audit_rules_media_export
  instructions: |-
    To verify that auditing is configured for all media exportation events, run the following command:
    $ sudo auditctl -l | grep syscall | grep mount
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-media-export
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-audit-rules-media-export
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41299"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-media-export
    uid: c98ae5c5-a8b9-4971-93a1-d11e56bd021e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify the System's Network Environment
    The network environment should not be modified by anything other
    than administrator action. Any change to network parameters should be
    audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_networkconfig_modification
  instructions: |-
    To determine if the system is configured to audit changes to its network configuration,
    run the following command:
    auditctl -l | egrep '(/etc/issue|/etc/issue.net|/etc/hosts|/etc/sysconfig/network)'
    If the system is configured to watch for network configuration changes, a line should be returned for
    each file specified (and perm=wa should be indicated for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-networkconfig-modification
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-networkconfig-modification
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41330"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-networkconfig-modification
    uid: b7121c08-fadf-40b2-a54e-983883947e67
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - at
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_at
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep '\bat\b' /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-at
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41289"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-at
    uid: b4b3b9d2-49ae-472d-9b8c-3f0b48542bf8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - chage
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_chage
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep chage /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-chage
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-chage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41073"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-chage
    uid: e421ba81-1a7c-49a2-9963-21a241bc15b2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - chsh
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_chsh
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep chsh /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-chsh
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-chsh
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41303"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-chsh
    uid: b6a95adf-1e9b-4bd6-bbac-c4f46a5edb41
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - crontab
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_crontab
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep crontab /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-crontab
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-crontab
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40463"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-crontab
    uid: 03d06684-2564-4ad6-9ff0-738b7e6a0d5c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - gpasswd
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_gpasswd
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep gpasswd /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-gpasswd
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-gpasswd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40696"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-gpasswd
    uid: 42254180-1f05-4426-8715-e168e6d6565e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - mount
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_mount
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep mount /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-mount
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-mount
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40872"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-mount
    uid: b9d83a4f-7876-4f03-9232-dd95aa804883
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - newgidmap
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_newgidmap
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep newgidmap /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-newgidmap
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-newgidmap
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41040"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-newgidmap
    uid: 0b1ed927-eaa7-42f9-af1d-56018d790f7c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - newgrp
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_newgrp
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep newgrp /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-newgrp
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-newgrp
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40601"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-newgrp
    uid: 3596596c-df93-4ede-9776-6c434a451888
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - newuidmap
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_newuidmap
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep newuidmap /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-newuidmap
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-newuidmap
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40425"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-newuidmap
    uid: 27f879e9-38de-4c61-b4d1-e6b36b216640
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - pam_timestamp_check
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_pam_timestamp_check
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep pam_timestamp_check /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-pam-timestamp-check
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-pam-timestamp-check
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40818"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-pam-timestamp-check
    uid: 5683f24d-870a-42ae-94e9-8f259fffd8e5
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - passwd
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_passwd
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep passwd /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-passwd
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-passwd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40897"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-passwd
    uid: b749fde6-8b23-430c-9fb1-be16d266bcff
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - postdrop
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_postdrop
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep postdrop /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-postdrop
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-postdrop
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41416"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-postdrop
    uid: 824de230-60be-4b7d-83b7-03ca8ad64bcf
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - postqueue
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_postqueue
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep postqueue /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-postqueue
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-postqueue
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40688"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-postqueue
    uid: dc5622f7-ec1b-4eda-a862-ff3edb15f3d0
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - pt_chown
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_pt_chown
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep pt_chown /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-pt-chown
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-pt-chown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41334"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-pt-chown
    uid: 13831cd2-c383-4c45-bf02-6ac4339ca3a5
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - ssh-keysign
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_ssh_keysign
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep ssh-keysign /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-ssh-keysign
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-ssh-keysign
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40725"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-ssh-keysign
    uid: 05dc80d8-817d-4b34-bb00-ffb89486fad0
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - su
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_su
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep su /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-su
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-su
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40393"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-su
    uid: 6bea5d8c-835d-440d-ad7d-e6998b365175
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - sudo
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_sudo
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep sudo /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-sudo
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-sudo
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40384"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-sudo
    uid: 9025735e-e34b-4243-832c-574b7fdf149e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - sudoedit
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_sudoedit
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep sudoedit /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-sudoedit
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-sudoedit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40733"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-sudoedit
    uid: 8a76dabe-7638-4d1f-ad5d-038e56f0b160
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - umount
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_umount
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep umount /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-umount
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-umount
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40553"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-umount
    uid: 8ba58703-b74b-4101-be0b-54ef3af04f44
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - unix_chkpwd
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_unix_chkpwd
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep unix_chkpwd /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-unix-chkpwd
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-unix-chkpwd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40937"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-unix-chkpwd
    uid: 6fe26a60-4b7e-4c6f-9db0-cf591e521a94
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - userhelper
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_userhelper
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep userhelper /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-userhelper
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-userhelper
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40456"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-userhelper
    uid: ab811ddb-e309-4dac-9e86-98cec596fdcf
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects Information on the Use of Privileged Commands - usernetctl
    Misuse of privileged functions, either intentionally or unintentionally by
    authorized users, or by unauthorized external entities that have compromised system accounts,
    is a serious and ongoing concern and can have significant adverse impacts on organizations.
    Auditing the use of privileged functions is one way to detect such misuse and identify
    the risk from insider and advanced persistent threats.

    Privileged programs are subject to escalation-of-privilege attacks,
    which attempt to subvert their normal role of providing some necessary but
    limited capability. As such, motivation exists to monitor these programs for
    unusual activity.
  id: xccdf_org.ssgproject.content_rule_audit_rules_privileged_commands_usernetctl
  instructions: |-
    To verify that auditing of privileged command use is configured, run the
    following command:
    $ sudo grep usernetctl /etc/audit/audit.rules /etc/audit/rules.d/*
    It should return a relevant line in the audit rules.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-privileged-commands-usernetctl
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-audit-rules-privileged-commands-usernetctl
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40585"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-privileged-commands-usernetctl
    uid: 141d30af-5b15-41cf-a1e7-5c631fdb4ed6
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Process and Session Initiation Information
    Manual editing of these files may indicate nefarious activity, such
    as an attacker attempting to remove evidence of an intrusion.
  id: xccdf_org.ssgproject.content_rule_audit_rules_session_events
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-session-events
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-audit-rules-session-events
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41131"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-session-events
    uid: 22f0a21d-233c-47e1-8d18-ff6d3de51ecd
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Collects System Administrator Actions
    The actions taken by system administrators should be audited to keep a record
    of what was executed on the system, as well as, for accountability purposes.
  id: xccdf_org.ssgproject.content_rule_audit_rules_sysadmin_actions
  instructions: |-
    To verify that auditing is configured for system administrator actions, run the following command:
    $ sudo auditctl -l | grep "watch=/etc/sudoers\|watch=/etc/sudoers.d\|-w /etc/sudoers\|-w /etc/sudoers.d"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-sysadmin-actions
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-audit-rules-sysadmin-actions
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41366"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-sysadmin-actions
    uid: 26ec7e46-be8f-4af9-9279-59809a927384
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record attempts to alter time through adjtimex
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_adjtimex
  instructions: |-
    To determine if the system is configured to audit calls to the
    adjtimex system call, run the following command:
    preserve$ sudo grep "adjtimex" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-adjtimex
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-audit-rules-time-adjtimex
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41276"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-time-adjtimex
    uid: de2a0276-b818-40f0-aefd-192fb3ce000f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Time Through clock_settime
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_clock_settime
  instructions: |-
    To determine if the system is configured to audit calls to the
    clock_settime system call, run the following command:
    preserve$ sudo grep "clock_settime" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-clock-settime
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-time-clock-settime
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41320"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-time-clock-settime
    uid: bb8e6a6b-48f5-4a07-a333-61ad3459db55
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record attempts to alter time through settimeofday
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_settimeofday
  instructions: |-
    To determine if the system is configured to audit calls to the
    settimeofday system call, run the following command:
    preserve$ sudo grep "settimeofday" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-settimeofday
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-audit-rules-time-settimeofday
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40590"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-time-settimeofday
    uid: 0c4e5fea-6fc7-4a3a-87dc-39a1a6afce16
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter Time Through stime
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_stime
  instructions: |-
    If the system is not configured to audit time changes, this is a finding.
    If the system is 64-bit only, this is not applicable
    ocil: |
    To determine if the system is configured to audit calls to the
    stime system call, run the following command:
    preserve$ sudo grep "stime" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-stime
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-time-stime
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41338"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-time-stime
    uid: 796bec19-4687-4a37-a018-f6f1dafeb6e2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Attempts to Alter the localtime File
    Arbitrary changes to the system time can be used to obfuscate
    nefarious activities in log files, as well as to confuse network services that
    are highly dependent upon an accurate system time (such as sshd). All changes
    to the system time should be audited.
  id: xccdf_org.ssgproject.content_rule_audit_rules_time_watch_localtime
  instructions: |-
    To determine if the system is configured to audit attempts to
    alter time via the /etc/localtime file, run the following
    command:
    $ sudo auditctl -l | grep "watch=/etc/localtime"
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-time-watch-localtime
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-audit-rules-time-watch-localtime
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41374"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-time-watch-localtime
    uid: fe5e4f60-55e7-481b-9da2-ceeb45f7440a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - chmod
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_chmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    chmod system call, run the following command:
    preserve$ sudo grep "chmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-chmod
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-chmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40496"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-chmod
    uid: deb5dd43-6206-4552-9a45-3780278948bf
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - chown
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_chown
  instructions: |-
    To determine if the system is configured to audit calls to the
    chown system call, run the following command:
    preserve$ sudo grep "chown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-chown
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-chown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40512"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-chown
    uid: c70dd209-ff34-4ed6-bc10-12fef9daa781
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - creat
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    creat system call, run the following command:
    preserve$ sudo grep "creat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-creat
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41441"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-creat
    uid: 154275ca-83e3-4e2b-84c3-7ea4c4db89e6
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fchmod
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchmod
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmod system call, run the following command:
    preserve$ sudo grep "fchmod" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchmod
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchmod
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40570"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchmod
    uid: e1b7e81a-26e7-44af-b359-3a132b9aaa49
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fchmodat
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchmodat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchmodat system call, run the following command:
    preserve$ sudo grep "fchmodat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchmodat
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchmodat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40435"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchmodat
    uid: 8e3a4886-e192-4e39-bd26-aeaf0bc8911b
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - fchown
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchown system call, run the following command:
    preserve$ sudo grep "fchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchown
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40808"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchown
    uid: 668a1768-0515-4448-8bc6-f9dc2fe46192
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - fchownat
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fchownat
  instructions: |-
    To determine if the system is configured to audit calls to the
    fchownat system call, run the following command:
    preserve$ sudo grep "fchownat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fchownat
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchownat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41249"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fchownat
    uid: 4402322f-a1bf-410f-b11d-574bc28d12e3
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fremovexattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fremovexattr system call, run the following command:
    preserve$ sudo grep "fremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fremovexattr
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40502"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fremovexattr
    uid: 43e60d59-4700-4743-88be-be9171aa518a
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - fsetxattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_fsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    fsetxattr system call, run the following command:
    preserve$ sudo grep "fsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-fsetxattr
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41135"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-fsetxattr
    uid: ab3786a6-ce6c-46c4-b188-c697af898523
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - ftruncate
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_ftruncate
  instructions: |-
    To determine if the system is configured to audit calls to the
    ftruncate system call, run the following command:
    preserve$ sudo grep "ftruncate" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-ftruncate
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-ftruncate
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40519"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-ftruncate
    uid: 25d91d3c-8b52-4b77-b678-4bcba86a1b29
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Ownership Changes to Files - lchown
    Unsuccessful attempts to change ownership of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_lchown
  instructions: |-
    To determine if the system is configured to audit calls to the
    lchown system call, run the following command:
    preserve$ sudo grep "lchown" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-lchown
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-lchown
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41176"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-lchown
    uid: 2dc63ce2-1dd0-47a0-a9b8-1aefebb2ec88
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S lchown,fchown,chown,fchownat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - lremovexattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_lremovexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lremovexattr system call, run the following command:
    preserve$ sudo grep "lremovexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-lremovexattr
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-lremovexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41323"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-lremovexattr
    uid: ec189d4d-4bc3-468c-9b78-1e39128edd98
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - lsetxattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_lsetxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    lsetxattr system call, run the following command:
    preserve$ sudo grep "lsetxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-lsetxattr
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-lsetxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41313"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-lsetxattr
    uid: 37ce3367-f0d3-4ba7-ad49-4ed1c7da5bca
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - open
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40525"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open
    uid: 45bac2bb-206e-43bb-9ada-2e31b4d87e5a
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - open_by_handle_at
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40703"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at
    uid: cfbc55d8-a3e8-49bb-bc82-a8d4a35670ed
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Creation Attempts to Files - open_by_handle_at O_CREAT
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at_o_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at-o-creat
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41444"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-creat
    uid: 11ff4f5c-efaa-443a-a57f-fab9d9d82ca9
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&0100 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-create
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Modification Attempts to Files - open_by_handle_at O_TRUNC_WRITE
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at_o_trunc_write
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at-o-trunc-write
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-trunc-write
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41099"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at-o-trunc-write
    uid: 7a3aa307-eef6-489a-8fa5-a64321f68468
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&01003 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-modification
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Unauthorized Access Attempts To open_by_handle_at Are Ordered Correctly
    The more specific rules cover a subset of events covered by the less specific rules.
    By ordering them from more specific to less specific, it is assured that the less specific
    rule will not catch events better recorded by the more specific rule.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_by_handle_at_rule_order
  instructions: |-
    To determine if the system is configured to audit calls to the
    open_by_handle_at system call, run the following command:
    preserve$ sudo grep "open_by_handle_at" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-by-handle-at-rule-order
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at-rule-order
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40428"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-by-handle-at-rule-order
    uid: ffbfe57b-95bf-42e8-bd3e-48fe3ccdec11
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Creation Attempts to Files - open O_CREAT
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_o_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-o-creat
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-o-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40397"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-o-creat
    uid: b5489a18-f2aa-4718-985d-b7874037237f
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&0100 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-create
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Modification Attempts to Files - open O_TRUNC_WRITE
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_o_trunc_write
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-o-trunc-write
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-o-trunc-write
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41048"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-o-trunc-write
    uid: 297ae43c-7d5f-4f5a-b3f5-8da3bdcce567
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S open -F a1&01003 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-modification
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Rules For Unauthorized Attempts To open Are Ordered Correctly
    The more specific rules cover a subset of events covered by the less specific rules.
    By ordering them from more specific to less specific, it is assured that the less specific
    rule will not catch events better recorded by the more specific rule.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_open_rule_order
  instructions: |-
    To determine if the system is configured to audit calls to the
    open system call, run the following command:
    preserve$ sudo grep "open" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-open-rule-order
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-rule-order
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40880"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-open-rule-order
    uid: b2ba253e-6f58-4188-97e0-7faf99364402
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - openat
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41484"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat
    uid: 43edb88d-54cc-41c7-98b4-abffb8d06347
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Creation Attempts to Files - openat O_CREAT
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat_o_creat
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat-o-creat
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat-o-creat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41389"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat-o-creat
    uid: 5a09358e-5cd4-4aee-8111-8504040377f5
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&0100 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-create
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Modification Attempts to Files - openat O_TRUNC_WRITE
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat_o_trunc_write
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat-o-trunc-write
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat-o-trunc-write
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41139"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat-o-trunc-write
    uid: 551a3b9b-ea8f-44fb-a95d-90d952110ec3
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&01003 -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-modification
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure auditd Rules For Unauthorized Attempts To openat Are Ordered Correctly
    The more specific rules cover a subset of events covered by the less specific rules.
    By ordering them from more specific to less specific, it is assured that the less specific
    rule will not catch events better recorded by the more specific rule.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_openat_rule_order
  instructions: |-
    To determine if the system is configured to audit calls to the
    openat system call, run the following command:
    preserve$ sudo grep "openat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-openat-rule-order
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat-rule-order
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40508"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-openat-rule-order
    uid: 5c8aaf07-7da8-48b9-b8f1-19cac70387af
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - removexattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_removexattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    removexattr system call, run the following command:
    preserve$ sudo grep "removexattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-removexattr
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-removexattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40407"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-removexattr
    uid: 4887a95a-c55d-4b47-91cd-0483a29f1a0d
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - rename
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_rename
  instructions: |-
    To determine if the system is configured to audit calls to the
    rename system call, run the following command:
    preserve$ sudo grep "rename" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-rename
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-rename
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40803"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-rename
    uid: ab9695f7-06ac-4ea4-bade-71337de84eed
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - renameat
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_renameat
  instructions: |-
    To determine if the system is configured to audit calls to the
    renameat system call, run the following command:
    preserve$ sudo grep "renameat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-renameat
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-renameat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41471"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-renameat
    uid: 1015e068-88f7-4882-ae5d-43a583015fe6
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Permission Changes to Files - setxattr
    Unsuccessful attempts to change permissions of files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_setxattr
  instructions: |-
    To determine if the system is configured to audit calls to the
    setxattr system call, run the following command:
    preserve$ sudo grep "setxattr" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-setxattr
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-setxattr
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41217"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-setxattr
    uid: 3d08c353-3871-4b1f-9035-027c9525d760
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the audit rule checks a system call independently of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S chmod,fchmod,fchmodat,setxattr,lsetxattr,fsetxattr -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-perm-change
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessful Access Attempts to Files - truncate
    Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_truncate
  instructions: |-
    To determine if the system is configured to audit calls to the
    truncate system call, run the following command:
    preserve$ sudo grep "truncate" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-truncate
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-truncate
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41437"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-truncate
    uid: 125a734d-8a25-4d7a-9b5a-7a85545a55ab
  severity: medium
  status: FAIL
  warnings:
  - Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping these system calls with others as identifying earlier in this guide is more efficient.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - unlink
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_unlink
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlink system call, run the following command:
    preserve$ sudo grep "unlink" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-unlink
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-unlink
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41468"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-unlink
    uid: 352e38c5-71f5-4e79-91be-1d25f74c66c9
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Unsuccessul Delete Attempts to Files - unlinkat
    Unsuccessful attempts to delete files could be an indicator of malicious activity on a system. Auditing
    these events could serve as evidence of potential system compromise.
  id: xccdf_org.ssgproject.content_rule_audit_rules_unsuccessful_file_modification_unlinkat
  instructions: |-
    To determine if the system is configured to audit calls to the
    unlinkat system call, run the following command:
    preserve$ sudo grep "unlinkat" /etc/audit/audit.*
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-unsuccessful-file-modification-unlinkat
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-unlinkat
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41488"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-unsuccessful-file-modification-unlinkat
    uid: 0f2812c1-b7fd-4008-99d4-0aa549c35a5a
  severity: medium
  status: FAIL
  warnings:
  - |-
    Note that these rules can be configured in a number of ways while still achieving the desired effect. Here the system calls have been placed independent of other system calls. Grouping system calls related to the same event is more efficient. See the following example:

    -a always,exit -F arch=b32 -S unlink,unlinkat,rename,renameat -F exit=-EACCES -F auid>=1000 -F auid!=unset -F key=unsuccesful-delete
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/group
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_group
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/group)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-group
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-audit-rules-usergroup-modification-group
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41114"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-usergroup-modification-group
    uid: e72e9c1d-8480-4dec-a30c-fbb421c8f284
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/gshadow
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_gshadow
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/gshadow)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-gshadow
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-audit-rules-usergroup-modification-gshadow
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40892"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-usergroup-modification-gshadow
    uid: a10f6823-eb27-4bde-a532-4519182a15e5
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/security/opasswd
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_opasswd
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/security/opasswd)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-opasswd
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-usergroup-modification-opasswd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40841"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-usergroup-modification-opasswd
    uid: b7c07361-3208-4276-a42b-01ef1cc9eb08
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/passwd
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_passwd
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/passwd)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-passwd
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-audit-rules-usergroup-modification-passwd
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40836"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-usergroup-modification-passwd
    uid: 2536402c-ebb8-4a05-a587-6b04051fb1ab
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Events that Modify User/Group Information - /etc/shadow
    In addition to auditing new user and group accounts, these watches
    will alert the system administrator(s) to any modifications. Any unexpected
    users, groups, or modifications should be investigated for legitimacy.
  id: xccdf_org.ssgproject.content_rule_audit_rules_usergroup_modification_shadow
  instructions: |-
    To determine if the system is configured to audit account changes,
    run the following command:

    auditctl -l | egrep '(/etc/shadow)'

    If the system is configured to watch for account changes, lines should be returned for
    each file specified (and with perm=wa for each).
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: audit-rules-usergroup-modification-shadow
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-audit-rules-usergroup-modification-shadow
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40413"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-audit-rules-usergroup-modification-shadow
    uid: 1edcae0a-ce85-461c-b16b-45168fdc130c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Disk Error Action on Disk Error
    Taking appropriate action in case of disk errors will minimize the possibility of
    losing audit records.
  id: xccdf_org.ssgproject.content_rule_auditd_data_disk_error_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to either log to syslog,
    switch to single-user mode, execute a script,
    or halt when the disk errors:
    disk_error_action single
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-disk-error-action
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-auditd-data-disk-error-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40558"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-disk-error-action
    uid: 00477943-b574-481c-9fa3-011f959939c6
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Disk Full Action when Disk Space Is Full
    Taking appropriate action in case of a filled audit storage volume will minimize
    the possibility of losing audit records.
  id: xccdf_org.ssgproject.content_rule_auditd_data_disk_full_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to either log to syslog,
    switch to single-user mode, execute a script,
    or halt when the disk is out of space:
    disk_full_action single
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-disk-full-action
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-auditd-data-disk-full-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41254"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-disk-full-action
    uid: 5e438c9e-d03d-4077-8990-df43be4589cd
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd admin_space_left Action on Low Disk Space
    Administrators should be made aware of an inability to record
    audit records. If a separate partition or logical volume of adequate size
    is used, running low on space for audit records should never occur.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_admin_space_left_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to either suspend, switch to single user mode,
    or halt when disk space has run low:
    admin_space_left_action single
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-admin-space-left-action
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-auditd-data-retention-admin-space-left-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41144"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-admin-space-left-action
    uid: 39e1a889-d76a-4fac-8699-1ff1bc640968
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd flush priority
    Audit data should be synchronously written to disk to ensure
    log integrity. These parameters assure that all audit event data is fully
    synchronized with the log files on the disk.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_flush
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to synchronize audit event data
    with the log files on the disk:
    $ sudo grep flush /etc/audit/auditd.conf
    flush = DATA
    Acceptable values are DATA, and SYNC. The setting is
    case-insensitive.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-flush
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-auditd-data-retention-flush
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41194"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-flush
    uid: 416e75c7-e934-4773-aba6-2cd785ed5bb5
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Max Log File Size
    The total storage for audit log files must be large enough to retain
    log information over the period required. This is a function of the maximum
    log file size and the number of logs retained.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_max_log_file
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine how much data the system will retain in each audit log file:
    $ sudo grep max_log_file /etc/audit/auditd.conf
    max_log_file = 6
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-max-log-file
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-auditd-data-retention-max-log-file
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41222"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-max-log-file
    uid: 2fccee5d-6f7c-4f8b-9667-50e472c1533a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd max_log_file_action Upon Reaching Maximum Log Size
    Automatically rotating logs (by setting this to rotate)
    minimizes the chances of the system unexpectedly running out of disk space by
    being overwhelmed with log data. However, for systems that must never discard
    log data, or which use external processes to transfer it and reclaim space,
    keep_logs can be employed.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_max_log_file_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to rotate logs when they reach their
    maximum size:
    $ sudo grep max_log_file_action /etc/audit/auditd.conf
    max_log_file_action rotate
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-max-log-file-action
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-auditd-data-retention-max-log-file-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41011"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-max-log-file-action
    uid: 8d91061c-d597-42df-ba01-34db163695d5
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd Number of Logs Retained
    The total storage for audit log files must be large enough to retain
    log information over the period required. This is a function of the maximum log
    file size and the number of logs retained.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_num_logs
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine how many logs the system is configured to retain after rotation:
    $ sudo grep num_logs /etc/audit/auditd.conf
    num_logs = 5
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-num-logs
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-auditd-data-retention-num-logs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41293"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-num-logs
    uid: fcb5b453-aefb-4514-b28d-832bc054a50f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd space_left on Low Disk Space
    Notifying administrators of an impending disk space problem may allow them to
    take corrective action prior to any disruption.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_space_left
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured correctly:
    space_left SIZE_in_MB
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-space-left
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-auditd-data-retention-space-left
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40825"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-space-left
    uid: d657c216-f41c-4d56-9d4a-6ea6c51f9f22
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure auditd space_left Action on Low Disk Space
    Notifying administrators of an impending disk space problem may
    allow them to take corrective action prior to any disruption.
  id: xccdf_org.ssgproject.content_rule_auditd_data_retention_space_left_action
  instructions: |-
    Inspect /etc/audit/auditd.conf and locate the following line to
    determine if the system is configured to email the administrator when
    disk space is starting to run low:
    $ sudo grep space_left_action /etc/audit/auditd.conf
    space_left_action
    Acceptable values are email, suspend, single, and halt.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-data-retention-space-left-action
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-auditd-data-retention-space-left-action
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41358"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-data-retention-space-left-action
    uid: d52425de-0395-43d4-ba8c-62d87c8f2cab
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set number of records to cause an explicit flush to audit logs
    If option freq isn't set to 50, the flush to disk
    may happen after higher number of records, increasing the danger
    of audit loss.
  id: xccdf_org.ssgproject.content_rule_auditd_freq
  instructions: |-
    To verify that Audit Daemon is configured to flush to disk after
    every 50 records, run the following command:
    $ sudo grep freq /etc/audit/auditd.conf
    The output should return the following:
    freq = 50
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-freq
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-auditd-freq
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41001"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-freq
    uid: c2955f7a-ec45-43d6-9f7b-1d220fa61b09
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Include Local Events in Audit Logs
    If option local_events isn't set to yes only events from
    network will be aggregated.
  id: xccdf_org.ssgproject.content_rule_auditd_local_events
  instructions: |-
    To verify that Audit Daemon is configured to include local events, run the
    following command:
    $ sudo grep local_events /etc/audit/auditd.conf
    The output should return the following:
    local_events = yes
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-local-events
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-auditd-local-events
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40993"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-local-events
    uid: 61571f6c-c290-41a2-8984-e3311db7f5da
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Resolve information before writing to audit logs
    If option log_format isn't set to ENRICHED, the
    audit records will be stored in a format exactly as the kernel sends them.
  id: xccdf_org.ssgproject.content_rule_auditd_log_format
  instructions: |-
    To verify that Audit Daemon is configured to resolve all uid, gid, syscall,
    architecture, and socket address information before writing the event to disk,
    run the following command:
    $ sudo grep log_format /etc/audit/auditd.conf
    The output should return the following:
    log_format = ENRICHED
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-log-format
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-auditd-log-format
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41149"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-log-format
    uid: 85db819d-03ad-44e8-a9c7-f3ea0ffa4e8f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set hostname as computer node name in audit logs
    If option name_format is left at its default value of
    none, audit events from different computers may be hard
    to distinguish.
  id: xccdf_org.ssgproject.content_rule_auditd_name_format
  instructions: |-
    To verify that Audit Daemon is configured to record the hostname
    in audit events, run the following command:
    $ sudo grep name_format /etc/audit/auditd.conf
    The output should return the following:
    name_format = hostname
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-name-format
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-auditd-name-format
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40852"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-name-format
    uid: d5a4e905-6dba-4551-875d-c7a35874a065
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Write Audit Logs to the Disk
    If write_logs isn't set to yes, the Audit logs will
    not be written to the disk.
  id: xccdf_org.ssgproject.content_rule_auditd_write_logs
  instructions: |-
    To verify that Audit Daemon is configured to write logs to the disk, run the
    following command:
    $ sudo grep write_logs /etc/audit/auditd.conf
    The output should return the following:
    write_logs = yes
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: auditd-write-logs
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-auditd-write-logs
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41356"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-auditd-write-logs
    uid: 5fd9232e-e866-4ac2-98c9-e1971e72432c
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Modify the System Login Banner
    Display of a standardized and approved use notification before granting
    access to the operating system ensures privacy and security notification
    verbiage used is consistent with applicable federal laws, Executive Orders,
    directives, policies, regulations, standards, and guidance.

    System use notifications are required only for access via login interfaces
    with human users and are not required when such human interfaces do not
    exist.
  id: xccdf_org.ssgproject.content_rule_banner_etc_issue
  instructions: |-
    To check if the system login banner is compliant,
    run the following command:
    $ cat /etc/issue
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: banner-etc-issue
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-banner-etc-issue
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40988"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-banner-etc-issue
    uid: 6cf3a097-075c-4cf3-91a8-f86e2bf3011d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Booting from USB Devices in Boot Firmware
    Booting a system from a USB device would allow an attacker to
    circumvent any security measures provided by the operating system. Attackers
    could mount partitions and modify the configuration of the OS.
  id: xccdf_org.ssgproject.content_rule_bios_disable_usb_boot
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: bios-disable-usb-boot
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-bios-disable-usb-boot
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40404"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-bios-disable-usb-boot
    uid: 3b3fe8d2-3e22-4105-911c-0de42d9ae840
  severity: unknown
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable chrony daemon from acting as server
    Minimizing the exposure of the server functionality of the chrony
    daemon diminishes the attack surface.
  id: xccdf_org.ssgproject.content_rule_chronyd_client_only
  instructions: |-
    To verify that port has been set properly, perform the following:
    $ grep '\bport\b' /etc/chrony.conf
    The output should return
    port 0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-client-only
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-chronyd-client-only
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41034"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-chronyd-client-only
    uid: ae1338f2-357a-4eb6-832b-79cd757c770a
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable network management of chrony daemon
    Not exposing the management interface of the chrony daemon on
    the network diminishes the attack space.
  id: xccdf_org.ssgproject.content_rule_chronyd_no_chronyc_network
  instructions: |-
    To verify that cmdport has been set properly, perform the following:
    $ grep '\bcmdport\b' /etc/chrony.conf
    The output should return
    cmdport 0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-no-chronyc-network
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-chronyd-no-chronyc-network
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41397"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-chronyd-no-chronyc-network
    uid: 808a88a4-80cc-46c8-8864-623e1b04dd42
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Time Service Maxpoll Interval
    Inaccurate time stamps make it more difficult to correlate
    events and can lead to an inaccurate analysis. Determining the correct
    time a particular event occurred on a system is critical when conducting
    forensic analysis and investigating system events. Sources outside the
    configured acceptable allowance (drift) may be inaccurate.
  id: xccdf_org.ssgproject.content_rule_chronyd_or_ntpd_set_maxpoll
  instructions: |-
    To verify that maxpoll has been set properly, perform the following:
    $ sudo grep maxpoll /etc/ntp.conf /etc/chrony.conf
    The output should return
    maxpoll .
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-or-ntpd-set-maxpoll
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-chronyd-or-ntpd-set-maxpoll
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41420"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-chronyd-or-ntpd-set-maxpoll
    uid: c4cbb77e-9236-4ea1-9b31-ebf0d447f202
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Specify Additional Remote NTP Servers
    Specifying additional NTP servers increases the availability of
    accurate time data, in the event that one of the specified servers becomes
    unavailable. This is typical for a system acting as an NTP server for
    other systems.
  id: xccdf_org.ssgproject.content_rule_chronyd_or_ntpd_specify_multiple_servers
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-or-ntpd-specify-multiple-servers
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-chronyd-or-ntpd-specify-multiple-servers
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41263"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-chronyd-or-ntpd-specify-multiple-servers
    uid: 944c760a-2d72-43c5-acc5-3c83aee1177e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Specify a Remote NTP Server
    Synchronizing with an NTP server makes it possible to collate system
    logs from multiple sources or correlate computer events with real time events.
  id: xccdf_org.ssgproject.content_rule_chronyd_or_ntpd_specify_remote_server
  instructions: |-
    To verify that a remote NTP service is configured for time synchronization,
    open the following file:
    /etc/chrony.conf in the case the system in question is
    configured to use the chronyd as the NTP daemon (default setting)/etc/ntp.conf in the case the system in question is configured
    to use the ntpd as the NTP daemon
    In the file, there should be a section similar to the following:
    server ntpserver
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: chronyd-or-ntpd-specify-remote-server
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-chronyd-or-ntpd-specify-remote-server
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41065"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-chronyd-or-ntpd-specify-remote-server
    uid: bbca0be0-6d63-467d-b2b0-10cc36d8825f
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure System Cryptography Policy
    Centralized cryptographic policies simplify applying secure ciphers across an operating system and
    the applications that run on that operating system. Use of weak or untested encryption algorithms
    undermines the purposes of utilizing encryption to protect data.
  id: xccdf_org.ssgproject.content_rule_configure_crypto_policy
  instructions: |-
    To verify that cryptography policy has been configured correctly, run the
    following command:
    $ update-crypto-policies --show
    The output should return .
    Run the command to check if the policy is correctly applied:
    $ update-crypto-policies --is-applied
    The output should be The configured policy is applied.
    Moreover, check if settings for selected crypto policy are as expected.
    List all libraries for which it holds that their crypto policies do not have symbolic link in /etc/crypto-policies/back-ends.
    $ ls -l /etc/crypto-policies/back-ends/ | grep '^[^l]' | tail -n +2 | awk -F' ' '{print $NF}' | awk -F'.' '{print $1}' | sort
    Subsequently, check if matching libraries have drop in files in the /etc/crypto-policies/local.d directory.
    $ ls /etc/crypto-policies/local.d/ | awk -F'-' '{print $1}' | uniq | sort
    Outputs of two previous commands should match.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-crypto-policy
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-configure-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40581"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-configure-crypto-policy
    uid: 86945cb1-23b4-4a6f-94d4-5ab9a72d8ab8
  severity: high
  status: FAIL
  warnings:
  - The system needs to be rebooted for these changes to take effect.
  - System Crypto Modules must be provided by a vendor that undergoes FIPS-140 certifications. FIPS-140 is applicable to all Federal agencies that use cryptographic-based security systems to protect sensitive information in computer and telecommunication systems (including voice systems) as defined in Section 5131 of the Information Technology Management Reform Act of 1996, Public Law 104-106. This standard shall be used in designing and implementing cryptographic modules that Federal departments and agencies operate or are operated for them under contract. See *https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.140-2.pdf* To meet this, the system has to have cryptographic software provided by a vendor that has undergone this certification. This means providing documentation, test results, design information, and independent third party review by an accredited lab. While open source software is capable of meeting this, it does not meet FIPS-140 unless the vendor submits to this process.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Kerberos to use System Crypto Policy
    Overriding the system crypto policy makes the behavior of Kerberos violate expectations,
    and makes system configuration more fragmented.
  id: xccdf_org.ssgproject.content_rule_configure_kerberos_crypto_policy
  instructions: |-
    Check that the symlink exists and target the correct kerberos crypto policy, with the following command:
    file /etc/krb5.conf.d/crypto-policies
    If command ouput shows the following line, kerberos is configured to use the system-wide crypto policy.
    /etc/krb5.conf.d/crypto-policies: symbolic link to /etc/crypto-policies/back-ends/krb5.config
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-kerberos-crypto-policy
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-configure-kerberos-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41015"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-configure-kerberos-crypto-policy
    uid: 41c1543d-129f-4386-aaad-3f18381edab4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure OpenSSL library to use System Crypto Policy
    Overriding the system crypto policy makes the behavior of the Java runtime violates expectations,
    and makes system configuration more fragmented.
  id: xccdf_org.ssgproject.content_rule_configure_openssl_crypto_policy
  instructions: |-
    To verify that OpenSSL uses the system crypto policy, check out that the OpenSSL config file
    /etc/pki/tls/openssl.cnf contains the [ crypto_policy ] section with the
    .include /etc/crypto-policies/back-ends/opensslcnf.config directive:
    grep '\.include\s* /etc/crypto-policies/back-ends/opensslcnf.config$' /etc/pki/tls/openssl.cnf.
          Is it the case that the OpenSSL config file doesn't contain the whole section,
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-openssl-crypto-policy
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-configure-openssl-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40406"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-configure-openssl-crypto-policy
    uid: bcf4b870-7a65-4d8a-b534-1ab46e305ec7
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure SSH to use System Crypto Policy
    Overriding the system crypto policy makes the behavior of the SSH service violate expectations,
    and makes system configuration more fragmented.
  id: xccdf_org.ssgproject.content_rule_configure_ssh_crypto_policy
  instructions: |-
    Check that the CRYPTO_POLICY variable is not set or is commented in the
    /etc/sysconfig/sshd.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-ssh-crypto-policy
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-configure-ssh-crypto-policy
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41241"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-configure-ssh-crypto-policy
    uid: 737f3a64-f095-4c85-9c1b-8d1435a441c9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Log USBGuard daemon audit events using Linux Audit
    Using the Linux Audit logging allows for centralized trace
    of events.
  id: xccdf_org.ssgproject.content_rule_configure_usbguard_auditbackend
  instructions: |-
    To verify that Linux Audit logging si enabled for the USBGuard daemon,
    run the following command:
    $ sudo grep AuditBackend /etc/usbguard/usbguard-daemon.conf
    The output should be
    AuditBackend=LinuxAudit
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: configure-usbguard-auditbackend
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-configure-usbguard-auditbackend
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41214"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-configure-usbguard-auditbackend
    uid: 68c6c5df-7883-4e2c-a9c3-7ef4ace9b6c8
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable core dump backtraces
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data
    and is generally useful only for developers or system operators trying to
    debug problems.

    Enabling core dumps on production systems is not recommended,
    however there may be overriding operational requirements to enable advanced
    debuging. Permitting temporary enablement of core dumps during such situations
    should be reviewed through local needs and policy.
  id: xccdf_org.ssgproject.content_rule_coredump_disable_backtraces
  instructions: |-
    To verify that logging core dump backtraces is disabled, run the
    following command:
    $ grep ProcessSizeMax /etc/systemd/coredump.conf
    The output should be:
    ProcessSizeMax=0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coredump-disable-backtraces
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-coredump-disable-backtraces
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41448"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coredump-disable-backtraces
    uid: 5bfd5851-5a6c-4922-98fe-3522b3909f8a
  severity: medium
  status: FAIL
  warnings:
  - If the /etc/systemd/coredump.conf file does not already contain the [Coredump] section, the value will not be configured correctly.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable storing core dump
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data
    and is generally useful only for developers or system operators trying to
    debug problems. Enabling core dumps on production systems is not recommended,
    however there may be overriding operational requirements to enable advanced
    debuging. Permitting temporary enablement of core dumps during such situations
    should be reviewed through local needs and policy.
  id: xccdf_org.ssgproject.content_rule_coredump_disable_storage
  instructions: |-
    To verify that storing core dumps are disabled, run the following command:
    $ grep Storage /etc/systemd/coredump.conf
    The output should be:
    Storage=none
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coredump-disable-storage
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-coredump-disable-storage
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41210"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coredump-disable-storage
    uid: d883630f-7db1-422b-957a-8954db1f823f
  severity: medium
  status: FAIL
  warnings:
  - If the /etc/systemd/coredump.conf file does not already contain the [Coredump] section, the value will not be configured correctly.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Extend Audit Backlog Limit for the Audit Daemon
    audit_backlog_limit sets the queue length for audit events awaiting transfer
    to the audit daemon. Until the audit daemon is up and running, all log messages
    are stored in this queue.  If the queue is overrun during boot process, the action
    defined by audit failure flag is taken.
  id: xccdf_org.ssgproject.content_rule_coreos_audit_backlog_limit_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    audit=1, then auditing is enabled at boot time.

    To ensure audit_backlog_limit=8192 is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-audit-backlog-limit-kernel-argument
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-coreos-audit-backlog-limit-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40955"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-audit-backlog-limit-kernel-argument
    uid: ec1d8cb3-352f-4d40-81a4-2ccf8e0cc6c0
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Auditing for Processes Which Start Prior to the Audit Daemon
    Each process on the system carries an "auditable" flag which indicates whether
    its activities can be audited. Although auditd takes care of enabling
    this for all processes which launch after it does, adding the kernel argument
    ensures it is set for every process during boot.
  id: xccdf_org.ssgproject.content_rule_coreos_audit_option
  instructions: |-
    Inspect the form of BLS (Boot Loader Specification) options lines for the Linux operating system
    in /boot/loader/entries/*.conf. If they include audit=1, then auditing
    is enabled at boot time.
    # grep 'options.*audit=1.*' /boot/loader/entires/*.conf
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-audit-option
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-coreos-audit-option
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40719"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-audit-option
    uid: a0401456-efba-4de4-ac90-af1d22386d3f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify that Interactive Boot is Disabled
    Using interactive boot, the console user could disable auditing, firewalls,
    or other services, weakening system security.
  id: xccdf_org.ssgproject.content_rule_coreos_disable_interactive_boot
  instructions: |-
    Inspect /proc/cmdline for any instances of
    systemd.confirm_spawn=(1|yes|true|on) in the kernel boot arguments.
    Presence of a systemd.confirm_spawn=(1|yes|true|on) indicates
    that interactive boot is enabled at boot time.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-disable-interactive-boot
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-coreos-disable-interactive-boot
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41287"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-disable-interactive-boot
    uid: 319e5f5d-803e-46ef-b131-e94dd1659b58
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure SELinux Not Disabled in the kernel arguments
    Disabling a major host protection feature, such as SELinux, at boot time prevents
    it from confining system services at boot time.  Further, it increases
    the chances that it will remain off during system operation.
  id: xccdf_org.ssgproject.content_rule_coreos_enable_selinux_kernel_argument
  instructions: |-
    Inspect /proc/cmdline for any instances of selinux=0
    in the kernel boot arguments.  Presence of selinux=0 indicates
    that SELinux is disabled at boot time.

    If it would be disabled anywhere, make sure to enable it via a
    MachineConfig object.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-enable-selinux-kernel-argument
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-coreos-enable-selinux-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40712"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-enable-selinux-kernel-argument
    uid: 8c3c0808-078c-48cd-8379-0e7a78f31094
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Support for USB via Bootloader Configuration
    Disabling the USB subsystem within the Linux kernel at system boot will
    protect against potentially malicious USB devices, although it is only practical
    in specialized systems.
  id: xccdf_org.ssgproject.content_rule_coreos_nousb_kernel_argument
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-nousb-kernel-argument
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-coreos-nousb-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41093"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-nousb-kernel-argument
    uid: 18cc1258-80d8-41e3-894c-a664b57b2586
  severity: medium
  status: FAIL
  warnings:
  - Disabling all kernel support for USB will cause problems for systems with USB-based keyboards, mice, or printers. This configuration is infeasible for systems which require USB devices, which is common.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable page allocator poisoning
    Poisoning writes an arbitrary value to freed pages, so any modification or
    reference to that page after being freed or before being initialized will be
    detected and prevented.
    This prevents many types of use-after-free vulnerabilities at little performance cost.
    Also prevents leak of data and detection of corrupted memory.
  id: xccdf_org.ssgproject.content_rule_coreos_page_poison_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    page_poison=1, then page poisoning is enabled at boot time.

    To ensure page_poison=1 is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-page-poison-kernel-argument
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-coreos-page-poison-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40416"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-page-poison-kernel-argument
    uid: 7a257446-8a70-4e0e-8af4-251f545d3d22
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Page-Table Isolation (KPTI)
    Kernel page-table isolation is a kernel feature that mitigates
    the Meltdown security vulnerability and hardens the kernel
    against attempts to bypass kernel address space layout
    randomization (KASLR).
  id: xccdf_org.ssgproject.content_rule_coreos_pti_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    pti=on, then Kernel page-table isolation is enabled at boot time.

    To ensure pti=on is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-pti-kernel-argument
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-coreos-pti-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41258"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-pti-kernel-argument
    uid: 24988bc8-9da3-402a-8889-2504460d2260
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable vsyscalls
    Virtual Syscalls provide an opportunity of attack for a user who has control
    of the return instruction pointer.
  id: xccdf_org.ssgproject.content_rule_coreos_vsyscall_kernel_argument
  instructions: |-
    Inspect the form of all the BLS (Boot Loader Specification) entries
    ('options' line) in /boot/loader/entries/*.conf. If they include
    vsyscall=none, then virtual syscalls are not enabled at boot time.

    To ensure vsyscall=none is configured on the installed kernel, add
    the kernel argument via a MachineConfig object to the appropriate
    pools.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: coreos-vsyscall-kernel-argument
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: INFO
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-coreos-vsyscall-kernel-argument
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40477"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-coreos-vsyscall-kernel-argument
    uid: ccea21e3-e4a0-44bd-a72b-64c76fcb64f3
  severity: medium
  status: INFO
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Record Access Events to Audit Log Directory
    Attempts to read the logs should be recorded, suspicious access to audit log files could be an indicator of malicious activity on a system.
    Auditing these events could serve as evidence of potential system compromise.'
  id: xccdf_org.ssgproject.content_rule_directory_access_var_log_audit
  instructions: |-
    To determine if the system is configured to audit accesses to
    /var/log/audit directory, run the following command:
    preserve$ sudo grep "dir=/var/log/audit" /etc/audit/audit.rules
    If the system is configured to audit this activity, it will return a line.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: directory-access-var-log-audit
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-directory-access-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40972"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-directory-access-var-log-audit
    uid: 24dc0bda-eedc-4e62-923b-ce74e82bc161
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    System Audit Logs Must Have Mode 0750 or Less Permissive
    If users can write to audit logs, audit trails can be modified or destroyed.
  id: xccdf_org.ssgproject.content_rule_directory_permissions_var_log_audit
  instructions: |-
    Run the following command to check the mode of the system audit logs:
    $ sudo ls -ld /var/log/audit
    Audit log directories must be mode 0700 or less permissive.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: directory-permissions-var-log-audit
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-directory-permissions-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40794"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-directory-permissions-var-log-audit
    uid: 1d736166-125b-4fea-ac2f-ab56f785a7c6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Ctrl-Alt-Del Burst Action
    A locally logged-in user who presses Ctrl-Alt-Del, when at the console,
    can reboot the system. If accidentally pressed, as could happen in
    the case of mixed OS environment, this can create the risk of short-term
    loss of availability of systems due to unintentional reboot.
  id: xccdf_org.ssgproject.content_rule_disable_ctrlaltdel_burstaction
  instructions: |-
    To ensure the system is configured to ignore the Ctrl-Alt-Del setting,
    enter the following command:
    $ sudo grep -i ctrlaltdelburstaction /etc/systemd/system.conf
    The output should return:
    CtrlAltDelBurstAction=none
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: disable-ctrlaltdel-burstaction
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-disable-ctrlaltdel-burstaction
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40641"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-disable-ctrlaltdel-burstaction
    uid: d2ac2a60-1b75-4ffc-a438-b3eb28aeb187
  severity: high
  status: FAIL
  warnings:
  - Disabling the Ctrl-Alt-Del key sequence in /etc/init/control-alt-delete.conf DOES NOT disable the Ctrl-Alt-Del key sequence if running in runlevel 6 (e.g. in GNOME, KDE, etc.)! The Ctrl-Alt-Del key sequence will only be disabled if running in the non-graphical runlevel 3.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Ctrl-Alt-Del Reboot Activation
    A locally logged-in user who presses Ctrl-Alt-Del, when at the console,
    can reboot the system. If accidentally pressed, as could happen in
    the case of mixed OS environment, this can create the risk of short-term
    loss of availability of systems due to unintentional reboot.
  id: xccdf_org.ssgproject.content_rule_disable_ctrlaltdel_reboot
  instructions: |-
    To ensure the system is configured to mask the Ctrl-Alt-Del sequence, Check
    that the ctrl-alt-del.target is masked and not active with the following
    command:
    sudo systemctl status ctrl-alt-del.target
    The output should indicate that the target is masked and not active. It
    might resemble following output:
    ctrl-alt-del.target
    Loaded: masked (/dev/null; bad)
    Active: inactive (dead)
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: disable-ctrlaltdel-reboot
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-disable-ctrlaltdel-reboot
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40391"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-disable-ctrlaltdel-reboot
    uid: 7c07b9e9-ee4f-4f82-98da-d86692ca15d5
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Core Dumps for All Users
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data and is generally useful
    only for developers trying to debug problems.
  id: xccdf_org.ssgproject.content_rule_disable_users_coredumps
  instructions: |-
    To verify that core dumps are disabled for all users, run the following command:
    $ grep core /etc/security/limits.conf
    The output should be:
    *     hard   core    0
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: disable-users-coredumps
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-disable-users-coredumps
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41474"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-disable-users-coredumps
    uid: 29348221-f1e4-47d7-9b69-060234213a72
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable FIPS Mode
    Use of weak or untested encryption algorithms undermines the purposes of utilizing encryption to
    protect data. The operating system must implement cryptographic modules adhering to the higher
    standards approved by the federal government since this provides assurance they have been tested
    and validated.
  id: xccdf_org.ssgproject.content_rule_enable_fips_mode
  instructions: |-
    To verify that FIPS is enabled properly, run the following command:
    fips-mode-setup --check
    The output should contain the following:
    FIPS mode is enabled.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: enable-fips-mode
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-enable-fips-mode
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41166"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-enable-fips-mode
    uid: e5421fed-4564-41e8-9924-bed871fba9e0
  severity: high
  status: FAIL
  warnings:
  - The system needs to be rebooted for these changes to take effect.
  - System Crypto Modules must be provided by a vendor that undergoes FIPS-140 certifications. FIPS-140 is applicable to all Federal agencies that use cryptographic-based security systems to protect sensitive information in computer and telecommunication systems (including voice systems) as defined in Section 5131 of the Information Technology Management Reform Act of 1996, Public Law 104-106. This standard shall be used in designing and implementing cryptographic modules that Federal departments and agencies operate or are operated for them under contract. See *https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.140-2.pdf* To meet this, the system has to have cryptographic software provided by a vendor that has undergone this certification. This means providing documentation, test results, design information, and independent third party review by an accredited lab. While open source software is capable of meeting this, it does not meet FIPS-140 unless the vendor submits to this process.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure Logrotate Runs Periodically
    Log files that are not properly rotated run the risk of growing so large
    that they fill up the /var/log partition. Valuable logging information could be lost
    if the /var/log partition becomes full.
  id: xccdf_org.ssgproject.content_rule_ensure_logrotate_activated
  instructions: |-
    To determine the status and frequency of logrotate, run the following command:
    $ sudo grep logrotate /var/log/cron*
    If logrotate is configured properly, output should include references to
    /etc/cron.daily.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: ensure-logrotate-activated
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-ensure-logrotate-activated
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40831"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-ensure-logrotate-activated
    uid: 86f5685c-dca6-45d2-a894-d259dfb8d63e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Group Who Owns SSH Server config file
    Service configuration files enable or disable features of their respective
    services that if configured incorrectly can lead to insecure and vulnerable
    configurations. Therefore, service configuration files should be owned by the
    correct group to prevent unauthorized changes.
  id: xccdf_org.ssgproject.content_rule_file_groupowner_sshd_config
  instructions: |-
    To check the group ownership of /etc/ssh/sshd_config,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/ssh/sshd_config
    If properly configured, the output should indicate the following group-owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-groupowner-sshd-config
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-file-groupowner-sshd-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41394"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-groupowner-sshd-config
    uid: 87385215-2c51-458d-958c-8363e3684f01
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Owner on SSH Server config file
    Service configuration files enable or disable features of their respective
    services that if configured incorrectly can lead to insecure and vulnerable
    configurations. Therefore, service configuration files should be owned by the
    correct group to prevent unauthorized changes.
  id: xccdf_org.ssgproject.content_rule_file_owner_sshd_config
  instructions: |-
    To check the ownership of /etc/ssh/sshd_config,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -lL /etc/ssh/sshd_config
    If properly configured, the output should indicate the following owner:
    root
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-owner-sshd-config
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-file-owner-sshd-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41109"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-owner-sshd-config
    uid: 3294a1d6-a24a-49f0-894a-c0107b9e8e2b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    System Audit Logs Must Be Owned By Root
    Unauthorized disclosure of audit records can reveal system and configuration data to
    attackers, thus compromising its confidentiality.
  id: xccdf_org.ssgproject.content_rule_file_ownership_var_log_audit
  instructions: "To properly set the owner of /var/log/audit, run the command:\n$ sudo chown root /var/log/audit \n\nTo properly set the owner of /var/log/audit/*, run the command:\n$ sudo chown root /var/log/audit/*"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-ownership-var-log-audit
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-file-ownership-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40961"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-ownership-var-log-audit
    uid: bdfb5351-ab10-4dc9-a525-10cf8214fff2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on SSH Server config file
    Service configuration files enable or disable features of their respective
    services that if configured incorrectly can lead to insecure and vulnerable
    configurations. Therefore, service configuration files should be owned by the
    correct group to prevent unauthorized changes.
  id: xccdf_org.ssgproject.content_rule_file_permissions_sshd_config
  instructions: |-
    To check the permissions of /etc/ssh/sshd_config,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/ssh/sshd_config
    If properly configured, the output should indicate the following permissions:
    -rw-------
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-sshd-config
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-file-permissions-sshd-config
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41363"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-permissions-sshd-config
    uid: 32228969-08ba-420b-a181-7f7c601142b6
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on SSH Server Private *_key Key Files
    If an unauthorized user obtains the private SSH host key file, the host could be
    impersonated.
  id: xccdf_org.ssgproject.content_rule_file_permissions_sshd_private_key
  instructions: |-
    To check the permissions of /etc/ssh/*_key,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/ssh/*_key
    If properly configured, the output should indicate the following permissions:
    -rw-r-----
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-sshd-private-key
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-file-permissions-sshd-private-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41007"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-permissions-sshd-private-key
    uid: ca3b7dcd-9c96-4a23-8cf5-c198abb58598
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Verify Permissions on SSH Server Public *.pub Key Files
    If a public host key file is modified by an unauthorized user, the SSH service
    may be compromised.
  id: xccdf_org.ssgproject.content_rule_file_permissions_sshd_pub_key
  instructions: |-
    To check the permissions of /etc/ssh/*.pub,
    you'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Then,run the command:
    $ ls -l /etc/ssh/*.pub
    If properly configured, the output should indicate the following permissions:
    -rw-r--r--
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-sshd-pub-key
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-file-permissions-sshd-pub-key
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40635"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-permissions-sshd-pub-key
    uid: 73cca835-dfca-46a8-a832-5bf87da4bd75
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    System Audit Logs Must Have Mode 0640 or Less Permissive
    If users can write to audit logs, audit trails can be modified or destroyed.
  id: xccdf_org.ssgproject.content_rule_file_permissions_var_log_audit
  instructions: |-
    Run the following command to check the mode of the system audit logs:
    $ sudo ls -l /var/log/audit
    Audit logs must be mode 0640 or less permissive.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: file-permissions-var-log-audit
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-file-permissions-var-log-audit
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41239"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-file-permissions-var-log-audit
    uid: f7b776ca-4847-45d4-8b96-689642c83874
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable ATM Support
    Disabling ATM protects the system against exploitation of any
    flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_atm_disabled
  instructions: |-
    If the system is configured to prevent the loading of the atm kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r atm /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-atm-disabled
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-kernel-module-atm-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40388"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-atm-disabled
    uid: d4f3efe8-5383-4066-920f-c21e3d732f2b
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Bluetooth Kernel Module
    If Bluetooth functionality must be disabled, preventing the kernel
    from loading the kernel module provides an additional safeguard against its
    activation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_bluetooth_disabled
  instructions: |-
    If the system is configured to prevent the loading of the bluetooth kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r bluetooth /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-bluetooth-disabled
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-kernel-module-bluetooth-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41405"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-bluetooth-disabled
    uid: 45fa607f-debb-4f87-9a06-cf52b890ae0a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable CAN Support
    Disabling CAN protects the system against exploitation of any
    flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_can_disabled
  instructions: |-
    If the system is configured to prevent the loading of the can kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r can /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-can-disabled
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-kernel-module-can-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40536"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-can-disabled
    uid: 2712af7c-d64d-46e0-9f7c-1a6ea8e9bc7e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of cramfs
    Removing support for unneeded filesystem types reduces the local attack surface
    of the server.
  id: xccdf_org.ssgproject.content_rule_kernel_module_cramfs_disabled
  instructions: |-
    If the system is configured to prevent the loading of the cramfs kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r cramfs /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-cramfs-disabled
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-kernel-module-cramfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41381"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-cramfs-disabled
    uid: 4bbaff31-01ff-4f5f-905c-8461dd51b9d2
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable IEEE 1394 (FireWire) Support
    Disabling FireWire protects the system against exploitation of any
    flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_firewire-core_disabled
  instructions: |-
    If the system is configured to prevent the loading of the firewire-core kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r firewire-core /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-firewire-core-disabled
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-kernel-module-firewire-core-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40400"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-firewire-core-disabled
    uid: c02dd29d-ddcd-4172-ae1b-6562d030b75f
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of freevxfs
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_freevxfs_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-freevxfs-disabled
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-kernel-module-freevxfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41360"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-freevxfs-disabled
    uid: d68b4bbe-d4fb-4b64-8609-52f7d05d2f66
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of hfs
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_hfs_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-hfs-disabled
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-kernel-module-hfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41059"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-hfs-disabled
    uid: 750097e1-a753-4d5c-977d-117430468561
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of hfsplus
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_hfsplus_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-hfsplus-disabled
    creationTimestamp: "2021-07-13T16:59:03Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:03Z"
    name: rhcos4-moderate-worker-kernel-module-hfsplus-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40857"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-hfsplus-disabled
    uid: 99a66c14-786b-4917-bd07-9b1630db4fb4
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of jffs2
    Linux kernel modules which implement filesystems that are not needed by the
    local system should be disabled.
  id: xccdf_org.ssgproject.content_rule_kernel_module_jffs2_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-jffs2-disabled
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-kernel-module-jffs2-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41020"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-jffs2-disabled
    uid: 543c84f0-b7bf-4b89-8b24-1ccbac216e06
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable SCTP Support
    Disabling SCTP protects
    the system against exploitation of any flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_sctp_disabled
  instructions: |-
    If the system is configured to prevent the loading of the sctp kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r sctp /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-sctp-disabled
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-kernel-module-sctp-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41226"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-sctp-disabled
    uid: d4c1ea07-345f-4a5d-ae47-27d3e8aa07e7
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of squashfs
    Removing support for unneeded filesystem types reduces the local attack
    surface of the system.
  id: xccdf_org.ssgproject.content_rule_kernel_module_squashfs_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-squashfs-disabled
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-kernel-module-squashfs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41433"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-squashfs-disabled
    uid: 48b8c720-64d4-41ca-943b-2d4e80ff8b46
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable TIPC Support
    Disabling TIPC protects
    the system against exploitation of any flaws in its implementation.
  id: xccdf_org.ssgproject.content_rule_kernel_module_tipc_disabled
  instructions: |-
    If the system is configured to prevent the loading of the tipc kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r tipc /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-tipc-disabled
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-kernel-module-tipc-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40446"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-tipc-disabled
    uid: 20a4b9d0-cadf-4dcf-b341-406294152c7f
  severity: medium
  status: FAIL
  warnings:
  - This configuration baseline was created to deploy the base operating system for general purpose workloads. When the operating system is configured for certain purposes, such as a node in High Performance Computing cluster, it is expected that the tipc kernel module will be loaded.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Mounting of udf
    Removing support for unneeded filesystem types reduces the local
    attack surface of the system.
  id: xccdf_org.ssgproject.content_rule_kernel_module_udf_disabled
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-udf-disabled
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: low
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-kernel-module-udf-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41128"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-udf-disabled
    uid: 15742dc2-445a-49fb-b4f2-edfc3c9d2647
  severity: low
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Modprobe Loading of USB Storage Driver
    USB storage devices such as thumb drives can be used to introduce
    malicious software.
  id: xccdf_org.ssgproject.content_rule_kernel_module_usb-storage_disabled
  instructions: |-
    If the system is configured to prevent the loading of the usb-storage kernel module,
    it will contain lines inside any file in /etc/modprobe.d or the deprecated/etc/modprobe.conf.
    These lines instruct the module loading system to run another program (such as /bin/true) upon a module install event.
    Run the following command to search for such lines in all files in /etc/modprobe.d and the deprecated /etc/modprobe.conf:
    $ grep -r usb-storage /etc/modprobe.conf /etc/modprobe.d
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: kernel-module-usb-storage-disabled
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-kernel-module-usb-storage-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40468"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-kernel-module-usb-storage-disabled
    uid: b5900aab-31ab-485c-9dbd-c8d509029831
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Direct root Logins Not Allowed
    Disabling direct root logins ensures proper accountability and multifactor
    authentication to privileged accounts. Users will first login, then escalate
    to privileged (root) access via su / sudo. This is required for FISMA Low
    and FISMA Moderate systems.
  id: xccdf_org.ssgproject.content_rule_no_direct_root_logins
  instructions: |-
    To ensure root may not directly login to the system over physical consoles,
    run the following command:
    cat /etc/securetty
    If any output is returned, this is a finding.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-direct-root-logins
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-no-direct-root-logins
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41172"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-no-direct-root-logins
    uid: c2d6e7e1-aa4f-4634-9c04-378525d95a60
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Prevent Login to Accounts With Empty Password
    If an account has an empty password, anyone could log in and
    run commands with the privileges of that account. Accounts with
    empty passwords should never be used in operational environments.
  id: xccdf_org.ssgproject.content_rule_no_empty_passwords
  instructions: |-
    To verify that null passwords cannot be used, run the following command:

    $ grep nullok /etc/pam.d/system-auth

    If this produces any output, it may be possible to log into accounts
    with empty passwords. Remove any instances of the nullok option to
    prevent logins with empty passwords.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-empty-passwords
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: high
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-no-empty-passwords
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40541"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-no-empty-passwords
    uid: 749950f4-5533-4531-bc0b-d9966445c24e
  severity: high
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: "Verify No netrc Files Exist\nUnencrypted passwords for remote FTP servers may be stored in .netrc\nfiles. "
  id: xccdf_org.ssgproject.content_rule_no_netrc_files
  instructions: |-
    To check the system for the existence of any .netrc files,
    run the following command:
    $ sudo find /home -xdev -name .netrc
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-netrc-files
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-no-netrc-files
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40790"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-no-netrc-files
    uid: e93ed566-d1a6-4430-8c6e-0bf94b7a5d1b
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure that System Accounts Do Not Run a Shell Upon Login
    Ensuring shells are not given to system accounts upon login makes it more
    difficult for attackers to make use of system accounts.
  id: xccdf_org.ssgproject.content_rule_no_shelllogin_for_systemaccounts
  instructions: |-
    To obtain a listing of all users, their UIDs, and their shells, run the
    command: $ awk -F: '{print $1 ":" $3 ":" $7}' /etc/passwd Identify
    the system accounts from this listing. These will primarily be the accounts
    with UID numbers less than UID_MIN, other than root. Value of the UID_MIN
    directive is set in /etc/login.defs configuration file. In the default
    configuration UID_MIN is set to 1000.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-shelllogin-for-systemaccounts
    creationTimestamp: "2021-07-13T16:59:16Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:16Z"
    name: rhcos4-moderate-worker-no-shelllogin-for-systemaccounts
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41452"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-no-shelllogin-for-systemaccounts
    uid: eeca7aff-c8f3-4f8f-92fd-2755f16e54b3
  severity: medium
  status: PASS
  warnings:
  - Do not perform the steps in this section on the root account. Doing so might cause the system to become inaccessible.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Prevent user from disabling the screen lock
    Not listing tmux among permitted shells
    prevents malicious program running as user
    from lowering security by disabling the screen lock.
  id: xccdf_org.ssgproject.content_rule_no_tmux_in_shells
  instructions: |-
    To verify that tmux is not listed as allowed shell on the system
    run the following command:
    $ grep 'tmux$' /etc/shells
    The output should be empty.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: no-tmux-in-shells
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-no-tmux-in-shells
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41460"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-no-tmux-in-shells
    uid: 0d1871cb-6d5c-4cb5-a3cf-023a9ee0f871
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure the audit Subsystem is Installed
    The auditd service is an access monitoring and accounting daemon, watching system calls to audit any access, in comparison with potential local access control policy such as SELinux policy.
  id: xccdf_org.ssgproject.content_rule_package_audit_installed
  instructions: Is it the case that the package is not installed?
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-audit-installed
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-package-audit-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41318"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-package-audit-installed
    uid: 81d47c0d-7c85-4c43-8e20-c45568e55b6a
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Install iptables Package
    iptables controls the Linux kernel network packet filtering
    code. iptables allows system operators to set up firewalls and IP
    masquerading, etc.
  id: xccdf_org.ssgproject.content_rule_package_iptables_installed
  instructions: 'Run the following command to determine if the iptables package is installed: $ rpm -q iptables'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-iptables-installed
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-package-iptables-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40576"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-package-iptables-installed
    uid: 38f7473d-29c3-47a0-8be4-b57c1164e718
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Install sudo Package
    sudo is a program designed to allow a system administrator to give
    limited root privileges to users and log root activity. The basic philosophy
    is to give as few privileges as possible but still allow system users to
    get their work done.
  id: xccdf_org.ssgproject.content_rule_package_sudo_installed
  instructions: 'Run the following command to determine if the sudo package is installed: $ rpm -q sudo'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-sudo-installed
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-package-sudo-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41247"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-package-sudo-installed
    uid: 268893f4-9c8f-4ee5-86e0-73566c61c2d5
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Install usbguard Package
    usbguard is a software framework that helps to protect
    against rogue USB devices by implementing basic whitelisting/blacklisting
    capabilities based on USB device attributes.
  id: xccdf_org.ssgproject.content_rule_package_usbguard_installed
  instructions: 'Run the following command to determine if the usbguard package is installed: $ rpm -q usbguard'
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: package-usbguard-installed
    creationTimestamp: "2021-07-13T16:59:08Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:08Z"
    name: rhcos4-moderate-worker-package-usbguard-installed
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41104"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-package-usbguard-installed
    uid: af5d2d04-5bc4-4e01-a520-fafd33b3c41c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Require Authentication for Single User Mode
    This prevents attackers with physical access from trivially bypassing security
    on the machine and gaining root access. Such accesses are further prevented
    by configuring the bootloader password.
  id: xccdf_org.ssgproject.content_rule_require_singleuser_auth
  instructions: |-
    To check if authentication is required for single-user mode, run the following command:
    $ grep sulogin /usr/lib/systemd/system/rescue.service
    The output should be similar to the following, and the line must begin with
    ExecStart and /usr/lib/systemd/systemd-sulogin-shell.
        ExecStart=-/usr/lib/systemd/systemd-sulogin-shell rescue
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: require-singleuser-auth
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-require-singleuser-auth
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40424"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-require-singleuser-auth
    uid: 31fa1e87-9b2d-4e35-aeea-a1fcf5dcbaf4
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure No Daemons are Unconfined by SELinux
    Daemons which run with the unconfined_service_t context may cause AVC denials,
    or allow privileges that the daemon does not require.
  id: xccdf_org.ssgproject.content_rule_selinux_confinement_of_daemons
  instructions: |-
    Ensure there are no unconfined daemons running on the system,
    the following command should produce no output:
    $ sudo ps -eZ | grep "unconfined_service_t"
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: selinux-confinement-of-daemons
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-selinux-confinement-of-daemons
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40932"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-selinux-confinement-of-daemons
    uid: b0d76c09-1108-45de-a351-c841c6f81c85
  severity: medium
  status: FAIL
  warnings:
  - Automatic remediation of this control is not available. Remediation can be achieved by amending SELinux policy or stopping the unconfined daemons as outlined above.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure SELinux Policy
    Setting the SELinux policy to targeted or a more specialized policy
    ensures the system will confine processes that are likely to be
    targeted for exploitation, such as network or system services.

    Note: During the development or debugging of SELinux modules, it is common to
    temporarily place non-production systems in permissive mode. In such
    temporary cases, SELinux policies should be developed, and once work
    is completed, the system should be reconfigured to
    .
  id: xccdf_org.ssgproject.content_rule_selinux_policytype
  instructions: |-
    Check the file /etc/selinux/config and ensure the following line appears:
    SELINUXTYPE=
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: selinux-policytype
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-selinux-policytype
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41030"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-selinux-policytype
    uid: 8ffec01c-3622-400f-9e27-fb6db5a09949
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Ensure SELinux State is Enforcing
    Setting the SELinux state to enforcing ensures SELinux is able to confine
    potentially compromised processes to the security policy, which is designed to
    prevent them from causing damage to the system or further elevating their
    privileges.
  id: xccdf_org.ssgproject.content_rule_selinux_state
  instructions: |-
    Check the file /etc/selinux/config and ensure the following line appears:
    SELINUX=
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: selinux-state
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-selinux-state
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41156"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-selinux-state
    uid: fb9e6582-77fd-4bd3-8dbc-9efa9b469995
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable auditd Service
    Without establishing what type of events occurred, it would be difficult
    to establish, correlate, and investigate the events leading up to an outage or attack.
    Ensuring the auditd service is active ensures audit records
    generated by the kernel are appropriately recorded.

    Additionally, a properly configured audit subsystem ensures that actions of
    individual system users can be uniquely traced to those users so they
    can be held accountable for their actions.
  id: xccdf_org.ssgproject.content_rule_service_auditd_enabled
  instructions: |-
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    auditd service:
    $ systemctl is-active auditd
    If the service is running, it should return the following: active
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-auditd-enabled
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-service-auditd-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40753"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-auditd-enabled
    uid: cb305085-db3a-4ae0-bf5c-3a1827d2cab0
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable the Automounter
    Disabling the automounter permits the administrator to
    statically control filesystem mounting through /etc/fstab.

    Additionally, automatically mounting filesystems permits easy introduction of
    unknown devices, thereby facilitating malicious activity.
  id: xccdf_org.ssgproject.content_rule_service_autofs_disabled
  instructions: |-
    To check that the autofs service is disabled in system boot configuration,
    You'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Subsequently,run the following command:
    $ systemctl is-enabled autofs
    Output should indicate the autofs service has either not been installed,
    or has been disabled at all runlevels, as shown in the example below:
    $ systemctl is-enabled autofs disabled

    Run the following command to verify autofs is not active (i.e. not running) through current runtime configuration:
    $ systemctl is-active autofs

    If the service is not running the command will return the following output:
    inactive

    The service will also be masked, to check that the autofs is masked, run the following command:
    $ systemctl show autofs | grep "LoadState\|UnitFileState"

    If the service is masked the command will return the following outputs:

    LoadState=masked

    UnitFileState=masked
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-autofs-disabled
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-service-autofs-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40596"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-autofs-disabled
    uid: 21251010-7b98-4efe-9e70-980f9b7553dc
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Bluetooth Service
    Disabling the bluetooth service prevents the system from attempting
    connections to Bluetooth devices, which entails some security risk.
    Nevertheless, variation in this risk decision may be expected due to the
    utility of Bluetooth connectivity and its limited range.
  id: xccdf_org.ssgproject.content_rule_service_bluetooth_disabled
  instructions: |-
    To check that the bluetooth service is disabled in system boot configuration,
    You'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Subsequently,run the following command:
    $ systemctl is-enabled bluetooth
    Output should indicate the bluetooth service has either not been installed,
    or has been disabled at all runlevels, as shown in the example below:
    $ systemctl is-enabled bluetooth disabled

    Run the following command to verify bluetooth is not active (i.e. not running) through current runtime configuration:
    $ systemctl is-active bluetooth

    If the service is not running the command will return the following output:
    inactive

    The service will also be masked, to check that the bluetooth is masked, run the following command:
    $ systemctl show bluetooth | grep "LoadState\|UnitFileState"

    If the service is masked the command will return the following outputs:

    LoadState=masked

    UnitFileState=masked
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-bluetooth-disabled
    creationTimestamp: "2021-07-13T16:58:58Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:58Z"
    name: rhcos4-moderate-worker-service-bluetooth-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40531"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-bluetooth-disabled
    uid: ff06c751-39ff-42fe-ade5-99db32595734
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the NTP Daemon
    Enabling some of chronyd or ntpd services ensures
    that the NTP daemon will be running and that the system will synchronize its
    time to any servers specified. This is important whether the system is
    configured to be a client (and synchronize only its own clock) or it is also
    acting as an NTP server to other systems.  Synchronizing time is essential for
    authentication services such as Kerberos, but it is also important for
    maintaining accurate logs and auditing possible security breaches.

    The chronyd and ntpd NTP daemons offer all of the
    functionality of ntpdate, which is now deprecated. Additional
    information on this is available at

        http://support.ntp.org/bin/view/Dev/DeprecatingNtpdate
  id: xccdf_org.ssgproject.content_rule_service_chronyd_or_ntpd_enabled
  instructions: |-
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    chronyd service:
    $ systemctl is-active chronyd
    If the service is running, it should return the following: active

    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    ntpd service:
    $ systemctl is-active ntpd
    If the service is running, it should return the following: active
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-chronyd-or-ntpd-enabled
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-service-chronyd-or-ntpd-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41089"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-chronyd-or-ntpd-enabled
    uid: a75bfea9-4b6f-498a-97f8-8d09cf324885
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable debug-shell SystemD Service
    This prevents attackers with physical access from trivially bypassing security
    on the machine through valid troubleshooting configurations and gaining root
    access when the system is rebooted.
  id: xccdf_org.ssgproject.content_rule_service_debug-shell_disabled
  instructions: |-
    To check that the debug-shell service is disabled in system boot configuration,
    You'll need to log into a node in the cluster.
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Subsequently,run the following command:
    $ systemctl is-enabled debug-shell
    Output should indicate the debug-shell service has either not been installed,
    or has been disabled at all runlevels, as shown in the example below:
    $ systemctl is-enabled debug-shell disabled

    Run the following command to verify debug-shell is not active (i.e. not running) through current runtime configuration:
    $ systemctl is-active debug-shell

    If the service is not running the command will return the following output:
    inactive

    The service will also be masked, to check that the debug-shell is masked, run the following command:
    $ systemctl show debug-shell | grep "LoadState\|UnitFileState"

    If the service is masked the command will return the following outputs:

    LoadState=masked

    UnitFileState=masked
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-debug-shell-disabled
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-service-debug-shell-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41267"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-debug-shell-disabled
    uid: ae794a2d-46c1-4db7-ba93-f87f87b8f500
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable acquiring, saving, and processing core dumps
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data
    and is generally useful only for developers trying to debug problems.
  id: xccdf_org.ssgproject.content_rule_service_systemd-coredump_disabled
  instructions: |-
    To verify that acquiring, saving, and processing core dumps is disabled, run the
    following command:
    $ systemctl status systemd-coredump.socket
    The output should be similar to:
    â— systemd-coredump.socket
       Loaded: masked (Reason: Unit systemd-coredump.socket is masked.)
       Active: inactive (dead) ...
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-systemd-coredump-disabled
    creationTimestamp: "2021-07-13T16:59:11Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:11Z"
    name: rhcos4-moderate-worker-service-systemd-coredump-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41233"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-systemd-coredump-disabled
    uid: dc11d081-eff4-4862-9814-05f6622851ee
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable the USBGuard Service
    The usbguard service must be running in order to
    enforce the USB device authorization policy for all USB devices.
  id: xccdf_org.ssgproject.content_rule_service_usbguard_enabled
  instructions: |-
    As a user with administrator privileges, log into a node in the relevant pool:

    $ oc debug node/$NODE_NAME

    At the sh-4.4# prompt, run:

    # chroot /host


    Run the following command to determine the current status of the
    usbguard service:
    $ systemctl is-active usbguard
    If the service is running, it should return the following: active
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: service-usbguard-enabled
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-service-usbguard-enabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40922"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-service-usbguard-enabled
    uid: d82da6bd-6740-4b8d-a58c-7370bdfddfb2
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable SSH Support for .rhosts Files
    SSH trust relationships mean a compromise on one host
    can allow an attacker to move trivially to other hosts.
  id: xccdf_org.ssgproject.content_rule_sshd_disable_rhosts
  instructions: |-
    To determine how the SSH daemon's IgnoreRhosts option is set, run the following command:
    $ sudo grep -i IgnoreRhosts /etc/ssh/sshd_config
    If no line, a commented line, or a line indicating the value yes is returned, then the required value is set.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-disable-rhosts
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-sshd-disable-rhosts
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41284"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sshd-disable-rhosts
    uid: 2dc25bcb-9035-4e5d-95dc-a8ba3f39b1f9
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Limit Users' SSH Access
    Specifying which accounts are allowed SSH access into the system reduces the
    possibility of unauthorized access to the system.
  id: xccdf_org.ssgproject.content_rule_sshd_limit_user_access
  instructions: |-
    To ensure sshd limits the users who can log in, run the following:
    $ sudo grep AllowUsers /etc/ssh/sshd_config
    If properly configured, the output should be a list of usernames allowed to log in
    to this system.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-limit-user-access
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-sshd-limit-user-access
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40983"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sshd-limit-user-access
    uid: 908c6f55-d944-49e2-8f83-1f7f23128708
  severity: unknown
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set SSH Idle Timeout Interval
    Terminating an idle ssh session within a short time period reduces the window of
    opportunity for unauthorized personnel to take control of a management session
    enabled on the console or console port that has been let unattended.
  id: xccdf_org.ssgproject.content_rule_sshd_set_idle_timeout
  instructions: |-
    Run the following command to see what the timeout interval is:
    $ sudo grep ClientAliveInterval /etc/ssh/sshd_config
    If properly configured, the output should be:
    ClientAliveInterval
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-set-idle-timeout
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-sshd-set-idle-timeout
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40672"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sshd-set-idle-timeout
    uid: 3f462f52-dd89-4a04-b49e-25ea788968ca
  severity: medium
  status: FAIL
  warnings:
  - SSH disconnecting idle clients will not have desired effect without also configuring ClientAliveCountMax in the SSH service configuration.
  - |-
    Following conditions may prevent the SSH session to time out:

    * Remote processes on the remote machine generates output. As the output has to be transferred over the network to the client, the timeout is reset every time such transfer happens.
    * Any scp or sftp activity by the same user to the host resets the timeout.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Set SSH Client Alive Count Max to zero
    This ensures a user login will be terminated as soon as the ClientAliveInterval
    is reached.
  id: xccdf_org.ssgproject.content_rule_sshd_set_keepalive_0
  instructions: |-
    To ensure ClientAliveInterval is set correctly, run the following command:
    $ sudo grep ClientAliveCountMax /etc/ssh/sshd_config
    If properly configured, the output should be:
    ClientAliveCountMax 0

    In this case, the SSH idle timeout occurs precisely when
    the ClientAliveInterval is set.
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sshd-set-keepalive-0
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-sshd-set-keepalive-0
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40784"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sshd-set-keepalive-0
    uid: 4b7485e3-b724-40c6-a51f-e59375689cc5
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Enforce DAC on Hardlinks
    By enabling this kernel parameter, users can no longer create soft or hard links to
    files which they do not own. Disallowing such hardlinks mitigate vulnerabilities
    based on insecure file system accessed by privileged programs, avoiding an
    exploitation vector exploiting unsafe use of open() or creat().
  id: xccdf_org.ssgproject.content_rule_sysctl_fs_protected_hardlinks
  instructions: "The runtime status of the fs.protected_hardlinks kernel parameter can be queried\nby running the following command:\n$ sysctl fs.protected_hardlinks\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*fs.protected_hardlinks\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nfs.protected_hardlinks = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains fs.protected_hardlinks = 1, and that one assignment\nis returned when \n$ grep -r fs.protected_hardlinks /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-fs-protected-hardlinks
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-sysctl-fs-protected-hardlinks
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41354"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-fs-protected-hardlinks
    uid: 8d2fb008-7a7a-4782-b361-90dca21aae58
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Enforce DAC on Symlinks
    By enabling this kernel parameter, symbolic links are permitted to be followed
    only when outside a sticky world-writable directory, or when the UID of the
    link and follower match, or when the directory owner matches the symlink's owner.
    Disallowing such symlinks helps mitigate vulnerabilities based on insecure file system
    accessed by privileged programs, avoiding an exploitation vector exploiting unsafe use of
    open() or creat().
  id: xccdf_org.ssgproject.content_rule_sysctl_fs_protected_symlinks
  instructions: "The runtime status of the fs.protected_symlinks kernel parameter can be queried\nby running the following command:\n$ sysctl fs.protected_symlinks\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*fs.protected_symlinks\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nfs.protected_symlinks = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains fs.protected_symlinks = 1, and that one assignment\nis returned when \n$ grep -r fs.protected_symlinks /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-fs-protected-symlinks
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-sysctl-fs-protected-symlinks
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41159"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-fs-protected-symlinks
    uid: 4c38a9be-192c-4fbf-9310-67846e7c5898
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable storing core dumps
    A core dump includes a memory image taken at the time the operating system
    terminates an application. The memory image could contain sensitive data and is generally useful
    only for developers trying to debug problems.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_core_pattern
  instructions: "The runtime status of the kernel.core_pattern kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.core_pattern\nThe output of the command should indicate a value of |/bin/false.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.core_pattern\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.core_pattern = |/bin/false\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.core_pattern = |/bin/false, and that one assignment\nis returned when \n$ grep -r kernel.core_pattern /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-core-pattern
    creationTimestamp: "2021-07-13T16:59:02Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:02Z"
    name: rhcos4-moderate-worker-sysctl-kernel-core-pattern
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40778"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-core-pattern
    uid: 0e99b30d-daa6-4e23-bd39-779efed686d2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Restrict Access to Kernel Message Buffer
    Unprivileged access to the kernel syslog can expose sensitive kernel
    address information.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_dmesg_restrict
  instructions: "The runtime status of the kernel.dmesg_restrict kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.dmesg_restrict\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.dmesg_restrict\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.dmesg_restrict = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.dmesg_restrict = 1, and that one assignment\nis returned when \n$ grep -r kernel.dmesg_restrict /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-dmesg-restrict
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-sysctl-kernel-dmesg-restrict
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40614"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-dmesg-restrict
    uid: 353e5b84-939b-4a65-b149-8797a6d5e13e
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |
    Disable Kernel Image Loading
    Disabling kexec_load allows greater control of the kernel memory.
    It makes it impossible to load another kernel image after it has been disabled.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_kexec_load_disabled
  instructions: "The runtime status of the kernel.kexec_load_disabled kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.kexec_load_disabled\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.kexec_load_disabled\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.kexec_load_disabled = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.kexec_load_disabled = 1, and that one assignment\nis returned when \n$ grep -r kernel.kexec_load_disabled /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-kexec-load-disabled
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-sysctl-kernel-kexec-load-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41409"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-kexec-load-disabled
    uid: c23ddb7c-384e-40d7-b23e-8616f5688961
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Restrict Exposed Kernel Pointer Addresses Access
    Exposing kernel pointers (through procfs or seq_printf()) exposes
    kernel writeable structures that can contain functions pointers. If a write vulnereability occurs
    in the kernel allowing a write access to any of this structure, the kernel can be compromise. This
    option disallow any program withtout the CAP_SYSLOG capability from getting the kernel pointers addresses,
    replacing them with 0.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_kptr_restrict
  instructions: "The runtime status of the kernel.kptr_restrict kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.kptr_restrict\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.kptr_restrict\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.kptr_restrict = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.kptr_restrict = 1, and that one assignment\nis returned when \n$ grep -r kernel.kptr_restrict /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-kptr-restrict
    creationTimestamp: "2021-07-13T16:59:12Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:12Z"
    name: rhcos4-moderate-worker-sysctl-kernel-kptr-restrict
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41280"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-kptr-restrict
    uid: 3da7b046-2e9a-4a16-9286-b8b1982100f2
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disallow kernel profiling by unprivileged users
    Kernel profiling can reveal sensitive information about kernel behaviour.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_perf_event_paranoid
  instructions: "The runtime status of the kernel.perf_event_paranoid kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.perf_event_paranoid\nThe output of the command should indicate a value of 2.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.perf_event_paranoid\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.perf_event_paranoid = 2\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.perf_event_paranoid = 2, and that one assignment\nis returned when \n$ grep -r kernel.perf_event_paranoid /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-perf-event-paranoid
    creationTimestamp: "2021-07-13T16:59:15Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:15Z"
    name: rhcos4-moderate-worker-sysctl-kernel-perf-event-paranoid
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41413"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-perf-event-paranoid
    uid: 1f2c568b-5127-4e91-8092-008b2a71f67d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Access to Network bpf() Syscall From Unprivileged Processes
    Loading and accessing the packet filters programs and maps using the bpf()
    syscall has the potential of revealing sensitive information about the kernel state.
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_unprivileged_bpf_disabled
  instructions: "The runtime status of the kernel.unprivileged_bpf_disabled kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.unprivileged_bpf_disabled\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.unprivileged_bpf_disabled\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.unprivileged_bpf_disabled = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.unprivileged_bpf_disabled = 1, and that one assignment\nis returned when \n$ grep -r kernel.unprivileged_bpf_disabled /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-unprivileged-bpf-disabled
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-sysctl-kernel-unprivileged-bpf-disabled
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40996"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-unprivileged-bpf-disabled
    uid: 5a8329a6-f4ea-4d22-ab91-8dfd63c19524
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |
    Restrict usage of ptrace to descendant processes
    Unrestricted usage of ptrace allows compromised binaries to run ptrace
    on another processes of the user. Like this, the attacker can steal
    sensitive information from the target processes (e.g. SSH sessions, web browser, ...)
    without any additional assistance from the user (i.e. without resorting to phishing).
  id: xccdf_org.ssgproject.content_rule_sysctl_kernel_yama_ptrace_scope
  instructions: "The runtime status of the kernel.yama.ptrace_scope kernel parameter can be queried\nby running the following command:\n$ sysctl kernel.yama.ptrace_scope\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*kernel.yama.ptrace_scope\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nkernel.yama.ptrace_scope = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains kernel.yama.ptrace_scope = 1, and that one assignment\nis returned when \n$ grep -r kernel.yama.ptrace_scope /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-kernel-yama-ptrace-scope
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-sysctl-kernel-yama-ptrace-scope
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40747"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-kernel-yama-ptrace-scope
    uid: 11ad2c05-cc3a-4d4d-a7bd-4dd048992b1a
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Harden the operation of the BPF just-in-time compiler
    When hardened, the extended Berkeley Packet Filter just-in-time compiler
    will randomize any kernel addresses in the BPF programs and maps,
    and will not expose the JIT addresses in /proc/kallsyms.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_core_bpf_jit_harden
  instructions: "The runtime status of the net.core.bpf_jit_harden kernel parameter can be queried\nby running the following command:\n$ sysctl net.core.bpf_jit_harden\nThe output of the command should indicate a value of 2.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.core.bpf_jit_harden\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.core.bpf_jit_harden = 2\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.core.bpf_jit_harden = 2, and that one assignment\nis returned when \n$ grep -r net.core.bpf_jit_harden /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-core-bpf-jit-harden
    creationTimestamp: "2021-07-13T16:59:06Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:06Z"
    name: rhcos4-moderate-worker-sysctl-net-core-bpf-jit-harden
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41025"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-core-bpf-jit-harden
    uid: f35dcce1-f466-4fcc-8fb2-7c43297ff48b
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Accepting ICMP Redirects for All IPv4 Interfaces
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages modify the
    host's route table and are unauthenticated. An illicit ICMP redirect
    message could result in a man-in-the-middle attack.

    This feature of the IPv4 protocol has few legitimate uses. It should be
    disabled unless absolutely required."
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_accept_redirects
  instructions: "The runtime status of the net.ipv4.conf.all.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-accept-redirects
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40665"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-accept-redirects
    uid: 4e8cc8b6-b976-405a-a32a-4a2192897945
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on all IPv4 Interfaces
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router,
    which can be used to bypass network security measures. This requirement
    applies only to the forwarding of source-routerd traffic, such as when IPv4
    forwarding is enabled and the system is functioning as a router.

    Accepting source-routed packets in the IPv4 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_accept_source_route
  instructions: "The runtime status of the net.ipv4.conf.all.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-accept-source-route
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41458"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-accept-source-route
    uid: c3b85881-f698-47db-9d0f-f21857d21fbf
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Log Martian Packets on all IPv4 Interfaces
    The presence of "martian" packets (which have impossible addresses)
    as well as spoofed packets, source-routed packets, and redirects could be a
    sign of nefarious network activity. Logging these packets enables this activity
    to be detected.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_log_martians
  instructions: "The runtime status of the net.ipv4.conf.all.log_martians kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.log_martians\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.log_martians\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.log_martians = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.log_martians = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.log_martians /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-log-martians
    creationTimestamp: "2021-07-13T16:59:00Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:00Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-log-martians
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40659"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-log-martians
    uid: 3fc00aad-4008-4f4f-b667-eb40d3a14f22
  severity: unknown
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Use Reverse Path Filtering on all IPv4 Interfaces
    Enabling reverse path filtering drops packets with source addresses
    that should not have been able to be received on the interface they were
    received on. It should not be used on systems which are routers for
    complicated networks, but is helpful for end hosts and routers serving small
    networks.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_rp_filter
  instructions: "The runtime status of the net.ipv4.conf.all.rp_filter kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.rp_filter\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.rp_filter\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.rp_filter = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.rp_filter = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.rp_filter /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-rp-filter
    creationTimestamp: "2021-07-13T16:59:07Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:07Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-rp-filter
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41085"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-rp-filter
    uid: f5f2b86e-321e-42cb-a01d-07ae8b21bc90
  severity: medium
  status: PASS
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Secure ICMP Redirects on all IPv4 Interfaces
    Accepting "secure" ICMP redirects (from those gateways listed as
    default gateways) has few legitimate uses. It should be disabled unless it is
    absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_secure_redirects
  instructions: "The runtime status of the net.ipv4.conf.all.secure_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.secure_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.secure_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.secure_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.secure_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.secure_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-secure-redirects
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-secure-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40943"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-secure-redirects
    uid: f06ff605-98bb-48ce-b8dd-5384ff54547d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Sending ICMP Redirects on all IPv4 Interfaces
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages contain information
    from the system's route table possibly revealing portions of the network topology.

    The ability to send ICMP redirects is only appropriate for systems acting as routers.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_all_send_redirects
  instructions: "The runtime status of the net.ipv4.conf.all.send_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.all.send_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.all.send_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.all.send_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.all.send_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.all.send_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-all-send-redirects
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-send-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41151"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-all-send-redirects
    uid: 4eb469e9-8629-44fa-8feb-fbee6ec39311
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting ICMP Redirects by Default on IPv4 Interfaces
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages modify the
    host's route table and are unauthenticated. An illicit ICMP redirect
    message could result in a man-in-the-middle attack.
    This feature of the IPv4 protocol has few legitimate uses. It should
    be disabled unless absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_accept_redirects
  instructions: "The runtime status of the net.ipv4.conf.default.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-accept-redirects
    creationTimestamp: "2021-07-13T16:59:10Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:10Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41197"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-accept-redirects
    uid: 8705418d-c86e-4cd6-a924-5a812fb4d378
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on IPv4 Interfaces by Default
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router,
    which can be used to bypass network security measures.

    Accepting source-routed packets in the IPv4 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required, such as when
    IPv4 forwarding is enabled and the system is legitimately functioning as a
    router.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_accept_source_route
  instructions: "The runtime status of the net.ipv4.conf.default.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-accept-source-route
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41342"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-accept-source-route
    uid: 81c5689d-9923-444c-8cf5-f5c9b643c493
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Paremeter to Log Martian Packets on all IPv4 Interfaces by Default
    The presence of "martian" packets (which have impossible addresses)
    as well as spoofed packets, source-routed packets, and redirects could be a
    sign of nefarious network activity. Logging these packets enables this activity
    to be detected.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_log_martians
  instructions: "The runtime status of the net.ipv4.conf.default.log_martians kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.log_martians\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.log_martians\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.log_martians = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.log_martians = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.log_martians /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-log-martians
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-log-martians
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40607"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-log-martians
    uid: 696d11bf-8021-4abc-af38-ca07c2fe5912
  severity: unknown
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Use Reverse Path Filtering on all IPv4 Interfaces by Default
    Enabling reverse path filtering drops packets with source addresses
    that should not have been able to be received on the interface they were
    received on. It should not be used on systems which are routers for
    complicated networks, but is helpful for end hosts and routers serving small
    networks.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_rp_filter
  instructions: "The runtime status of the net.ipv4.conf.default.rp_filter kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.rp_filter\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.rp_filter\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.rp_filter = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.rp_filter = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.rp_filter /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-rp-filter
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-rp-filter
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40887"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-rp-filter
    uid: 2e430acb-d223-439b-8620-47c3b07cf1d4
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Kernel Parameter for Accepting Secure Redirects By Default
    Accepting "secure" ICMP redirects (from those gateways listed as
    default gateways) has few legitimate uses. It should be disabled unless it is
    absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_secure_redirects
  instructions: "The runtime status of the net.ipv4.conf.default.secure_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.secure_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.secure_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.secure_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.secure_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.secure_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-secure-redirects
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-secure-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40758"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-secure-redirects
    uid: 84a9bd2e-2753-434e-a8e2-18c19febcfea
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Sending ICMP Redirects on all IPv4 Interfaces by Default
    ICMP redirect messages are used by routers to inform hosts that a more
    direct route exists for a particular destination. These messages contain information
    from the system's route table possibly revealing portions of the network topology.

    The ability to send ICMP redirects is only appropriate for systems acting as routers.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_conf_default_send_redirects
  instructions: "The runtime status of the net.ipv4.conf.default.send_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.conf.default.send_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.conf.default.send_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.conf.default.send_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.conf.default.send_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv4.conf.default.send_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-conf-default-send-redirects
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-send-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40440"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-conf-default-send-redirects
    uid: 5c0d22bf-5418-47b4-8a18-02d441ca641c
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Ignore ICMP Broadcast Echo Requests on IPv4 Interfaces
    Responding to broadcast (ICMP) echoes facilitates network mapping
    and provides a vector for amplification attacks.

    Ignoring ICMP echo requests (pings) sent to broadcast or multicast
    addresses makes the system slightly more difficult to enumerate on the network.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_icmp_echo_ignore_broadcasts
  instructions: "The runtime status of the net.ipv4.icmp_echo_ignore_broadcasts kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.icmp_echo_ignore_broadcasts\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.icmp_echo_ignore_broadcasts\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.icmp_echo_ignore_broadcasts = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.icmp_echo_ignore_broadcasts /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-icmp-echo-ignore-broadcasts
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-icmp-echo-ignore-broadcasts
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40430"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-icmp-echo-ignore-broadcasts
    uid: 63f7c375-56cd-4720-888e-dc4dc8b542ba
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Ignore Bogus ICMP Error Responses on IPv4 Interfaces
    Ignoring bogus ICMP error responses reduces
    log size, although some activity would not be logged.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_icmp_ignore_bogus_error_responses
  instructions: "The runtime status of the net.ipv4.icmp_ignore_bogus_error_responses kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.icmp_ignore_bogus_error_responses\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.icmp_ignore_bogus_error_responses\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.icmp_ignore_bogus_error_responses = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.icmp_ignore_bogus_error_responses /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-icmp-ignore-bogus-error-responses
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-icmp-ignore-bogus-error-responses
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41350"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-icmp-ignore-bogus-error-responses
    uid: 0ffd1ddb-49ad-429e-ac1c-3201a930157d
  severity: unknown
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Enable Kernel Parameter to Use TCP Syncookies on IPv4 Interfaces
    A TCP SYN flood attack can cause a denial of service by filling a
    system's TCP connection table with connections in the SYN_RCVD state.
    Syncookies can be used to track a connection when a subsequent ACK is received,
    verifying the initiator is attempting a valid connection and is not a flood
    source. This feature is activated when a flood condition is detected, and
    enables the system to continue servicing valid connection requests.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv4_tcp_syncookies
  instructions: "The runtime status of the net.ipv4.tcp_syncookies kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv4.tcp_syncookies\nThe output of the command should indicate a value of 1.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv4.tcp_syncookies\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv4.tcp_syncookies = 1\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv4.tcp_syncookies = 1, and that one assignment\nis returned when \n$ grep -r net.ipv4.tcp_syncookies /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv4-tcp-syncookies
    creationTimestamp: "2021-07-13T16:59:01Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:01Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv4-tcp-syncookies
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40770"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv4-tcp-syncookies
    uid: 817e3935-5ffb-4643-a22d-4133c5449e71
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Configure Accepting Router Advertisements on All IPv6 Interfaces
    An illicit router advertisement message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_all_accept_ra
  instructions: "The runtime status of the net.ipv6.conf.all.accept_ra kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.all.accept_ra\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.all.accept_ra\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.all.accept_ra = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.all.accept_ra = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.all.accept_ra /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-all-accept-ra
    creationTimestamp: "2021-07-13T16:58:57Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:57Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv6-conf-all-accept-ra
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40471"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv6-conf-all-accept-ra
    uid: 2f49a739-3d9e-483f-ad0e-436f66b2456b
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Accepting ICMP Redirects for All IPv6 Interfaces
    An illicit ICMP redirect message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_all_accept_redirects
  instructions: "The runtime status of the net.ipv6.conf.all.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.all.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.all.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.all.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.all.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.all.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-all-accept-redirects
    creationTimestamp: "2021-07-13T16:59:09Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:09Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv6-conf-all-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41179"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv6-conf-all-accept-redirects
    uid: faddef3e-26a7-4477-af52-bcebc4dc925d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on all IPv6 Interfaces
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router, which can
    be used to bypass network security measures. This requirement applies only to the
    forwarding of source-routerd traffic, such as when IPv6 forwarding is enabled and
    the system is functioning as a router.

    Accepting source-routed packets in the IPv6 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_all_accept_source_route
  instructions: "The runtime status of the net.ipv6.conf.all.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.all.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.all.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.all.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.all.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.all.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-all-accept-source-route
    creationTimestamp: "2021-07-13T16:59:05Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:05Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv6-conf-all-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40948"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv6-conf-all-accept-source-route
    uid: eaa12c18-3f11-441a-ae97-e2162a4eb469
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Accepting Router Advertisements on all IPv6 Interfaces by Default
    An illicit router advertisement message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_default_accept_ra
  instructions: "The runtime status of the net.ipv6.conf.default.accept_ra kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.default.accept_ra\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.default.accept_ra\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.default.accept_ra = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.default.accept_ra = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.default.accept_ra /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-default-accept-ra
    creationTimestamp: "2021-07-13T16:59:14Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:14Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv6-conf-default-accept-ra
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41378"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv6-conf-default-accept-ra
    uid: 188d046f-83d3-4a80-9984-d1427685ae41
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting ICMP Redirects by Default on IPv6 Interfaces
    An illicit ICMP redirect message could result in a man-in-the-middle attack.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_default_accept_redirects
  instructions: "The runtime status of the net.ipv6.conf.default.accept_redirects kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.default.accept_redirects\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.default.accept_redirects\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.default.accept_redirects = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.default.accept_redirects = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.default.accept_redirects /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-default-accept-redirects
    creationTimestamp: "2021-07-13T16:58:56Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:56Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv6-conf-default-accept-redirects
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40386"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv6-conf-default-accept-redirects
    uid: a73b6b40-6e8e-4f77-8b49-05655b3e1a4d
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable Kernel Parameter for Accepting Source-Routed Packets on IPv6 Interfaces by Default
    Source-routed packets allow the source of the packet to suggest routers
    forward the packet along a different path than configured on the router, which can
    be used to bypass network security measures. This requirement applies only to the
    forwarding of source-routerd traffic, such as when IPv6 forwarding is enabled and
    the system is functioning as a router.

    Accepting source-routed packets in the IPv6 protocol has few legitimate
    uses. It should be disabled unless it is absolutely required.
  id: xccdf_org.ssgproject.content_rule_sysctl_net_ipv6_conf_default_accept_source_route
  instructions: "The runtime status of the net.ipv6.conf.default.accept_source_route kernel parameter can be queried\nby running the following command:\n$ sysctl net.ipv6.conf.default.accept_source_route\nThe output of the command should indicate a value of 0.\nThe preferable way how to assure the runtime compliance is to have\ncorrect persistent configuration, and rebooting the system.\n\nThe persistent kernel parameter configuration is performed by specifying the appropriate\nassignment in any file located in the /etc/sysctl.d directory.\nVerify that there is not any existing incorrect configuration by executing the following command:\n$ grep -r '^\\s*net.ipv6.conf.default.accept_source_route\\s*=' /etc/sysctl.conf /etc/sysctl.d\nIf any assignments other than\nnet.ipv6.conf.default.accept_source_route = 0\nare found, or the correct assignment is duplicated, remove those offending lines from respective files,\nand make sure that exactly one file in \n/etc/sysctl.d contains net.ipv6.conf.default.accept_source_route = 0, and that one assignment\nis returned when \n$ grep -r net.ipv6.conf.default.accept_source_route /etc/sysctl.conf /etc/sysctl.d\nis executed."
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: sysctl-net-ipv6-conf-default-accept-source-route
    creationTimestamp: "2021-07-13T16:59:04Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:04Z"
    name: rhcos4-moderate-worker-sysctl-net-ipv6-conf-default-accept-source-route
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40917"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-sysctl-net-ipv6-conf-default-accept-source-route
    uid: 499c3a89-62cd-4978-80a2-051fe803e8ed
  severity: medium
  status: FAIL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Authorize Human Interface Devices and USB hubs in USBGuard daemon
    Without allowing Human Interface Devices, it might not be possible
    to interact with the system. Without allowing hubs, it might not be possible to use any
    USB devices on the system.
  id: xccdf_org.ssgproject.content_rule_usbguard_allow_hid_and_hub
  instructions: |-
    To verify that USB Human Interface Devices and hubs will be authorized by the USBGuard daemon,
    run the following command:
    $ sudo grep allow /etc/usbguard/rules.conf
    The output lines should include
    allow with-interface match-all { 03:*:* 09:00:* }
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: usbguard-allow-hid-and-hub
    creationTimestamp: "2021-07-13T16:58:59Z"
    generation: 1
    labels:
      compliance.openshift.io/automated-remediation: ""
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: FAIL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/automated-remediation: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
        f:warnings: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:58:59Z"
    name: rhcos4-moderate-worker-usbguard-allow-hid-and-hub
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "40629"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-usbguard-allow-hid-and-hub
    uid: 19fa5861-c777-46b0-8c21-00bc85d389b8
  severity: medium
  status: FAIL
  warnings:
  - This rule should be understood primarily as a convenience administration feature. This rule ensures that if the USBGuard default rules.conf file is present, it will alter it so that USB human interface devices and hubs are allowed. However, if the rules.conf file is altered by system administrator, the rule does not check if USB human interface devices and hubs are allowed. This assumes that an administrator modified the file with some purpose in mind.
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Disable WiFi or Bluetooth in BIOS
    Disabling wireless support in the BIOS prevents easy
    activation of the wireless interface, generally requiring administrators
    to reboot the system first.
  id: xccdf_org.ssgproject.content_rule_wireless_disable_in_bios
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: wireless-disable-in-bios
    creationTimestamp: "2021-07-13T16:59:13Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: unknown
      compliance.openshift.io/check-status: MANUAL
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:13Z"
    name: rhcos4-moderate-worker-wireless-disable-in-bios
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41327"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-wireless-disable-in-bios
    uid: a13d280e-6021-4412-acab-af93f20bf07d
  severity: unknown
  status: MANUAL
- apiVersion: compliance.openshift.io/v1alpha1
  description: |-
    Deactivate Wireless Network Interfaces
    The use of wireless networking can introduce many different attack vectors into
    the organization's network. Common attack vectors such as malicious association
    and ad hoc networks will allow an attacker to spoof a wireless access point
    (AP), allowing validated systems to connect to the malicious AP and enabling the
    attacker to monitor and record network traffic. These malicious APs can also
    serve to create a man-in-the-middle attack or be used to create a denial of
    service to valid network resources.
  id: xccdf_org.ssgproject.content_rule_wireless_disable_interfaces
  instructions: |-
    Verify that there are no wireless interfaces configured on the system
    with the following command:

    $ sudo nmcli device
    The output should contain the following:
    wifi disconnected
  kind: ComplianceCheckResult
  metadata:
    annotations:
      compliance.openshift.io/rule: wireless-disable-interfaces
    creationTimestamp: "2021-07-13T16:59:17Z"
    generation: 1
    labels:
      compliance.openshift.io/check-severity: medium
      compliance.openshift.io/check-status: PASS
      compliance.openshift.io/scan-name: rhcos4-moderate-worker
      compliance.openshift.io/suite: nist-moderate-node
    managedFields:
    - apiVersion: compliance.openshift.io/v1alpha1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:id: {}
        f:instructions: {}
        f:metadata:
          f:annotations:
            .: {}
            f:compliance.openshift.io/rule: {}
          f:labels:
            .: {}
            f:compliance.openshift.io/check-severity: {}
            f:compliance.openshift.io/check-status: {}
            f:compliance.openshift.io/scan-name: {}
            f:compliance.openshift.io/suite: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5c48d266-b4c8-4bf3-960e-4b6498020522"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:severity: {}
        f:status: {}
      manager: compliance-operator
      operation: Update
      time: "2021-07-13T16:59:17Z"
    name: rhcos4-moderate-worker-wireless-disable-interfaces
    namespace: openshift-compliance
    ownerReferences:
    - apiVersion: compliance.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: ComplianceScan
      name: rhcos4-moderate-worker
      uid: 5c48d266-b4c8-4bf3-960e-4b6498020522
    resourceVersion: "41491"
    selfLink: /apis/compliance.openshift.io/v1alpha1/namespaces/openshift-compliance/compliancecheckresults/rhcos4-moderate-worker-wireless-disable-interfaces
    uid: 72bad9ee-490d-437f-91a2-e97e3b3fbee6
  severity: medium
  status: PASS
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
